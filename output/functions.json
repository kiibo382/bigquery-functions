[
  {
    "name": "ABS",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes absolute value. Returns an error if the argument is an integer and the\noutput value cannot be represented as the same type; this happens only for the\nlargest negative input value, which has no positive representation.\n\n| X | ABS(X) |\n| --- | --- |\n| 25 | 25 |\n| -25 | 25 |\n|  `+inf` |  `+inf` |\n|  `-inf` |  `+inf` |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "ACOS",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the principal value of the inverse cosine of X. The return value is in\nthe range [0,π]. Generates an error if X is a value outside of the\nrange [-1, 1].\n\n| X | ACOS(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n| X < -1 | Error |\n| X > 1 | Error |\n\n\n\n"
  },
  {
    "name": "ACOSH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the inverse hyperbolic cosine of X. Generates an error if X is a value\nless than 1.\n\n| X | ACOSH(X) |\n| --- | --- |\n|  `+inf` |  `+inf` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n| X < 1 | Error |\n\n\n\n"
  },
  {
    "name": "AEAD.DECRYPT_BYTES",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nUses the matching key from `keyset` to decrypt `ciphertext` and verifies the\nintegrity of the data using `additional_data` . Returns an error if decryption or\nverification fails.\n\n `keyset` is a serialized `BYTES` value returned by one of the `KEYS` functions or a `STRUCT` returned by `KEYS.KEYSET_CHAIN` . `keyset` must contain the key that was used to\nencrypt `ciphertext` , and the key must be in an `'ENABLED'` state, or else the\nfunction returns an error. `AEAD.DECRYPT_BYTES` identifies the matching key\nin `keyset` by finding the key with the key ID that matches the one encrypted in `ciphertext` .\n\n `ciphertext` is a `BYTES` value that is the result of\na call to `AEAD.ENCRYPT` where the input `plaintext` was of type `BYTES` .\n\nIf `ciphertext` includes an initialization vector (IV),\nit should be the first bytes of `ciphertext` . If `ciphertext` includes an\nauthentication tag, it should be the last bytes of `ciphertext` . If the\nIV and authentic tag are one (SIV), it should be the first bytes of `ciphertext` . The IV and authentication tag commonly require 16 bytes, but may\nvary in size.\n\n `additional_data` is a `STRING` or `BYTES` value that binds the ciphertext to\nits context. This forces the ciphertext to be decrypted in the same context in\nwhich it was encrypted. This function casts any `STRING` value to `BYTES` .\nThis must be the same as the `additional_data` provided to `AEAD.ENCRYPT` to\nencrypt `ciphertext` , ignoring its type, or else the function returns an error.\n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nThis example creates a table of unique IDs with associated plaintext values and\nkeysets. Then it uses these keysets to encrypt the plaintext values as `BYTES` and store them in a new table. Finally, it\nuses `AEAD.DECRYPT_BYTES` to decrypt the encrypted values and display them as\nplaintext.\n\nThe following statement creates a table `CustomerKeysets` containing a column of\nunique IDs, a column of `AEAD_AES_GCM_256` keysets, and a column of favorite\nanimals.\n\nThe following statement creates a table `EncryptedCustomerData` containing a\ncolumn of unique IDs and a column of ciphertext. The statement encrypts the\nplaintext `favorite_animal` using the keyset value from `CustomerKeysets` corresponding to each unique ID.\n\nThe following query uses the keysets in the `CustomerKeysets` table to decrypt\ndata in the `EncryptedCustomerData` table.\n\n\n"
  },
  {
    "name": "AEAD.DECRYPT_STRING",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nLike [AEAD.DECRYPT_BYTES](#aeaddecrypt_bytes) , but where `additional_data` is\nof type `STRING` .\n\n **Return Data Type** \n\n `STRING` \n\n\n\n"
  },
  {
    "name": "AEAD.ENCRYPT",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nEncrypts `plaintext` using the primary cryptographic key in `keyset` . The\nalgorithm of the primary key must be `AEAD_AES_GCM_256` . Binds the ciphertext to\nthe context defined by `additional_data` . Returns `NULL` if any input is `NULL` .\n\n `keyset` is a serialized `BYTES` value returned by one of the `KEYS` functions or a `STRUCT` returned by `KEYS.KEYSET_CHAIN` .\n\n `plaintext` is the `STRING` or `BYTES` value to be encrypted.\n\n `additional_data` is a `STRING` or `BYTES` value that binds the ciphertext to\nits context. This forces the ciphertext to be decrypted in the same context in\nwhich it was encrypted. `plaintext` and `additional_data` must be of the same\ntype. `AEAD.ENCRYPT(keyset, string1, string2)` is equivalent to `AEAD.ENCRYPT(keyset, CAST(string1 AS BYTES), CAST(string2 AS BYTES))` .\n\nThe output is ciphertext `BYTES` . The ciphertext contains a [Tink-specific](https://github.com/google/tink/blob/master/docs/KEY-MANAGEMENT.md) prefix indicating the key used to perform the encryption.\n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nThe following query uses the keysets for each `customer_id` in the `CustomerKeysets` table to encrypt the value of the plaintext `favorite_animal` in the `PlaintextCustomerData` table corresponding to that `customer_id` . The\noutput contains a column of `customer_id` values and a column of\ncorresponding ciphertext output as `BYTES` .\n\n\n"
  },
  {
    "name": "ANY_VALUE",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns `expression` for some row chosen from the group. Which row is chosen is\nnondeterministic, not random. Returns `NULL` when the input produces no\nrows. Returns `NULL` when `expression` or `expression2` is `NULL` for all rows in the group.\n\n `ANY_VALUE` behaves as if `IGNORE NULLS` is specified;\nrows for which `expression` is `NULL` are not considered and won't be\nselected.\n\nIf the `HAVING` clause is included in the `ANY_VALUE` function, the `OVER` clause can't be used with this function.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\nAny\n\n **Returned Data Types** \n\nMatches the input data type.\n\n **Examples** \n\n\n"
  },
  {
    "name": "APPENDS",
    "arguments": [],
    "category": "Table",
    "description_markdown": " **Preview** \n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\"\n    in the General Service Terms section of the [Service Specific Terms](/terms/service-terms) .\n    Pre-GA products and features are available \"as is\" and might have\n    limited support. For more information, see the [launch stage descriptions](/products#product-launch-stages) .\n\n **Note:** To provide feedback or request support for this feature, send an email to [bq-change-history-feedback@google.com](mailto:bq-change-history-feedback@google.com) . **Description** \n\nThe `APPENDS` function returns all rows appended to a table for a given\ntime range.\n\nThe following operations add rows to the `APPENDS` change history:\n\n-  [CREATE TABLE DDL statement](/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement) \n-  [INSERT DML statement](/bigquery/docs/reference/standard-sql/dml-syntax#insert_statement) \n-  [Data appended as part of a MERGE DML statement](/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement) \n-  [Loading data](/bigquery/docs/loading-data) into BigQuery\n-  [Streaming ingestion](/bigquery/docs/write-api#use_data_manipulation_language_dml_with_recently_streamed_data) \n\n **Definitions** \n\n-  `    table` : the BigQuery table name. This must be a regular\nBigQuery table. This argument must be preceded by the word `    TABLE` .\n-  `    start_timestamp` : a [TIMESTAMP](/bigquery/docs/reference/standard-sql/data-types#timestamp_type) value indicating the earliest time at which a\nchange is included in the output. If the value is `    NULL` , all changes since the\ntable creation are returned. If the table was created after the `    start_timestamp` value, the actual table creation time is used instead. An error\nis returned if the time specified is earlier than allowed by [time travel](/bigquery/docs/time-travel) , or\nif the table was created earlier than allowed by time travel if the `    start_timestamp` value is `    NULL` . For standard tables, this window is seven days,\nbut you can [configure the time travel window](/bigquery/docs/time-travel#configure_the_time_travel_window) to be less than\nthat.\n-  `    end_timestamp` : a `    TIMESTAMP` value indicating the latest time at\nwhich a change is included in the output. `    end_timestamp` is exclusive; for\nexample, if you specify `    2023-12-31 08:00:00` for `    start_timestamp` and `    2023-12-31 12:00:00` for `    end_timestamp` , all changes made from\n8 AM December 31, 2023 through 11:59 AM December 31, 2023 are returned.\n    \n    If the `    end_timestamp` value is `    NULL` , all changes made\nuntil the start of the query are included.\n    \n    \n\n **Details** \n\nRecords of inserted rows persist even if that data is later deleted. Deletions\nare not reflected in the `APPENDS` function. If a table\nis copied, calling the `APPENDS` function on the copied table returns every row\nas inserted at the time of table creation. If a row is modified due to an `UPDATE` operation, there's no effect.\n\n **Output** \n\nThe `APPENDS` function returns a table with the following columns:\n\n- All columns of the input table at the time the query is run. If a column is\nadded after the `    end_timestamp` value, it appears with `    NULL` values populated in any\nof the rows that were inserted before the addition of the column.\n-  `    _CHANGE_TYPE` : a `    STRING` value indicating the type of change that produced\nthe row. For `    APPENDS` , the only supported value is `    INSERT` .\n-  `    _CHANGE_TIMESTAMP` : a `    TIMESTAMP` value indicating the commit time of the\ntransaction that made the change.\n\n **Limitations** \n\n- The data returned by the `    APPENDS` function is limited to the time travel\nwindow of the table.\n- You can't call the `    APPENDS` function within a multi-statement transaction.\n- You can only use the `    APPENDS` function with regular BigQuery\ntables. Clones, snapshots, views, materialized views, external tables, and\nwildcard tables aren't supported.\n- Partition pseudo-columns for ingestion-time partitioned tables, such as `    _PARTITIONTIME` and `    _PARTITIONDATE` , aren't included in the function's\noutput.\n\n **Example** \n\nThis example shows the change history returned by the `APPENDS` function as various\nchanges are made to a table called `Produce` . First, create the table:\n\nNext, insert two rows into the table:\n\nTo view the full change history of appends, use `NULL` values to get the full\nhistory within the time travel window:\n\nThe output is similar to the following:\n\nNext, add a column, insert a new row of values, update the inventory, and delete\nthe `bananas` row:\n\nView the new table:\n\nThe output is similar to the following:\n\nView the full change history of appends:\n\nThe output is similar to the following:\n\nThe `inventory` column displays the values that were set when the rows were\noriginally inserted into to the table. It does not show the changes from the `UPDATE` statement. The row with information on bananas is still present because\nthe `APPENDS` function only captures additions to tables, not deletions.\n\n\n\n"
  },
  {
    "name": "APPROX_COUNT_DISTINCT",
    "arguments": [],
    "category": "Approximate_aggregate",
    "description_markdown": " **Description** \n\nReturns the approximate result for `COUNT(DISTINCT expression)` . The value\nreturned is a statistical estimate, not necessarily the actual value.\n\nThis function is less accurate than `COUNT(DISTINCT expression)` , but performs\nbetter on huge input.\n\n **Supported Argument Types** \n\nAny data type **except** :\n\n-  `    ARRAY` \n-  `    STRUCT` \n-  `    INTERVAL` \n\n **Returned Data Types** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "APPROX_QUANTILES",
    "arguments": [],
    "category": "Approximate_aggregate",
    "description_markdown": " **Description** \n\nReturns the approximate boundaries for a group of `expression` values, where `number` represents the number of quantiles to create. This function returns an\narray of `number` + 1 elements, sorted in ascending order, where the\nfirst element is the approximate minimum and the last element is the approximate\nmaximum.\n\nReturns `NULL` if there are zero input rows or `expression` evaluates to `NULL` for all rows.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n **Supported Argument Types** \n\n-  `    expression` : Any supported data type **except** :\n    \n    \n    -  `        ARRAY` \n    -  `        STRUCT` \n    -  `        INTERVAL` \n-  `    number` : `    INT64` literal or query parameter.\n    \n    \n\n **Returned Data Types** \n\n `ARRAY&lt;T&gt;` where `T` is the type specified by `expression` .\n\n **Examples** \n\n\n"
  },
  {
    "name": "APPROX_TOP_COUNT",
    "arguments": [],
    "category": "Approximate_aggregate",
    "description_markdown": " **Description** \n\nReturns the approximate top elements of `expression` as an array of `STRUCT` s.\nThe `number` parameter specifies the number of elements returned.\n\nEach `STRUCT` contains two fields. The first field (named `value` ) contains an\ninput value. The second field (named `count` ) contains an `INT64` specifying the\nnumber of times the value was returned.\n\nReturns `NULL` if there are zero input rows.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n **Supported Argument Types** \n\n-  `    expression` : Any data type that the `    GROUP BY` clause supports.\n-  `    number` : `    INT64` literal or query parameter.\n\n **Returned Data Types** \n\n `ARRAY&lt;STRUCT&gt;` \n\n **Examples** \n\n **NULL handling** \n\n `APPROX_TOP_COUNT` does not ignore `NULL` s in the input. For example:\n\n\n"
  },
  {
    "name": "APPROX_TOP_SUM",
    "arguments": [],
    "category": "Approximate_aggregate",
    "description_markdown": " **Description** \n\nReturns the approximate top elements of `expression` , based on the sum of an\nassigned `weight` . The `number` parameter specifies the number of elements\nreturned.\n\nIf the `weight` input is negative or `NaN` , this function returns an error.\n\nThe elements are returned as an array of `STRUCT` s.\nEach `STRUCT` contains two fields: `value` and `sum` .\nThe `value` field contains the value of the input expression. The `sum` field is\nthe same type as `weight` , and is the approximate sum of the input weight\nassociated with the `value` field.\n\nReturns `NULL` if there are zero input rows.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n **Supported Argument Types** \n\n-  `    expression` : Any data type that the `    GROUP BY` clause supports.\n-  `    weight` : One of the following:\n    \n    \n    -  `        INT64` \n    -  `        NUMERIC` \n    -  `        BIGNUMERIC` \n    -  `        FLOAT64` \n-  `    number` : `    INT64` literal or query parameter.\n    \n    \n\n **Returned Data Types** \n\n `ARRAY&lt;STRUCT&gt;` \n\n **Examples** \n\n **NULL handling** \n\n `APPROX_TOP_SUM` does not ignore `NULL` values for the `expression` and `weight` parameters.\n\n\n<span id=\"array_functions\"></span>\n## Array functions\n\n\nGoogleSQL for BigQuery supports the following array functions.\n\n\n\n"
  },
  {
    "name": "ARRAY",
    "arguments": [],
    "category": "Array",
    "description_markdown": " **Description** \n\nThe `ARRAY` function returns an `ARRAY` with one element for each row in a [subquery](/bigquery/docs/reference/standard-sql/subqueries) .\n\nIf `subquery` produces a\nSQL table,\nthe table must have exactly one column. Each element in the output `ARRAY` is\nthe value of the single column of a row in the table.\n\nIf `subquery` produces a\nvalue table,\nthen each element in the output `ARRAY` is the entire corresponding row of the\nvalue table.\n\n **Constraints** \n\n- Subqueries are unordered, so the elements of the output `    ARRAY` are not\nguaranteed to preserve any order in the source table for the subquery. However,\nif the subquery includes an `    ORDER BY` clause, the `    ARRAY` function will return\nan `    ARRAY` that honors that clause.\n- If the subquery returns more than one column, the `    ARRAY` function returns an\nerror.\n- If the subquery returns an `    ARRAY` typed column or `    ARRAY` typed rows, the `    ARRAY` function returns an error that GoogleSQL does not support `    ARRAY` s with elements of type [ARRAY](/bigquery/docs/reference/standard-sql/data-types#array_type) .\n- If the subquery returns zero rows, the `    ARRAY` function returns an empty `    ARRAY` . It never returns a `    NULL`  `    ARRAY` .\n\n **Return type** \n\n `ARRAY` \n\n **Examples** \n\nTo construct an `ARRAY` from a subquery that contains multiple\ncolumns, change the subquery to use `SELECT AS STRUCT` . Now\nthe `ARRAY` function will return an `ARRAY` of `STRUCT` s. The `ARRAY` will\ncontain one `STRUCT` for each row in the subquery, and each of these `STRUCT` s\nwill contain a field for each column in that row.\n\nSimilarly, to construct an `ARRAY` from a subquery that contains\none or more `ARRAY` s, change the subquery to use `SELECT AS STRUCT` .\n\n\n"
  },
  {
    "name": "ARRAY_AGG",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns an ARRAY of `expression` values.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nIf this function is used with the `OVER` clause, it's part of a\nwindow function call. In a window function call,\naggregate function clauses can't be used.\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nAn error is raised if an array in the final query result contains a `NULL` element.\n\n **Supported Argument Types** \n\nAll data types except ARRAY.\n\n **Returned Data Types** \n\nARRAY\n\nIf there are zero input rows, this function returns `NULL` .\n\n **Examples** \n\n\n"
  },
  {
    "name": "ARRAY_CONCAT",
    "arguments": [],
    "category": "Array",
    "description_markdown": " **Description** \n\nConcatenates one or more arrays with the same element type into a single array.\n\nThe function returns `NULL` if any input argument is `NULL` .\n\n **Note:** You can also use the [|| concatenation operator](#operators) to concatenate arrays. **Return type** \n\n `ARRAY` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ARRAY_CONCAT_AGG",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nConcatenates elements from `expression` of type `ARRAY` , returning a single\narray as a result.\n\nThis function ignores `NULL` input arrays, but respects the `NULL` elements in\nnon- `NULL` input arrays. An\nerror is raised, however, if an array in the final query result contains a `NULL` element. Returns `NULL` if there are zero input rows or `expression` evaluates to `NULL` for all rows.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n **Supported Argument Types** \n\n `ARRAY` \n\n **Returned Data Types** \n\n `ARRAY` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ARRAY_LENGTH",
    "arguments": [],
    "category": "Array",
    "description_markdown": " **Description** \n\nReturns the size of the array. Returns 0 for an empty array. Returns `NULL` if\nthe `array_expression` is `NULL` .\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ARRAY_REVERSE",
    "arguments": [],
    "category": "Array",
    "description_markdown": " **Description** \n\nReturns the input `ARRAY` with elements in reverse order.\n\n **Return type** \n\n `ARRAY` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ARRAY_TO_STRING",
    "arguments": [],
    "category": "Array",
    "description_markdown": " **Description** \n\nReturns a concatenation of the elements in `array_expression` as a `STRING` . The value for `array_expression` can either be an array of `STRING` or `BYTES` data types.\n\nIf the `null_text` parameter is used, the function replaces any `NULL` values in\nthe array with the value of `null_text` .\n\nIf the `null_text` parameter is not used, the function omits the `NULL` value\nand its preceding delimiter.\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ASCII",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns the ASCII code for the first character or byte in `value` . Returns `0` if `value` is empty or the ASCII code is `0` for the first character\nor byte.\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ASIN",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the principal value of the inverse sine of X. The return value is in\nthe range [-π/2,π/2]. Generates an error if X is outside of\nthe range [-1, 1].\n\n| X | ASIN(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n| X < -1 | Error |\n| X > 1 | Error |\n\n\n\n"
  },
  {
    "name": "ASINH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the inverse hyperbolic sine of X. Does not fail.\n\n| X | ASINH(X) |\n| --- | --- |\n|  `+inf` |  `+inf` |\n|  `-inf` |  `-inf` |\n|  `NaN` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "ATAN",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the principal value of the inverse tangent of X. The return value is\nin the range [-π/2,π/2]. Does not fail.\n\n| X | ATAN(X) |\n| --- | --- |\n|  `+inf` | π/2 |\n|  `-inf` | -π/2 |\n|  `NaN` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "ATAN2",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nCalculates the principal value of the inverse tangent of X/Y using the signs of\nthe two arguments to determine the quadrant. The return value is in the range\n[-π,π].\n\n| X | Y | ATAN2(X, Y) |\n| --- | --- | --- |\n|  `NaN` | Any value |  `NaN` |\n| Any value |  `NaN` |  `NaN` |\n| 0.0 | 0.0 | 0.0 |\n| Positive Finite value |  `-inf` | π |\n| Negative Finite value |  `-inf` | -π |\n| Finite value |  `+inf` | 0.0 |\n|  `+inf` | Finite value | π/2 |\n|  `-inf` | Finite value | -π/2 |\n|  `+inf` |  `-inf` | ¾π |\n|  `-inf` |  `-inf` | -¾π |\n|  `+inf` |  `+inf` | π/4 |\n|  `-inf` |  `+inf` | -π/4 |\n\n\n\n"
  },
  {
    "name": "ATANH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the inverse hyperbolic tangent of X. Generates an error if X is outside\nof the range (-1, 1).\n\n| X | ATANH(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n| X < -1 | Error |\n| X > 1 | Error |\n\n\n\n"
  },
  {
    "name": "AVG",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns the average of non- `NULL` values in an aggregated group.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nIf this function is used with the `OVER` clause, it's part of a\nwindow function call. In a window function call,\naggregate function clauses can't be used.\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n `AVG` can be used with differential privacy. For more information, see [Differentially private aggregate functions](#aggregate-dp-functions) .\n\nCaveats:\n\n- If the aggregated group is empty or the argument is `    NULL` for all rows in\nthe group, returns `    NULL` .\n- If the argument is `    NaN` for any row in the group, returns `    NaN` .\n- If the argument is `    [+|-]Infinity` for any row in the group, returns either `    [+|-]Infinity` or `    NaN` .\n- If there is numeric overflow, produces an error.\n- If a [floating-point type](/bigquery/docs/reference/standard-sql/data-types#floating_point_types) is returned, the result is [non-deterministic](/bigquery/docs/reference/standard-sql/data-types#floating-point-semantics) , which means you might receive a\ndifferent result each time you use this function.\n\n **Supported Argument Types** \n\n- Any numeric input type\n-  `    INTERVAL` \n\n **Returned Data Types** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |  `INTERVAL` |\n| --- | --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |  `INTERVAL` |\n\n **Examples** \n\n\n"
  },
  {
    "name": "BIT_AND",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nPerforms a bitwise AND operation on `expression` and returns the result.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n **Supported Argument Types** \n\n- INT64\n\n **Returned Data Types** \n\nINT64\n\n **Examples** \n\n\n"
  },
  {
    "name": "BIT_COUNT",
    "arguments": [],
    "category": "Bit",
    "description_markdown": " **Description** \n\nThe input, `expression` , must be an\ninteger or `BYTES` .\n\nReturns the number of bits that are set in the input `expression` .\nFor signed integers, this is the number of bits in two's complement form.\n\n **Return Data Type** \n\n `INT64` \n\n **Example** \n\n\n<span id=\"conversion_functions\"></span>\n## Conversion functions\n\n\nGoogleSQL for BigQuery supports conversion functions. These data type\nconversions are explicit, but some conversions can happen implicitly. You can\nlearn more about implicit and explicit conversion [here](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\n\n\n\n"
  },
  {
    "name": "BIT_OR",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nPerforms a bitwise OR operation on `expression` and returns the result.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n **Supported Argument Types** \n\n- INT64\n\n **Returned Data Types** \n\nINT64\n\n **Examples** \n\n\n"
  },
  {
    "name": "BIT_XOR",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nPerforms a bitwise XOR operation on `expression` and returns the result.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n **Supported Argument Types** \n\n- INT64\n\n **Returned Data Types** \n\nINT64\n\n **Examples** \n\n\n"
  },
  {
    "name": "BOOL",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nConverts a JSON boolean to a SQL `BOOL` value.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    If the JSON value is not a boolean, an error is produced. If the expression\nis SQL `    NULL` , the function returns SQL `    NULL` .\n    \n    \n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\nThe following examples show how invalid requests are handled:\n\n\n"
  },
  {
    "name": "BYTE_LENGTH",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nGets the number of `BYTES` in a `STRING` or `BYTES` value,\nregardless of whether the value is a `STRING` or `BYTES` type.\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "CAST",
    "arguments": [],
    "category": "Conversion",
    "description_markdown": " **Description** \n\nCast syntax is used in a query to indicate that the result type of an\nexpression should be converted to some other type.\n\nWhen using `CAST` , a query can fail if GoogleSQL is unable to perform\nthe cast. If you want to protect your queries from these types of errors, you\ncan use [SAFE_CAST](#safe_casting) .\n\nCasts between supported types that do not successfully map from the original\nvalue to the target domain produce runtime errors. For example, casting `BYTES` to `STRING` where the byte sequence is not valid UTF-8 results in a\nruntime error.\n\nSome casts can include a [format clause](/bigquery/docs/reference/standard-sql/format-elements#formatting_syntax) , which provides\ninstructions for how to conduct the\ncast. For example, you could\ninstruct a cast to convert a sequence of bytes to a BASE64-encoded string\ninstead of a UTF-8-encoded string.\n\nThe structure of the format clause is unique to each type of cast and more\ninformation is available in the section for that cast.\n\n **Examples** \n\nThe following query results in `\"true\"` if `x` is `1` , `\"false\"` for any other\nnon- `NULL` value, and `NULL` if `x` is `NULL` .\n\n\n"
  },
  {
    "name": "CATEGORIES",
    "arguments": [],
    "category": "Geography",
    "description_markdown": "The geography functions are grouped into the following categories based on their\nbehavior:\n\n| Category | Functions | Description |\n| --- | --- | --- |\n| Constructors |  [ST_GEOGPOINT](#st_geogpoint)     \n [ST_MAKELINE](#st_makeline)     \n [ST_MAKEPOLYGON](#st_makepolygon)     \n [ST_MAKEPOLYGONORIENTED](#st_makepolygonoriented) | Functions that build new\n        geography values from coordinates\n        or existing geographies. |\n| Parsers |  [ST_GEOGFROM](#st_geogfrom)     \n [ST_GEOGFROMGEOJSON](#st_geogfromgeojson)     \n [ST_GEOGFROMTEXT](#st_geogfromtext)     \n [ST_GEOGFROMWKB](#st_geogfromwkb)     \n [ST_GEOGPOINTFROMGEOHASH](#st_geogpointfromgeohash) | Functions that create geographies\n        from an external format such as [WKT](https://en.wikipedia.org/wiki/Well-known_text) and [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON) . |\n| Formatters |  [ST_ASBINARY](#st_asbinary)     \n [ST_ASGEOJSON](#st_asgeojson)     \n [ST_ASTEXT](#st_astext)     \n [ST_GEOHASH](#st_geohash) | Functions that export geographies\n        to an external format such as WKT. |\n| Transformations |  [ST_BOUNDARY](#st_boundary)     \n [ST_BUFFER](#st_buffer)     \n [ST_BUFFERWITHTOLERANCE](#st_bufferwithtolerance)     \n [ST_CENTROID](#st_centroid)     \n [ST_CENTROID_AGG](#st_centroid_agg) (Aggregate)    \n [ST_CLOSESTPOINT](#st_closestpoint)     \n [ST_CONVEXHULL](#st_convexhull)     \n [ST_DIFFERENCE](#st_difference)     \n [ST_EXTERIORRING](#st_exteriorring)     \n [ST_INTERIORRINGS](#st_interiorrings)     \n [ST_INTERSECTION](#st_intersection)     \n [ST_LINEINTERPOLATEPOINT](#st_lineinterpolatepoint)     \n [ST_LINESUBSTRING](#st_linesubstring)     \n [ST_SIMPLIFY](#st_simplify)     \n [ST_SNAPTOGRID](#st_snaptogrid)     \n [ST_UNION](#st_union)     \n [ST_UNION_AGG](#st_union_agg) (Aggregate) | Functions that generate a new\n        geography based on input. |\n| Accessors |  [ST_DIMENSION](#st_dimension)     \n [ST_DUMP](#st_dump)     \n [ST_ENDPOINT](#st_endpoint)     \n [ST_GEOMETRYTYPE](#st_geometrytype)     \n [ST_ISCLOSED](#st_isclosed)     \n [ST_ISCOLLECTION](#st_iscollection)     \n [ST_ISEMPTY](#st_isempty)     \n [ST_ISRING](#st_isring)     \n [ST_NPOINTS](#st_npoints)     \n [ST_NUMGEOMETRIES](#st_numgeometries)     \n [ST_NUMPOINTS](#st_numpoints)     \n [ST_POINTN](#st_pointn)     \n [ST_STARTPOINT](#st_startpoint)     \n [ST_X](#st_x)     \n [ST_Y](#st_y) | Functions that provide access to\n        properties of a geography without\n        side-effects. |\n| Predicates |  [ST_CONTAINS](#st_contains)     \n [ST_COVEREDBY](#st_coveredby)     \n [ST_COVERS](#st_covers)     \n [ST_DISJOINT](#st_disjoint)     \n [ST_DWITHIN](#st_dwithin)     \n [ST_EQUALS](#st_equals)     \n [ST_INTERSECTS](#st_intersects)     \n [ST_INTERSECTSBOX](#st_intersectsbox)     \n [ST_TOUCHES](#st_touches)     \n [ST_WITHIN](#st_within) | Functions that return `TRUE` or `FALSE` for some spatial\n        relationship between two\n        geographies or some property of\n        a geography. These functions\n        are commonly used in filter\n        clauses. |\n| Measures |  [ST_ANGLE](#st_angle)     \n [ST_AREA](#st_area)     \n [ST_AZIMUTH](#st_azimuth)     \n [ST_BOUNDINGBOX](#st_boundingbox)     \n [ST_DISTANCE](#st_distance)     \n [ST_EXTENT](#st_extent) (Aggregate)    \n [ST_HAUSDORFFDISTANCE](#st_hausdorffdistance)     \n [ST_LINELOCATEPOINT](#st_linelocatepoint)     \n [ST_LENGTH](#st_length)     \n [ST_MAXDISTANCE](#st_maxdistance)     \n [ST_PERIMETER](#st_perimeter) | Functions that compute measurements\n        of one or more geographies. |\n| Clustering |  [ST_CLUSTERDBSCAN](#st_clusterdbscan) | Functions that perform clustering on geographies. |\n| S2 functions |  [S2_CELLIDFROMPOINT](#s2_cellidfrompoint)     \n [S2_COVERINGCELLIDS](#s2_coveringcellids) | Functions for working with S2 cell coverings of GEOGRAPHY. |\n\n\n\n"
  },
  {
    "name": "CBRT",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the cube root of `X` . `X` can be any data type\nthat [coerces to FLOAT64](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the `SAFE.` prefix.\n\n| X | CBRT(X) |\n| --- | --- |\n|  `+inf` |  `inf` |\n|  `-inf` |  `-inf` |\n|  `NaN` |  `NaN` |\n|  `0` |  `0` |\n|  `NULL` |  `NULL` |\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "CEIL",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns the smallest integral value that is not less than X.\n\n| X | CEIL(X) |\n| --- | --- |\n| 2.0 | 2.0 |\n| 2.3 | 3.0 |\n| 2.8 | 3.0 |\n| 2.5 | 3.0 |\n| -2.3 | -2.0 |\n| -2.8 | -2.0 |\n| -2.5 | -2.0 |\n| 0 | 0 |\n|  `+inf` |  `+inf` |\n|  `-inf` |  `-inf` |\n|  `NaN` |  `NaN` |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "CEILING",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nSynonym of CEIL(X)\n\n\n\n"
  },
  {
    "name": "CHANGES",
    "arguments": [],
    "category": "Table",
    "description_markdown": " **Preview** \n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\"\n    in the General Service Terms section of the [Service Specific Terms](/terms/service-terms) .\n    Pre-GA products and features are available \"as is\" and might have\n    limited support. For more information, see the [launch stage descriptions](/products#product-launch-stages) .\n\n **Note:** To provide feedback or request support for this feature, send an email to [bq-change-history-feedback@google.com](mailto:bq-change-history-feedback@google.com) . **Description** \n\nThe `CHANGES` function returns all rows that have changed in a table for a given\ntime range. To use the `CHANGES` function on a table, you must set the table's [enable_change_history option](/bigquery/docs/reference/standard-sql/data-definition-language#table_option_list) to `TRUE` .\n\nThe following operations add rows to the `CHANGES` change history:\n\n-  [CREATE TABLE DDL statement](/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement) \n-  [INSERT DML statement](/bigquery/docs/reference/standard-sql/dml-syntax#insert_statement) \n-  [Data appended, changed or deleted as part of a MERGE DML statement](/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement) \n-  [UPDATE DML statement](/bigquery/docs/reference/standard-sql/dml-syntax#update_statement) \n-  [DELETE DML statement](/bigquery/docs/reference/standard-sql/dml-syntax#delete_statement) \n-  [Loading data](/bigquery/docs/loading-data) into BigQuery\n-  [Streaming ingestion](/bigquery/docs/write-api#use_data_manipulation_language_dml_with_recently_streamed_data) \n-  [TRUNCATE TABLE DML statement](/bigquery/docs/reference/standard-sql/dml-syntax#truncate_table_statement) \n-  [Jobs](/bigquery/docs/reference/rest/v2/Job) configured with a `    writeDisposition` of `    WRITE_TRUNCATE` \n- Individual [table partition deletions](/bigquery/docs/managing-partitioned-tables#delete_a_partition) \n\n **Definitions** \n\n-  `    table` : the BigQuery table name. This must be a regular\nBigQuery table, and must have the [enable_change_history option](/bigquery/docs/reference/standard-sql/data-definition-language#table_option_list) set to `    TRUE` . Enabling this table option has an impact on costs; for\nmore information see [Pricing and costs](/bigquery/docs/change-history#pricing_and_costs) .\nThis argument must be preceded by the word `    TABLE` .\n-  `    start_timestamp` : a [TIMESTAMP](/bigquery/docs/reference/standard-sql/data-types#timestamp_type) value indicating the earliest\ntime at which a change is included in the output. If the value is `    NULL` , all\nchanges since the table creation are returned. If you set the `    enable_change_history` option after setting the `    start_timestamp` option,\nthe history before the enablement time might be incomplete.\nIf the table was created after\nthe `    start_timestamp` value, the actual table creation time is used instead.\nAn error is returned if the time specified is earlier than allowed by [time travel](/bigquery/docs/time-travel) , or\nif the table was created earlier than allowed by time travel if the `    start_timestamp` value is `    NULL` . For standard tables, this window is seven days,\nbut you can [configure the time travel window](/bigquery/docs/time-travel#configure_the_time_travel_window) to be less than\nthat.\n-  `    end_timestamp` : a `    TIMESTAMP` value indicating the latest time at\nwhich a change is included in the output. `    end_timestamp` is exclusive; for\nexample, if you specify `    2023-12-31 08:00:00` for `    start_timestamp` and `    2023-12-31 12:00:00` for `    end_timestamp` , all changes made from\n8 AM December 31, 2023 through 11:59 AM December 31, 2023 are returned. The `    end_timestamp` value must be at least ten minutes prior to the current\ntime. The maximum time range allowed between `    start_timestamp` and `    end_timestamp` is one day.\n\n **Details** \n\nIf a row is inserted, a record of the new row with an `INSERT` change type is\nproduced.\n\nIf a row is deleted, a record of the deleted row with a `DELETE` change type is\nproduced.\n\nIf a row is updated, a record of the old row with a `DELETE` change type and a\nrecord of the new row with an `UPDATE` change type are produced.\n\n **Output** \n\nThe `CHANGES` function returns a table with the following columns:\n\n- All columns of the input table at the time that the query is run. If a\ncolumn is added after the `    end_timestamp` value, it appears with `    NULL` values\npopulated in of the any rows that were changed before the addition of\nthe column.\n-  `    _CHANGE_TYPE` : a `    STRING` value indicating the type of change that produced\nthe row. For `    CHANGES` , the supported values are `    INSERT` , `    UPDATE` , and `    DELETE` .\n-  `    _CHANGE_TIMESTAMP` : a `    TIMESTAMP` value indicating the commit time of the\ntransaction that made the change.\n\n **Limitations** \n\n- The data returned by the `    CHANGES` function is limited to the time\ntravel window of the table.\n- The maximum allowed time range between the `    start_timestamp` and `    end_timestamp` arguments you specify for the function is one day.\n- The `    CHANGES` function can't query the last ten minutes of table history.\nTherefore, the `    end_timestamp` argument value must be at least ten minutes\nprior to the current time.\n- You can't call the `    CHANGES` function within a multi-statement transaction.\n- You can't use the `    CHANGES` function with tables that have had multi-statement\ntransactions committed to them within the requested time window.\n- You can only use the `    CHANGES` function with regular BigQuery tables.\nViews, materialized views, external tables, and wildcard tables aren't\nsupported.\n- For tables that have been cloned or snapshotted, and for tables that are\nrestored from a clone or snapshot, change history from the source table isn't\ncarried over to the new table, clone, or snapshot.\n- You can't use the `    CHANGES` function with a table that has [change data capture](/bigquery/docs/change-data-capture) enabled.\n- Partition pseudo-columns for ingestion-time partitioned tables, such as `    _PARTITIONTIME` and `    _PARTITIONDATE` , aren't included in the function's\noutput.\n- Change history isn't captured for table deletions made due to table partition\nexpiration.\n- Performing [data manipulation language (DML) statements over recently streamed data](/bigquery/docs/write-api#use_data_manipulation_language_dml_with_recently_streamed_data) fails on tables that have the `    enable_change_history` option set to `    TRUE` .\n\n **Example** \n\nThis example shows the change history returned by the `CHANGES` function as\nvarious changes are made to a table called `Produce` . First, create the table:\n\nInsert two rows into the table:\n\nDelete one row from the table:\n\nUpdate one row of the table:\n\nWait for 10 minutes and view the full change history of the changes:\n\nThe output is similar to the following:\n\n\n\n"
  },
  {
    "name": "CHARACTER_LENGTH",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nSynonym for [CHAR_LENGTH](#char_length) .\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "CHAR_LENGTH",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nGets the number of characters in a `STRING` value.\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "CHR",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes a Unicode [code point](https://en.wikipedia.org/wiki/Code_point) and returns\nthe character that matches the code point. Each valid code point should fall\nwithin the range of [0, 0xD7FF] and [0xE000, 0x10FFFF]. Returns an empty string\nif the code point is `0` . If an invalid Unicode code point is specified, an\nerror is returned.\n\nTo work with an array of Unicode code points, see [CODE_POINTS_TO_STRING](#code_points_to_string) \n\n **Return type** \n\n `STRING` \n\n **Examples** \n\n\n"
  },
  {
    "name": "CODE_POINTS_TO_BYTES",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes an array of extended ASCII [code points](https://en.wikipedia.org/wiki/Code_point) as `ARRAY&lt;INT64&gt;` and returns `BYTES` .\n\nTo convert from `BYTES` to an array of code points, see [TO_CODE_POINTS](#to_code_points) .\n\n **Return type** \n\n `BYTES` \n\n **Examples** \n\nThe following is a basic example using `CODE_POINTS_TO_BYTES` .\n\nThe following example uses a rotate-by-13 places (ROT13) algorithm to encode a\nstring.\n\n\n"
  },
  {
    "name": "CODE_POINTS_TO_STRING",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes an array of Unicode [code points](https://en.wikipedia.org/wiki/Code_point) as `ARRAY&lt;INT64&gt;` and returns a `STRING` .\n\nTo convert from a string to an array of code points, see [TO_CODE_POINTS](#to_code_points) .\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\nThe following are basic examples using `CODE_POINTS_TO_STRING` .\n\nThe following example computes the frequency of letters in a set of words.\n\n\n"
  },
  {
    "name": "COLLATE",
    "arguments": [],
    "category": "String",
    "description_markdown": "Takes a `STRING` and a [collation specification](/bigquery/docs/reference/standard-sql/collation-concepts#collate_spec_details) . Returns\na `STRING` with a collation specification. If `collate_specification` is empty,\nreturns a value with collation removed from the `STRING` .\n\nThe collation specification defines how the resulting `STRING` can be compared\nand sorted. To learn more, see [Working with collation](/bigquery/docs/reference/standard-sql/collation-concepts#working_with_collation) .\n\n-  `    collation_specification` must be a string literal, otherwise an error is\nthrown.\n- Returns `    NULL` if `    value` is `    NULL` .\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\nIn this example, the weight of `a` is less than the weight of `Z` . This\nis because the collate specification, `und:ci` assigns more weight to `Z` .\n\nIn this example, the weight of `a` is greater than the weight of `Z` . This\nis because the default collate specification assigns more weight to `a` .\n\n\n"
  },
  {
    "name": "CONCAT",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nConcatenates one or more values into a single result. All values must be `BYTES` or data types that can be cast to `STRING` .\n\nThe function returns `NULL` if any input argument is `NULL` .\n\n **Note:** You can also use the [|| concatenation operator](#operators) to concatenate\nvalues into a string. **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "CONTAINS_SUBSTR",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nPerforms a normalized, case-insensitive search to see if a value exists as a\nsubstring in an expression. Returns `TRUE` if the value exists, otherwise\nreturns `FALSE` .\n\nBefore values are compared, they are [normalized and case folded with NFKC normalization](#normalize_and_casefold) . Wildcard searches are not\nsupported.\n\n **Arguments** \n\n-  `    search_value_literal` : The value to search for. It must be a `    STRING` literal or a `    STRING` constant expression.\n-  `    expression` : The data to search over. The expression can be a column or\ntable reference. A table reference is evaluated as a `    STRUCT` whose fields\nare the columns of the table. A column reference is evaluated as one the\nfollowing data types:\n    \n    \n    -  `        STRING` \n    -  `        INT64` \n    -  `        BOOL` \n    -  `        NUMERIC` \n    -  `        BIGNUMERIC` \n    -  `        TIMESTAMP` \n    -  `        TIME` \n    -  `        DATE` \n    -  `        DATETIME` \n    -  `        ARRAY` \n    -  `        STRUCT` When the expression is evaluated, the result is cast to a `    STRING` , and then\nthe function looks for the search value in the result.\n    \n    You can perform a cross-field search on an expression that evaluates to a `    STRUCT` or `    ARRAY` . If the expression evaluates to a `    STRUCT` , the\ncross-field search is recursive and includes all subfields inside the `    STRUCT` .\n    \n    In a cross-field search, each field and subfield is individually converted\nto a string and searched for the value. The function returns `    TRUE` if at\nleast one field includes the search value; otherwise, if at least one field\nis `    NULL` , it returns `    NULL` ; otherwise, if the search value is not found\nand all fields are non- `    NULL` , it returns `    FALSE` .\n    \n    If the expression is `    NULL` , the return value is `    NULL` .\n    \n    \n-  `    json_scope` : A named argument with a `    STRING` value.\nTakes one of the following values to indicate the scope of `    JSON` data to be\nsearched. It has no effect if `    expression` is not `    JSON` or does not\ncontain a `    JSON` field.\n    \n    \n    -  `        'JSON_VALUES'` : Only the `        JSON` values are searched. If `        json_scope` is\nnot provided, this is used by default.\n    -  `        'JSON_KEYS'` : Only the `        JSON` keys are searched.\n    -  `        'JSON_KEYS_AND_VALUES'` : The `        JSON` keys and values are searched.\n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\nThe following query returns `TRUE` because this case-insensitive match\nwas found: `blue house` and `Blue house` .\n\nThe following query returns `TRUE` similar to the above example, but in this\ncase the search value is a constant expression with CONCAT function.\n\nThe following query returns `FALSE` because `blue` was not found\nin `the red house` .\n\nThe following query returns `TRUE` because `Ⅸ` and `IX` represent the same\nnormalized value.\n\nThe following query returns `TRUE` because `35` was found inside a `STRUCT` field.\n\nThe following query returns `TRUE` because `jk` was found during a\nrecursive search inside a `STRUCT` .\n\nThe following query returns `TRUE` because `NULL` s are ignored when\na match is found found inside a `STRUCT` or `ARRAY` .\n\nThe following query returns `NULL` because a `NULL` existed in a `STRUCT` that\ndid not result in a match.\n\nIn the following query, an error is thrown because the search value cannot be\na literal `NULL` .\n\nThe following examples reference a table called `Recipes` that you can emulate\nwith a `WITH` clause like this:\n\nThe following query searches across all columns of the `Recipes` table for the\nvalue `toast` and returns the rows that contain this value.\n\nThe following query searches the `Lunch` and `Dinner` columns of the `Recipe` table for the value `potato` and returns the row if either column\ncontains this value.\n\nThe following query searches across all columns of the `Recipes` table\nexcept for the `Lunch` and `Dinner` columns. It returns the rows of any\ncolumns other than `Lunch` or `Dinner` that contain the value `potato` .\n\nThe following query searches for the value `lunch` in the JSON `{\"lunch\":\"soup\"}` and returns `FALSE` because the default `json_scope` is `\"JSON_VALUES\"` , and `lunch` is a `JSON` key, not a `JSON` value.\n\nThe following query searches for the value `lunch` in the values of the JSON `{\"lunch\":\"soup\"}` and returns `FALSE` because `lunch` is a `JSON` key, not a `JSON` value.\n\nThe following query searches for the value `lunch` in the keys and values of the\nJSON `{\"lunch\":\"soup\"}` and returns `TRUE` because `lunch` is a `JSON` key.\n\nThe following query searches for the value `lunch` in the keys of the JSON `{\"lunch\":\"soup\"}` and returns `TRUE` because `lunch` is a `JSON` key.\n\n\n"
  },
  {
    "name": "CORR",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nReturns the [Pearson coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) of correlation of a set of number pairs. For each number pair, the first number\nis the dependent variable and the second number is the independent variable.\nThe return result is between `-1` and `1` . A result of `0` indicates no\ncorrelation.\n\nAll numeric types are supported. If the\ninput is `NUMERIC` or `BIGNUMERIC` then the internal aggregation is\nstable with the final output converted to a `FLOAT64` .\nOtherwise the input is converted to a `FLOAT64` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any input pairs that contain one or more `NULL` values. If\nthere are fewer than two input pairs without `NULL` values, this function\nreturns `NULL` .\n\n `NaN` is produced if:\n\n- Any input value is `    NaN` \n- Any input value is positive infinity or negative infinity.\n- The variance of `    X1` or `    X2` is `    0` .\n- The covariance of `    X1` and `    X2` is `    0` .\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "COS",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the cosine of X where X is specified in radians. Never fails.\n\n| X | COS(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "COSH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the hyperbolic cosine of X where X is specified in radians.\nGenerates an error if overflow occurs.\n\n| X | COSH(X) |\n| --- | --- |\n|  `+inf` |  `+inf` |\n|  `-inf` |  `+inf` |\n|  `NaN` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "COSINE_DISTANCE",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity#Cosine_distance) between two vectors.\n\n **Definitions** \n\n-  `    vector1` : A vector that is represented by an `    ARRAY&lt;T&gt;` value or a sparse vector that is\nrepresented by an `    ARRAY&lt;STRUCT&lt;dimension,magnitude&gt;&gt;` value.\n-  `    vector2` : A vector that is represented by an `    ARRAY&lt;T&gt;` value or a sparse vector that is\nrepresented by an `    ARRAY&lt;STRUCT&lt;dimension,magnitude&gt;&gt;` value.\n\n **Details** \n\n-  `    ARRAY&lt;T&gt;` can be used to represent a vector. Each zero-based index in this\narray represents a dimension. The value for each element in this array\nrepresents a magnitude.\n    \n     `    T` can represent the following and must be the same for both\nvectors:\n    \n    \n    -  `        FLOAT64` In the following example vector, there are four dimensions. The magnitude\nis `    10.0` for dimension `    0` , `    55.0` for dimension `    1` , `    40.0` for\ndimension `    2` , and `    34.0` for dimension `    3` :\n    \n    \n-  `    ARRAY&lt;STRUCT&lt;dimension,magnitude&gt;&gt;` can be used to represent a\nsparse vector. With a sparse vector, you only need to include\ndimension-magnitude pairs for non-zero magnitudes. If a magnitude isn't\npresent in the sparse vector, the magnitude is implicitly understood to be\nzero.\n    \n    For example, if you have a vector with 10,000 dimensions, but only 10\ndimensions have non-zero magnitudes, then the vector is a sparse vector.\nAs a result, it's more efficient to describe a sparse vector by only\nmentioning its non-zero magnitudes.\n    \n    In `    ARRAY&lt;STRUCT&lt;dimension,magnitude&gt;&gt;` , `    STRUCT&lt;dimension,magnitude&gt;` represents a dimension-magnitude pair for each non-zero magnitude in a\nsparse vector. These parts need to be included for each dimension-magnitude\npair:\n    \n    \n    -  `        dimension` : A `        STRING` or `        INT64` value that represents a\ndimension in a vector.\n        \n        \n    -  `        magnitude` : A `        FLOAT64` value that represents a\nnon-zero magnitude for a specific dimension in a vector.\n        \n        You don't need to include empty dimension-magnitude pairs in a\nsparse vector. For example, the following sparse vector and\nnon-sparse vector are equivalent:\n    \n    In a sparse vector, dimension-magnitude pairs don't need to be in any\nparticular order. The following sparse vectors are equivalent:\n    \n    \n- Both  non-sparse vectors\nin this function must share the same dimensions, and if they don't, an error\nis produced.\n    \n    \n- A vector can't be a zero vector. A vector is a zero vector if it has\nno dimensions or all dimensions have a magnitude of `    0` , such as `    []` or `    [0.0, 0.0]` . If a zero vector is encountered, an error is produced.\n    \n    \n- An error is produced if a magnitude in a vector is `    NULL` .\n    \n    \n- If a vector is `    NULL` , `    NULL` is returned.\n    \n    \n\n **Return type** \n\n `FLOAT64` \n\n **Examples** \n\nIn the following example, non-sparsevectors\nare used to compute the cosine distance:\n\nIn the following example, sparse vectors are used to compute the\ncosine distance:\n\nThe ordering of numeric values in a vector doesn't impact the results\nproduced by this function. For example these queries produce the same results\neven though the numeric values in each vector is in a different order:\n\nIn the following example, the function can't compute cosine distance against\nthe first vector, which is a zero vector:\n\nBoth non-sparse vectors must have the same\ndimensions. If not, an error is produced. In the following example, the\nfirst vector has two dimensions and the second vector has three:\n\nIf you use sparse vectors and you repeat a dimension, an error is\nproduced:\n\n\n"
  },
  {
    "name": "COT",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the cotangent for the angle of `X` , where `X` is specified in radians. `X` can be any data type\nthat [coerces to FLOAT64](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the `SAFE.` prefix.\n\n| X | COT(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n|  `0` |  `Error` |\n|  `NULL` |  `NULL` |\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "COTH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the hyperbolic cotangent for the angle of `X` , where `X` is specified\nin radians. `X` can be any data type\nthat [coerces to FLOAT64](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the `SAFE.` prefix.\n\n| X | COTH(X) |\n| --- | --- |\n|  `+inf` |  `1` |\n|  `-inf` |  `-1` |\n|  `NaN` |  `NaN` |\n|  `0` |  `Error` |\n|  `NULL` |  `NULL` |\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "COUNT",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": "1.\n\n2.\n\n **Description** \n\n1. Returns the number of rows in the input.\n1. Returns the number of rows with `    expression` evaluated to any value other\nthan `    NULL` .\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nThis function with DISTINCT supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n `COUNT` can be used with differential privacy. For more information, see [Differentially private aggregate functions](#aggregate-dp-functions) .\n\n **Supported Argument Types** \n\n `expression` can be any data type. If `DISTINCT` is present, `expression` can only be a data type that is [groupable](/bigquery/docs/reference/standard-sql/data-types#data_type_properties) .\n\n **Return Data Types** \n\nINT64\n\n **Examples** \n\nYou can use the `COUNT` function to return the number of rows in a table or the\nnumber of distinct values of an expression. For example:\n\nIf you want to count the number of distinct values of an expression for which a\ncertain condition is satisfied, this is one recipe that you can use:\n\nHere, `IF` will return the value of `expression` if `condition` is `TRUE` , or `NULL` otherwise. The surrounding `COUNT(DISTINCT ...)` will ignore the `NULL` values, so it will count only the distinct values of `expression` for which `condition` is `TRUE` .\n\nFor example, to count the number of distinct positive values of `x` :\n\nOr to count the number of distinct dates on which a certain kind of event\noccurred:\n\n\n"
  },
  {
    "name": "COUNTIF",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns the count of `TRUE` values for `expression` . Returns `0` if there are\nzero input rows, or if `expression` evaluates to `FALSE` or `NULL` for all rows.\n\nSince `expression` must be a `BOOL` , the form `COUNTIF(DISTINCT ...)` is\ngenerally not useful: there is only one distinct value of `TRUE` . So `COUNTIF(DISTINCT ...)` will return 1 if `expression` evaluates to `TRUE` for\none or more input rows, or 0 otherwise.\nUsually when someone wants to combine `COUNTIF` and `DISTINCT` , they\nwant to count the number of distinct values of an expression for which a certain\ncondition is satisfied. One recipe to achieve this is the following:\n\nNote that this uses `COUNT` , not `COUNTIF` ; the `IF` part has been moved inside.\nTo learn more, see the examples for [COUNT](#count) .\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\nBOOL\n\n **Return Data Types** \n\nINT64\n\n **Examples** \n\n\n"
  },
  {
    "name": "COVAR_POP",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nReturns the population [covariance](https://en.wikipedia.org/wiki/Covariance) of\na set of number pairs. The first number is the dependent variable; the second\nnumber is the independent variable. The return result is between `-Inf` and `+Inf` .\n\nAll numeric types are supported. If the\ninput is `NUMERIC` or `BIGNUMERIC` then the internal aggregation is\nstable with the final output converted to a `FLOAT64` .\nOtherwise the input is converted to a `FLOAT64` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any input pairs that contain one or more `NULL` values. If\nthere is no input pair without `NULL` values, this function returns `NULL` .\nIf there is exactly one input pair without `NULL` values, this function returns `0` .\n\n `NaN` is produced if:\n\n- Any input value is `    NaN` \n- Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "COVAR_SAMP",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nReturns the sample [covariance](https://en.wikipedia.org/wiki/Covariance) of a\nset of number pairs. The first number is the dependent variable; the second\nnumber is the independent variable. The return result is between `-Inf` and `+Inf` .\n\nAll numeric types are supported. If the\ninput is `NUMERIC` or `BIGNUMERIC` then the internal aggregation is\nstable with the final output converted to a `FLOAT64` .\nOtherwise the input is converted to a `FLOAT64` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any input pairs that contain one or more `NULL` values. If\nthere are fewer than two input pairs without `NULL` values, this function\nreturns `NULL` .\n\n `NaN` is produced if:\n\n- Any input value is `    NaN` \n- Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "CSC",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the cosecant of the input angle, which is in radians. `X` can be any data type\nthat [coerces to FLOAT64](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the `SAFE.` prefix.\n\n| X | CSC(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n|  `0` |  `Error` |\n|  `NULL` |  `NULL` |\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "CSCH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the hyperbolic cosecant of the input angle, which is in radians. `X` can be any data type\nthat [coerces to FLOAT64](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the `SAFE.` prefix.\n\n| X | CSCH(X) |\n| --- | --- |\n|  `+inf` |  `0` |\n|  `-inf` |  `0` |\n|  `NaN` |  `NaN` |\n|  `0` |  `Error` |\n|  `NULL` |  `NULL` |\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "CUME_DIST",
    "arguments": [],
    "category": "Numbering",
    "description_markdown": " **Description** \n\nReturn the relative rank of a row defined as NP/NR. NP is defined to be the\nnumber of rows that either precede or are peers with the current row. NR is the\nnumber of rows in the partition.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "CURRENT_DATE",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nReturns the current date as a `DATE` object. Parentheses are optional when\ncalled with no arguments.\n\nThis function supports the following arguments:\n\n-  `    time_zone_expression` : A `    STRING` expression that represents a [time zone](#timezone_definitions) . If no time zone is specified, the\ndefault time zone, UTC, is used. If this expression is\nused and it evaluates to `    NULL` , this function returns `    NULL` .\n\nThe current date is recorded at the start of the query\nstatement which contains this function, not when this specific function is\nevaluated.\n\n **Return Data Type** \n\n `DATE` \n\n **Examples** \n\nThe following query produces the current date in the default time zone:\n\nThe following queries produce the current date in a specified time zone:\n\nThe following query produces the current date in the default time zone.\nParentheses are not needed if the function has no arguments.\n\n\n"
  },
  {
    "name": "CURRENT_DATETIME",
    "arguments": [],
    "category": "Datetime",
    "description_markdown": " **Description** \n\nReturns the current time as a `DATETIME` object. Parentheses are optional when\ncalled with no arguments.\n\nThis function supports an optional `time_zone` parameter.\nSee [Time zone definitions](#timezone_definitions) for\ninformation on how to specify a time zone.\n\nThe current date and time is recorded at the start of the query\nstatement which contains this function, not when this specific function is\nevaluated.\n\n **Return Data Type** \n\n `DATETIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "CURRENT_TIME",
    "arguments": [],
    "category": "Time",
    "description_markdown": " **Description** \n\nReturns the current time as a `TIME` object. Parentheses are optional when\ncalled with no arguments.\n\nThis function supports an optional `time_zone` parameter.\nSee [Time zone definitions](#timezone_definitions) for information\non how to specify a time zone.\n\nThe current time is recorded at the start of the query\nstatement which contains this function, not when this specific function is\nevaluated.\n\n **Return Data Type** \n\n `TIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "CURRENT_TIMESTAMP",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nReturns the current date and time as a timestamp object. The timestamp is\ncontinuous, non-ambiguous, has exactly 60 seconds per minute and does not repeat\nvalues over the leap second. Parentheses are optional.\n\nThis function handles leap seconds by smearing them across a window of 20 hours\naround the inserted leap second.\n\nThe current date and time is recorded at the start of the query\nstatement which contains this function, not when this specific function is\nevaluated.\n\n **Supported Input Types** \n\nNot applicable\n\n **Result Data Type** \n\n `TIMESTAMP` \n\n **Examples** \n\n\n"
  },
  {
    "name": "DATE",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nConstructs or extracts a date.\n\nThis function supports the following arguments:\n\n-  `    year` : The `    INT64` value for year.\n-  `    month` : The `    INT64` value for month.\n-  `    day` : The `    INT64` value for day.\n-  `    timestamp_expression` : A `    TIMESTAMP` expression that contains the date.\n-  `    time_zone_expression` : A `    STRING` expression that represents a [time zone](#timezone_definitions) . If no time zone is specified with `    timestamp_expression` , the default time zone, UTC, is\nused.\n-  `    datetime_expression` : A `    DATETIME` expression that contains the date.\n\n **Return Data Type** \n\n `DATE` \n\n **Example** \n\n\n"
  },
  {
    "name": "DATETIME",
    "arguments": [],
    "category": "Datetime",
    "description_markdown": " **Description** \n\n1. Constructs a `    DATETIME` object using `    INT64` values\nrepresenting the year, month, day, hour, minute, and second.\n1. Constructs a `    DATETIME` object using a DATE object and an optional `    TIME` object.\n1. Constructs a `    DATETIME` object using a `    TIMESTAMP` object. It supports an\noptional parameter to [specify a time zone](#timezone_definitions) .\nIf no time zone is specified, the default time zone, UTC,\nis used.\n\n **Return Data Type** \n\n `DATETIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "DATETIME_ADD",
    "arguments": [],
    "category": "Datetime",
    "description_markdown": " **Description** \n\nAdds `int64_expression` units of `part` to the `DATETIME` object.\n\n `DATETIME_ADD` supports the following values for `part` :\n\n-  `    MICROSECOND` \n-  `    MILLISECOND` \n-  `    SECOND` \n-  `    MINUTE` \n-  `    HOUR` \n-  `    DAY` \n-  `    WEEK` . Equivalent to 7 `    DAY` s.\n-  `    MONTH` \n-  `    QUARTER` \n-  `    YEAR` \n\nSpecial handling is required for MONTH, QUARTER, and YEAR parts when the\ndate is at (or near) the last day of the month. If the resulting month has fewer\ndays than the original DATETIME's day, then the result day is the last day of\nthe new month.\n\n **Return Data Type** \n\n `DATETIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "DATETIME_BUCKET",
    "arguments": [],
    "category": "Time_series",
    "description_markdown": " **Description** \n\nGets the lower bound of the datetime bucket that contains a datetime.\n\n **Definitions** \n\n-  `    datetime_in_bucket` : A `    DATETIME` value that you can use to look up a\ndatetime bucket.\n-  `    bucket_width` : An `    INTERVAL` value that represents the width of\na datetime bucket. A [single interval](/bigquery/docs/reference/standard-sql/data-types#single_datetime_part_interval) with [date and time parts](/bigquery/docs/reference/standard-sql/data-types#interval_datetime_parts) is supported.\n-  `    bucket_origin_datetime` : A `    DATETIME` value that represents a point in\ntime. All buckets expand left and right from this point. If this argument\nis not set, `    1950-01-01 00:00:00` is used by default.\n\n **Return type** \n\n `DATETIME` \n\n **Examples** \n\nIn the following example, the origin is omitted and the default origin, `1950-01-01 00:00:00` is used. All buckets expand in both directions from the\norigin, and the size of each bucket is 12 hours. The lower bound of the bucket\nin which `my_datetime` belongs is returned:\n\nIn the following example, the origin has been changed to `2000-12-24 12:00:00` ,\nand all buckets expand in both directions from this point. The size of each\nbucket is seven days. The lower bound of the bucket in which `my_datetime` belongs is returned:\n\n\n"
  },
  {
    "name": "DATETIME_DIFF",
    "arguments": [],
    "category": "Datetime",
    "description_markdown": " **Description** \n\nGets the number of unit boundaries between two `DATETIME` values\n( `end_datetime` - `start_datetime` ) at a particular time granularity.\n\n **Definitions** \n\n-  `    start_datetime` : The starting `    DATETIME` value.\n-  `    end_datetime` : The ending `    DATETIME` value.\n-  `    granularity` : The datetime part that represents the granularity. If\nyou have passed in `    DATETIME` values for the first arguments, `    granularity` can be:\n    \n    \n    -  `        MICROSECOND` \n    -  `        MILLISECOND` \n    -  `        SECOND` \n    -  `        MINUTE` \n    -  `        HOUR` \n    -  `        DAY` \n    -  `        WEEK` : This date part begins on Sunday.\n    -  `        WEEK(&lt;WEEKDAY&gt;)` : This date part begins on `        WEEKDAY` . Valid values for `        WEEKDAY` are `        SUNDAY` , `        MONDAY` , `        TUESDAY` , `        WEDNESDAY` , `        THURSDAY` , `        FRIDAY` , and `        SATURDAY` .\n    -  `        ISOWEEK` : Uses [ISO 8601 week](https://en.wikipedia.org/wiki/ISO_week_date) boundaries. ISO weeks begin on Monday.\n    -  `        MONTH` \n    -  `        QUARTER` \n    -  `        YEAR` \n    -  `        ISOYEAR` : Uses the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year boundary. The ISO year boundary is the Monday of the\nfirst week whose Thursday belongs to the corresponding Gregorian calendar\nyear.\n\n **Details** \n\nIf `end_datetime` is earlier than `start_datetime` , the output is negative.\nProduces an error if the computation overflows, such as if the difference\nin microseconds\nbetween the two `DATETIME` values overflows.\n\n **Note:** The behavior of the this function follows the type of arguments passed in.\nFor example, `DATETIME_DIFF(TIMESTAMP, TIMESTAMP, PART)` behaves like `TIMESTAMP_DIFF(TIMESTAMP, TIMESTAMP, PART)` . **Return Data Type** \n\n `INT64` \n\n **Example** \n\nThe example above shows the result of `DATETIME_DIFF` for two `DATETIME` s that\nare 24 hours apart. `DATETIME_DIFF` with the part `WEEK` returns 1 because `DATETIME_DIFF` counts the number of part boundaries in this range of `DATETIME` s. Each `WEEK` begins on Sunday, so there is one part boundary between\nSaturday, `2017-10-14 00:00:00` and Sunday, `2017-10-15 00:00:00` .\n\nThe following example shows the result of `DATETIME_DIFF` for two dates in\ndifferent years. `DATETIME_DIFF` with the date part `YEAR` returns 3 because it\ncounts the number of Gregorian calendar year boundaries between the two `DATETIME` s. `DATETIME_DIFF` with the date part `ISOYEAR` returns 2 because the\nsecond `DATETIME` belongs to the ISO year 2015. The first Thursday of the 2015\ncalendar year was 2015-01-01, so the ISO year 2015 begins on the preceding\nMonday, 2014-12-29.\n\nThe following example shows the result of `DATETIME_DIFF` for two days in\nsuccession. The first date falls on a Monday and the second date falls on a\nSunday. `DATETIME_DIFF` with the date part `WEEK` returns 0 because this time\npart uses weeks that begin on Sunday. `DATETIME_DIFF` with the date part `WEEK(MONDAY)` returns 1. `DATETIME_DIFF` with the date part `ISOWEEK` also returns 1 because ISO weeks begin on Monday.\n\n\n"
  },
  {
    "name": "DATETIME_SUB",
    "arguments": [],
    "category": "Datetime",
    "description_markdown": " **Description** \n\nSubtracts `int64_expression` units of `part` from the `DATETIME` .\n\n `DATETIME_SUB` supports the following values for `part` :\n\n-  `    MICROSECOND` \n-  `    MILLISECOND` \n-  `    SECOND` \n-  `    MINUTE` \n-  `    HOUR` \n-  `    DAY` \n-  `    WEEK` . Equivalent to 7 `    DAY` s.\n-  `    MONTH` \n-  `    QUARTER` \n-  `    YEAR` \n\nSpecial handling is required for `MONTH` , `QUARTER` , and `YEAR` parts when the\ndate is at (or near) the last day of the month. If the resulting month has fewer\ndays than the original `DATETIME` 's day, then the result day is the last day of\nthe new month.\n\n **Return Data Type** \n\n `DATETIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "DATETIME_TRUNC",
    "arguments": [],
    "category": "Datetime",
    "description_markdown": " **Description** \n\nTruncates a `DATETIME` value at a particular time granularity. The `DATETIME` value is always rounded to the beginning of `granularity` .\n\n **Definitions** \n\n-  `    datetime_expression` : The `    DATETIME` value to truncate.\n-  `    granularity` : The datetime part that represents the granularity. If\nyou passed in a `    DATETIME` value for the first argument, `    granularity` can\nbe:\n    \n    \n    -  `        MICROSECOND` : If used, nothing is truncated from the value.\n        \n        \n    -  `        MILLISECOND` : The nearest lesser than or equal millisecond.\n        \n        \n    -  `        SECOND` : The nearest lesser than or equal second.\n        \n        \n    -  `        MINUTE` : The nearest lesser than or equal minute.\n        \n        \n    -  `        HOUR` : The nearest lesser than or equal hour.\n        \n        \n    -  `        DAY` : The day in the Gregorian calendar year that contains the `        DATETIME` value.\n        \n        \n    -  `        WEEK` : The first day in the week that contains the `        DATETIME` value. Weeks begin on Sundays. `        WEEK` is equivalent to `        WEEK(SUNDAY)` .\n        \n        \n    -  `        WEEK(WEEKDAY)` : The first day in the week that contains the `        DATETIME` value. Weeks begin on `        WEEKDAY` . `        WEEKDAY` must be one of the\nfollowing: `        SUNDAY` , `        MONDAY` , `        TUESDAY` , `        WEDNESDAY` , `        THURSDAY` , `        FRIDAY` ,\nor `        SATURDAY` .\n        \n        \n    -  `        ISOWEEK` : The first day in the [ISO 8601 week](https://en.wikipedia.org/wiki/ISO_week_date) that contains\nthe `        DATETIME` value. The ISO week begins on\nMonday. The first ISO week of each ISO year contains the first Thursday of the\ncorresponding Gregorian calendar year.\n        \n        \n    -  `        MONTH` : The first day in the month that contains the `        DATETIME` value.\n        \n        \n    -  `        QUARTER` : The first day in the quarter that contains the `        DATETIME` value.\n        \n        \n    -  `        YEAR` : The first day in the year that contains the `        DATETIME` value.\n        \n        \n    -  `        ISOYEAR` : The first day in the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year\nthat contains the `        DATETIME` value. The ISO year is the\nMonday of the first week where Thursday belongs to the corresponding\nGregorian calendar year.\n        \n        \n\n **Return Data Type** \n\n `DATETIME` \n\n **Examples** \n\nIn the following example, the original `DATETIME` falls on a Sunday. Because the `part` is `WEEK(MONDAY)` , `DATE_TRUNC` returns the `DATETIME` for the\npreceding Monday.\n\nIn the following example, the original `datetime_expression` is in the Gregorian\ncalendar year 2015. However, `DATETIME_TRUNC` with the `ISOYEAR` date part\ntruncates the `datetime_expression` to the beginning of the ISO year, not the\nGregorian calendar year. The first Thursday of the 2015 calendar year was\n2015-01-01, so the ISO year 2015 begins on the preceding Monday, 2014-12-29.\nTherefore the ISO year boundary preceding the `datetime_expression` 2015-06-15 00:00:00 is 2014-12-29.\n\n\n"
  },
  {
    "name": "DATE_ADD",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nAdds a specified time interval to a DATE.\n\n `DATE_ADD` supports the following `date_part` values:\n\n-  `    DAY` \n-  `    WEEK` . Equivalent to 7 `    DAY` s.\n-  `    MONTH` \n-  `    QUARTER` \n-  `    YEAR` \n\nSpecial handling is required for MONTH, QUARTER, and YEAR parts when\nthe date is at (or near) the last day of the month. If the resulting\nmonth has fewer days than the original date's day, then the resulting\ndate is the last date of that month.\n\n **Return Data Type** \n\nDATE\n\n **Example** \n\n\n"
  },
  {
    "name": "DATE_BUCKET",
    "arguments": [],
    "category": "Time_series",
    "description_markdown": " **Description** \n\nGets the lower bound of the date bucket that contains a date.\n\n **Definitions** \n\n-  `    date_in_bucket` : A `    DATE` value that you can use to look up a date bucket.\n-  `    bucket_width` : An `    INTERVAL` value that represents the width of\na date bucket. A [single interval](/bigquery/docs/reference/standard-sql/data-types#single_datetime_part_interval) with [date parts](/bigquery/docs/reference/standard-sql/data-types#interval_datetime_parts) is supported.\n-  `    bucket_origin_date` : A `    DATE` value that represents a point in time. All\nbuckets expand left and right from this point. If this argument is not set, `    1950-01-01` is used by default.\n\n **Return type** \n\n `DATE` \n\n **Examples** \n\nIn the following example, the origin is omitted and the default origin, `1950-01-01` is used. All buckets expand in both directions from the origin,\nand the size of each bucket is two days. The lower bound of the bucket in\nwhich `my_date` belongs is returned.\n\nIn the following example, the origin has been changed to `2000-12-24` ,\nand all buckets expand in both directions from this point. The size of each\nbucket is seven days. The lower bound of the bucket in which `my_date` belongs\nis returned:\n\n\n"
  },
  {
    "name": "DATE_DIFF",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nGets the number of unit boundaries between two `DATE` values ( `end_date` - `start_date` ) at a particular time granularity.\n\n **Definitions** \n\n-  `    start_date` : The starting `    DATE` value.\n-  `    end_date` : The ending `    DATE` value.\n-  `    granularity` : The date part that represents the granularity. If\nyou have passed in `    DATE` values for the first arguments, `    granularity` can\nbe:\n    \n    \n    -  `        DAY` \n    -  `        WEEK` This date part begins on Sunday.\n    -  `        WEEK(&lt;WEEKDAY&gt;)` : This date part begins on `        WEEKDAY` . Valid values for `        WEEKDAY` are `        SUNDAY` , `        MONDAY` , `        TUESDAY` , `        WEDNESDAY` , `        THURSDAY` , `        FRIDAY` , and `        SATURDAY` .\n    -  `        ISOWEEK` : Uses [ISO 8601 week](https://en.wikipedia.org/wiki/ISO_week_date) boundaries. ISO weeks\nbegin on Monday.\n    -  `        MONTH` \n    -  `        QUARTER` \n    -  `        YEAR` \n    -  `        ISOYEAR` : Uses the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year boundary.\nThe ISO year boundary is the Monday of the first week whose Thursday\nbelongs to the corresponding Gregorian calendar year.\n\n **Details** \n\nIf `end_date` is earlier than `start_date` , the output is negative.\n\n **Note:** The behavior of the this function follows the type of arguments passed in.\nFor example, `DATE_DIFF(TIMESTAMP, TIMESTAMP, PART)` behaves like `TIMESTAMP_DIFF(TIMESTAMP, TIMESTAMP, PART)` . **Return Data Type** \n\n `INT64` \n\n **Example** \n\nThe example above shows the result of `DATE_DIFF` for two days in succession. `DATE_DIFF` with the date part `WEEK` returns 1 because `DATE_DIFF` counts the\nnumber of date part boundaries in this range of dates. Each `WEEK` begins on\nSunday, so there is one date part boundary between Saturday, 2017-10-14\nand Sunday, 2017-10-15.\n\nThe following example shows the result of `DATE_DIFF` for two dates in different\nyears. `DATE_DIFF` with the date part `YEAR` returns 3 because it counts the\nnumber of Gregorian calendar year boundaries between the two dates. `DATE_DIFF` with the date part `ISOYEAR` returns 2 because the second date belongs to the\nISO year 2015. The first Thursday of the 2015 calendar year was 2015-01-01, so\nthe ISO year 2015 begins on the preceding Monday, 2014-12-29.\n\nThe following example shows the result of `DATE_DIFF` for two days in\nsuccession. The first date falls on a Monday and the second date falls on a\nSunday. `DATE_DIFF` with the date part `WEEK` returns 0 because this date part\nuses weeks that begin on Sunday. `DATE_DIFF` with the date part `WEEK(MONDAY)` returns 1. `DATE_DIFF` with the date part `ISOWEEK` also returns 1 because\nISO weeks begin on Monday.\n\n\n"
  },
  {
    "name": "DATE_FROM_UNIX_DATE",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nInterprets `int64_expression` as the number of days since 1970-01-01.\n\n **Return Data Type** \n\nDATE\n\n **Example** \n\n\n"
  },
  {
    "name": "DATE_SUB",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nSubtracts a specified time interval from a DATE.\n\n `DATE_SUB` supports the following `date_part` values:\n\n-  `    DAY` \n-  `    WEEK` . Equivalent to 7 `    DAY` s.\n-  `    MONTH` \n-  `    QUARTER` \n-  `    YEAR` \n\nSpecial handling is required for MONTH, QUARTER, and YEAR parts when\nthe date is at (or near) the last day of the month. If the resulting\nmonth has fewer days than the original date's day, then the resulting\ndate is the last date of that month.\n\n **Return Data Type** \n\nDATE\n\n **Example** \n\n\n"
  },
  {
    "name": "DATE_TRUNC",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nTruncates a `DATE` value at a particular time granularity. The `DATE` value\nis always rounded to the beginning of `granularity` .\n\n **Definitions** \n\n-  `    date_expression` : The `    DATE` value to truncate.\n-  `    granularity` : The date part that represents the granularity. If\nyou passed in a `    DATE` value for the first argument, `    granularity` can\nbe:\n    \n    \n    -  `        DAY` : The day in the Gregorian calendar year that contains the `        DATE` value.\n        \n        \n    -  `        WEEK` : The first day in the week that contains the `        DATE` value. Weeks begin on Sundays. `        WEEK` is equivalent to `        WEEK(SUNDAY)` .\n        \n        \n    -  `        WEEK(WEEKDAY)` : The first day in the week that contains the `        DATE` value. Weeks begin on `        WEEKDAY` . `        WEEKDAY` must be one of the\nfollowing: `        SUNDAY` , `        MONDAY` , `        TUESDAY` , `        WEDNESDAY` , `        THURSDAY` , `        FRIDAY` ,\nor `        SATURDAY` .\n        \n        \n    -  `        ISOWEEK` : The first day in the [ISO 8601 week](https://en.wikipedia.org/wiki/ISO_week_date) that contains\nthe `        DATE` value. The ISO week begins on\nMonday. The first ISO week of each ISO year contains the first Thursday of the\ncorresponding Gregorian calendar year.\n        \n        \n    -  `        MONTH` : The first day in the month that contains the `        DATE` value.\n        \n        \n    -  `        QUARTER` : The first day in the quarter that contains the `        DATE` value.\n        \n        \n    -  `        YEAR` : The first day in the year that contains the `        DATE` value.\n        \n        \n    -  `        ISOYEAR` : The first day in the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year\nthat contains the `        DATE` value. The ISO year is the\nMonday of the first week where Thursday belongs to the corresponding\nGregorian calendar year.\n        \n        \n\n **Return Data Type** \n\n `DATE` \n\n **Examples** \n\nIn the following example, the original date falls on a Sunday. Because\nthe `date_part` is `WEEK(MONDAY)` , `DATE_TRUNC` returns the `DATE` for the\npreceding Monday.\n\nIn the following example, the original `date_expression` is in the Gregorian\ncalendar year 2015. However, `DATE_TRUNC` with the `ISOYEAR` date part\ntruncates the `date_expression` to the beginning of the ISO year, not the\nGregorian calendar year. The first Thursday of the 2015 calendar year was\n2015-01-01, so the ISO year 2015 begins on the preceding Monday, 2014-12-29.\nTherefore the ISO year boundary preceding the `date_expression` 2015-06-15 is\n2014-12-29.\n\n\n"
  },
  {
    "name": "DENSE_RANK",
    "arguments": [],
    "category": "Numbering",
    "description_markdown": " **Description** \n\nReturns the ordinal (1-based) rank of each row within the window partition.\nAll peer rows receive the same rank value, and the subsequent rank value is\nincremented by one.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "DETERMINISTIC_DECRYPT_BYTES",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nUses the matching key from `keyset` to decrypt `ciphertext` and verifies the\nintegrity of the data using `additional_data` . Returns an error if decryption\nfails.\n\n `keyset` is a serialized `BYTES` value or a `STRUCT` value returned by one of the `KEYS` functions. `keyset` must contain\nthe key that was used to encrypt `ciphertext` , the key must be in an `'ENABLED'` state, and the key must be of type `DETERMINISTIC_AEAD_AES_SIV_CMAC_256` , or\nelse the function returns an error. `DETERMINISTIC_DECRYPT_BYTES` identifies the\nmatching key in `keyset` by finding the key with the key ID that matches the one\nencrypted in `ciphertext` .\n\n `ciphertext` is a `BYTES` value that is the result of a call to `DETERMINISTIC_ENCRYPT` where the input `plaintext` was of type `BYTES` .\n\nThe ciphertext must follow Tink's [wire format](https://developers.google.com/tink/wire-format#deterministic_aead) . The first\nbyte of `ciphertext` should contain a Tink key version followed by a 4 byte key\nhint. If `ciphertext` includes an initialization vector (IV), it should be the\nnext bytes of `ciphertext` . If `ciphertext` includes an authentication tag, it\nshould be the last bytes of `ciphertext` . If the IV and authentic tag are one\n(SIV), it should be the first bytes of `ciphertext` . The IV and authentication\ntag commonly require 16 bytes, but may vary in size.\n\n `additional_data` is a `STRING` or `BYTES` value that binds the ciphertext to\nits context. This forces the ciphertext to be decrypted in the same context in\nwhich it was encrypted. This function casts any `STRING` value to `BYTES` . This\nmust be the same as the `additional_data` provided to `DETERMINISTIC_ENCRYPT` to\nencrypt `ciphertext` , ignoring its type, or else the function returns an error.\n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nThis example creates a table of unique IDs with associated plaintext values and\nkeysets. Then it uses these keysets to encrypt the plaintext values as `BYTES` and store them in a new table. Finally, it uses `DETERMINISTIC_DECRYPT_BYTES` to\ndecrypt the encrypted values and display them as plaintext.\n\nThe following statement creates a table `CustomerKeysets` containing a column of\nunique IDs, a column of `DETERMINISTIC_AEAD_AES_SIV_CMAC_256` keysets, and a\ncolumn of favorite animals.\n\nThe following statement creates a table `EncryptedCustomerData` containing a\ncolumn of unique IDs and a column of ciphertext. The statement encrypts the\nplaintext `favorite_animal` using the keyset value from `CustomerKeysets` corresponding to each unique ID.\n\nThe following query uses the keysets in the `CustomerKeysets` table to decrypt\ndata in the `EncryptedCustomerData` table.\n\n\n"
  },
  {
    "name": "DETERMINISTIC_DECRYPT_STRING",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nLike [DETERMINISTIC_DECRYPT_BYTES](#deterministic_decrypt_bytes) , but where `plaintext` is of type `STRING` .\n\n **Return Data Type** \n\n `STRING` \n\n\n\n"
  },
  {
    "name": "DETERMINISTIC_ENCRYPT",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nEncrypts `plaintext` using the primary cryptographic key in `keyset` using [deterministic AEAD](https://developers.google.com/tink/deterministic-aead) . The algorithm of the primary key must\nbe `DETERMINISTIC_AEAD_AES_SIV_CMAC_256` . Binds the ciphertext to the context\ndefined by `additional_data` . Returns `NULL` if any input is `NULL` .\n\n `keyset` is a serialized `BYTES` value or a `STRUCT` value returned by one of the `KEYS` functions.\n\n `plaintext` is the `STRING` or `BYTES` value to be encrypted.\n\n `additional_data` is a `STRING` or `BYTES` value that binds the ciphertext to\nits context. This forces the ciphertext to be decrypted in the same context in\nwhich it was encrypted. `plaintext` and `additional_data` must be of the same\ntype. `DETERMINISTIC_ENCRYPT(keyset, string1, string2)` is equivalent to `DETERMINISTIC_ENCRYPT(keyset, CAST(string1 AS BYTES), CAST(string2 AS BYTES))` .\n\nThe output is ciphertext `BYTES` . The ciphertext contains a [Tink-specific](https://github.com/google/tink/blob/master/docs/KEY-MANAGEMENT.md) prefix indicating the key used to perform the encryption.\nGiven an identical `keyset` and `plaintext` , this function returns the same\nciphertext each time it is invoked (including across queries).\n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nThe following query uses the keysets for each `customer_id` in the `CustomerKeysets` table to encrypt the value of the plaintext `favorite_animal` in the `PlaintextCustomerData` table corresponding to that `customer_id` . The\noutput contains a column of `customer_id` values and a column of corresponding\nciphertext output as `BYTES` .\n\n\n"
  },
  {
    "name": "DIV",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns the result of integer division of X by Y. Division by zero returns\nan error. Division by -1 may overflow.\n\n| X | Y | DIV(X, Y) |\n| --- | --- | --- |\n| 20 | 4 | 5 |\n| 12 | -7 | -1 |\n| 20 | 3 | 6 |\n| 0 | 20 | 0 |\n| 20 | 0 | Error |\n\n **Return Data Type** \n\nThe return data type is determined by the argument types with the following\ntable.\n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |\n| --- | --- | --- | --- |\n|  `INT64` |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |\n|  `NUMERIC` |  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |\n\n\n\n"
  },
  {
    "name": "DLP_DETERMINISTIC_DECRYPT",
    "arguments": [],
    "category": "DLP_encryption",
    "description_markdown": " **Description** \n\nThis function decrypts `ciphertext` using an encryption key derived from `key` and `context` . You can use `surrogate` to prepend the decryption\nresult. To use DLP functions, you need a [new cryptographic key and then use that key to get a wrapped key](/bigquery/docs/column-key-encrypt#wrapped-key-dlp-functions) .\n\n **Definitions** \n\n-  `    key` : A serialized `    BYTES` value returned by [DLP_KEY_CHAIN](#dlp_key_chain) . `    key` must be set to `    ENABLED` in Cloud KMS. For\ninformation about how to generate a wrapped key, see [gcloud kms encrypt](https://cloud.google.com/sdk/gcloud/reference/kms/encrypt) .\n-  `    ciphertext` : The `    STRING` value to decrypt.\n-  `    surrogate` : A `    STRING` value that you can prepend to output. If you don't\nwant to use `    surrogate` , pass an empty string (enclosed in `    \"\"` ).\n-  `    context` : A `    STRING` value that is used with a\nCloud KMS key to derive a data encryption key. For more information,\nsee [CryptoDeterministicConfig:context](https://cloud.google.com/dlp/docs/reference/rest/v2/projects.deidentifyTemplates#cryptodeterministicconfig) .\n\n **Return data type** \n\n `STRING` \n\n **Examples** \n\nIn the following query, the wrapped key is presented in a `BYTES` literal format:\n\nIn the following query, the wrapped key is presented in the base64 format:\n\n\n"
  },
  {
    "name": "DLP_DETERMINISTIC_ENCRYPT",
    "arguments": [],
    "category": "DLP_encryption",
    "description_markdown": " **Description** \n\nThis function derives a data encryption key from `key` and `context` , and then\nencrypts `plaintext` . You can use `surrogate` to prepend the\nencryption result. To use DLP functions, you need a [new cryptographic key and then use that key to get a wrapped key](/bigquery/docs/column-key-encrypt#wrapped-key-dlp-functions) .\n\n **Definitions** \n\n-  `    key` : A serialized `    BYTES` value that is returned by [DLP_KEY_CHAIN](#dlp_key_chain) . `    key` must be set to `    ENABLED` in Cloud KMS. For\ninformation about how to generate a wrapped key, see [gcloud kms encrypt](https://cloud.google.com/sdk/gcloud/reference/kms/encrypt) .\n-  `    plaintext` : The `    STRING` value to encrypt.\n-  `    surrogate` : A `    STRING` value that you can prepend to output. If you don't\nwant to use `    surrogate` , pass an empty string (enclosed in `    \"\"` ).\n-  `    context` : A user-provided `    STRING` value that is used with a\nCloud KMS key to derive a data encryption key. For more information,\nsee [CryptoDeterministicConfig:context](https://cloud.google.com/dlp/docs/reference/rest/v2/projects.deidentifyTemplates#cryptodeterministicconfig) .\n\n **Return data type** \n\n `STRING` \n\n **Examples** \n\nIn the following query, the wrapped key is presented in a `BYTES` literal format:\n\nIn the following query, the wrapped key is presented in the base64 format:\n\n\n"
  },
  {
    "name": "DLP_KEY_CHAIN",
    "arguments": [],
    "category": "DLP_encryption",
    "description_markdown": " **Description** \n\nYou can use this function instead of the `key` argument for\nDLP deterministic encryption functions. This function lets\nyou use the [AES-SIV encryption functions](https://cloud.google.com/dlp/docs/pseudonymization#aes-siv) without including `plaintext` keys in a query. To use DLP functions, you need a [new cryptographic key and then use that key to get a wrapped key](/bigquery/docs/column-key-encrypt#wrapped-key-dlp-functions) .\n\n **Definitions** \n\n-  `    kms_resource_name` : A `    STRING` literal that contains the resource path to the\nCloud KMS key. `    kms_resource_name` cannot be `    NULL` and must reside\nin the same Cloud region where this function is executed. This argument is\nused to derive the data encryption key in the `    DLP_DETERMINISTIC_DECRYPT` and `    DLP_DETERMINISTIC_ENCRYPT` functions. A Cloud KMS key looks like\nthis:\n    \n    \n-  `    wrapped_key` : A `    BYTES` literal that represents a secret text chosen by the\nuser. This secret text can be 16, 24, or 32 bytes. For information about\nhow to generate a wrapped key, see [gcloud kms encrypt](https://cloud.google.com/sdk/gcloud/reference/kms/encrypt) .\n    \n    \n\n **Return data type** \n\n `STRUCT` \n\n **Examples** \n\nIn the following query, the wrapped key is presented in a `BYTES` literal format:\n\nIn the following query, the wrapped key is presented in the base64 format:\n\n\n<span id=\"geography_functions\"></span>\n## Geography functions\n\n\nGoogleSQL for BigQuery supports geography functions.\nGeography functions operate on or generate GoogleSQL `GEOGRAPHY` values. The signature of most geography\nfunctions starts with `ST_` . GoogleSQL for BigQuery supports the following functions\nthat can be used to analyze geographical data, determine spatial relationships\nbetween geographical features, and construct or manipulate `GEOGRAPHY` s.\n\nAll GoogleSQL geography functions return `NULL` if any input argument\nis `NULL` .\n\n\n\n"
  },
  {
    "name": "EDIT_DISTANCE",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nComputes the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two `STRING` or `BYTES` values.\n\n **Definitions** \n\n-  `    value1` : The first `    STRING` or `    BYTES` value to compare.\n-  `    value2` : The second `    STRING` or `    BYTES` value to compare.\n-  `    max_distance` : A named argument with a `    INT64` value that is greater than\nor equal to zero. Represents the maximum distance between the two values\nto compute.\n    \n    If this distance is exceeded, the function returns this value.\nThe default value for this argument is the maximum size of `    value1` and `    value2` .\n    \n    \n\n **Details** \n\nIf `value1` or `value2` is `NULL` , `NULL` is returned.\n\nYou can only compare values of the same type. Otherwise, an error is produced.\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\nIn the following example, the first character in both strings is different:\n\nIn the following example, the first and second characters in both strings are\ndifferent:\n\nIn the following example, only the first character in both strings is\ndifferent:\n\nIn the following example, the last six characters are different, but because\nthe maximum distance is `2` , this function exits early and returns `2` , the\nmaximum distance:\n\n\n"
  },
  {
    "name": "ENDS_WITH",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes two `STRING` or `BYTES` values. Returns `TRUE` if `suffix` is a suffix of `value` .\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ERROR",
    "arguments": [],
    "category": "Debugging",
    "description_markdown": " **Description** \n\nReturns an error.\n\n **Definitions** \n\n-  `    error_message` : A `    STRING` value that represents the error message to\nproduce. Any whitespace characters beyond a\nsingle space are trimmed from the results.\n\n **Details** \n\n `ERROR` is treated like any other expression that may\nresult in an error: there is no special guarantee of evaluation order.\n\n **Return Data Type** \n\nGoogleSQL infers the return type in context.\n\n **Examples** \n\nIn the following example, the query produces an error message:\n\nIn the following example, the query returns an error message if the value of the\nrow does not match one of two defined values.\n\nIn the following example, GoogleSQL may evaluate the `ERROR` function\nbefore or after thecondition, because GoogleSQL\ngenerally provides no ordering guarantees between `WHERE` clause conditions and\nthere are no special guarantees for the `ERROR` function.\n\nIn the next example, the `WHERE` clause evaluates an `IF` condition, which\nensures that GoogleSQL only evaluates the `ERROR` function if the\ncondition fails.\n\n\n<span id=\"aggregate-dp-functions\"></span>\n## Differentially private aggregate functions\n\n\nGoogleSQL for BigQuery supports differentially private aggregate functions.\nFor an explanation of how aggregate functions work, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nYou can only use differentially private aggregate functions with [differentially private queries](/bigquery/docs/differential-privacy) in a [differential privacy clause](/bigquery/docs/reference/standard-sql/query-syntax#dp_clause) .\n\n **Note:** In this topic, the privacy parameters in the examples are not\nrecommendations. You should work with your privacy or security officer to\ndetermine the optimal privacy parameters for your dataset and organization.\n"
  },
  {
    "name": "EUCLIDEAN_DISTANCE",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) between two vectors.\n\n **Definitions** \n\n-  `    vector1` : A vector that is represented by an `    ARRAY&lt;T&gt;` value or a sparse vector that is\nrepresented by an `    ARRAY&lt;STRUCT&lt;dimension,magnitude&gt;&gt;` value.\n-  `    vector2` : A vector that is represented by an `    ARRAY&lt;T&gt;` value or a sparse vector that is\nrepresented by an `    ARRAY&lt;STRUCT&lt;dimension,magnitude&gt;&gt;` value.\n\n **Details** \n\n-  `    ARRAY&lt;T&gt;` can be used to represent a vector. Each zero-based index in this\narray represents a dimension. The value for each element in this array\nrepresents a magnitude.\n    \n     `    T` can represent the following and must be the same for both\nvectors:\n    \n    \n    -  `        FLOAT64` In the following example vector, there are four dimensions. The magnitude\nis `    10.0` for dimension `    0` , `    55.0` for dimension `    1` , `    40.0` for\ndimension `    2` , and `    34.0` for dimension `    3` :\n    \n    \n-  `    ARRAY&lt;STRUCT&lt;dimension,magnitude&gt;&gt;` can be used to represent a\nsparse vector. With a sparse vector, you only need to include\ndimension-magnitude pairs for non-zero magnitudes. If a magnitude isn't\npresent in the sparse vector, the magnitude is implicitly understood to be\nzero.\n    \n    For example, if you have a vector with 10,000 dimensions, but only 10\ndimensions have non-zero magnitudes, then the vector is a sparse vector.\nAs a result, it's more efficient to describe a sparse vector by only\nmentioning its non-zero magnitudes.\n    \n    In `    ARRAY&lt;STRUCT&lt;dimension,magnitude&gt;&gt;` , `    STRUCT&lt;dimension,magnitude&gt;` represents a dimension-magnitude pair for each non-zero magnitude in a\nsparse vector. These parts need to be included for each dimension-magnitude\npair:\n    \n    \n    -  `        dimension` : A `        STRING` or `        INT64` value that represents a\ndimension in a vector.\n        \n        \n    -  `        magnitude` : A `        FLOAT64` value that represents a\nnon-zero magnitude for a specific dimension in a vector.\n        \n        You don't need to include empty dimension-magnitude pairs in a\nsparse vector. For example, the following sparse vector and\nnon-sparse vector are equivalent:\n    \n    In a sparse vector, dimension-magnitude pairs don't need to be in any\nparticular order. The following sparse vectors are equivalent:\n    \n    \n- Both  non-sparse vectors\nin this function must share the same dimensions, and if they don't, an error\nis produced.\n    \n    \n- A vector can be a zero vector. A vector is a zero vector if it has\nno dimensions or all dimensions have a magnitude of `    0` , such as `    []` or `    [0.0, 0.0]` .\n    \n    \n- An error is produced if a magnitude in a vector is `    NULL` .\n    \n    \n- If a vector is `    NULL` , `    NULL` is returned.\n    \n    \n\n **Return type** \n\n `FLOAT64` \n\n **Examples** \n\nIn the following example, non-sparse vectors\nare used to compute the Euclidean distance:\n\nIn the following example, sparse vectors are used to compute the\nEuclidean distance:\n\nThe ordering of magnitudes in a vector doesn't impact the results\nproduced by this function. For example these queries produce the same results\neven though the magnitudes in each vector is in a different order:\n\nBoth non-sparse vectors must have the same\ndimensions. If not, an error is produced. In the following example, the first\nvector has two dimensions and the second vector has three:\n\nIf you use sparse vectors and you repeat a dimension, an error is\nproduced:\n\n\n"
  },
  {
    "name": "EXP",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes *e* to the power of X, also called the natural exponential function. If\nthe result underflows, this function returns a zero. Generates an error if the\nresult overflows.\n\n| X | EXP(X) |\n| --- | --- |\n| 0.0 | 1.0 |\n|  `+inf` |  `+inf` |\n|  `-inf` | 0.0 |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "EXTERNAL_OBJECT_TRANSFORM",
    "arguments": [],
    "category": "Table",
    "description_markdown": " **Description** \n\nThis function returns a transformed object table with the original columns plus\none or more additional columns, depending on the `transform_types` values\nspecified.\n\nThis function only supports [object tables](https://cloud.google.com/bigquery/docs/object-table-introduction) as inputs. Subqueries or any other types of tables are not supported.\n\n `object_table_name` is the name of the object table to be transformed, in\nthe format `dataset_name.object_table_name` .\n\n `transform_types_array` is an array of `STRING` literals. Currently, the only\nsupported `transform_types_array` value is `SIGNED_URL` . Specifying `SIGNED_URL` creates read-only signed URLs for the objects in the identified object table,\nwhich are returned in a `signed_url` column. Generated signed URLs are\nvalid for 6 hours.\n\n **Return Type** \n\nTABLE\n\n **Example** \n\nRun the following query to return URIs and signed URLs for the objects in the `mydataset.myobjecttable` object table.\n\n\n"
  },
  {
    "name": "EXTRACT",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nReturns the value corresponding to the specified date part. The `part` must\nbe one of:\n\n-  `    DAYOFWEEK` : Returns values in the range [1,7] with Sunday as the first day\nof the week.\n-  `    DAY` \n-  `    DAYOFYEAR` \n-  `    WEEK` : Returns the week number of the date in the range [0, 53]. Weeks begin\nwith Sunday, and dates prior to the first Sunday of the year are in week 0.\n-  `    WEEK(&lt;WEEKDAY&gt;)` : Returns the week number of the date in the range [0, 53].\nWeeks begin on `    WEEKDAY` . Dates prior to\nthe first `    WEEKDAY` of the year are in week 0. Valid values for `    WEEKDAY` are `    SUNDAY` , `    MONDAY` , `    TUESDAY` , `    WEDNESDAY` , `    THURSDAY` , `    FRIDAY` , and `    SATURDAY` .\n-  `    ISOWEEK` : Returns the [ISO 8601 week](https://en.wikipedia.org/wiki/ISO_week_date) number of the `    date_expression` . `    ISOWEEK` s begin on Monday. Return values\nare in the range [1, 53]. The first `    ISOWEEK` of each ISO year begins on the\nMonday before the first Thursday of the Gregorian calendar year.\n-  `    MONTH` \n-  `    QUARTER` : Returns values in the range [1,4].\n-  `    YEAR` \n-  `    ISOYEAR` : Returns the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year, which is the Gregorian calendar year containing the\nThursday of the week to which `    date_expression` belongs.\n\n **Return Data Type** \n\nINT64\n\n **Examples** \n\nIn the following example, `EXTRACT` returns a value corresponding to the `DAY` date part.\n\nIn the following example, `EXTRACT` returns values corresponding to different\ndate parts from a column of dates near the end of the year.\n\nIn the following example, `date_expression` falls on a Sunday. `EXTRACT` calculates the first column using weeks that begin on Sunday, and it calculates\nthe second column using weeks that begin on Monday.\n\n\n"
  },
  {
    "name": "FARM_FINGERPRINT",
    "arguments": [],
    "category": "Hash",
    "description_markdown": " **Description** \n\nComputes the fingerprint of the `STRING` or `BYTES` input using the `Fingerprint64` function from the [open-source FarmHash library](https://github.com/google/farmhash) . The output\nof this function for a particular input will never change.\n\n **Return type** \n\nINT64\n\n **Examples** \n\n\n"
  },
  {
    "name": "FIRST_VALUE",
    "arguments": [],
    "category": "Navigation",
    "description_markdown": " **Description** \n\nReturns the value of the `value_expression` for the first row in the current\nwindow frame.\n\nThis function includes `NULL` values in the calculation unless `IGNORE NULLS` is\npresent. If `IGNORE NULLS` is present, the function excludes `NULL` values from\nthe calculation.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\n `value_expression` can be any data type that an expression can return.\n\n **Return Data Type** \n\nSame type as `value_expression` .\n\n **Examples** \n\nThe following example computes the fastest time for each division.\n\n\n"
  },
  {
    "name": "FLOAT64",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nConverts a JSON number to a SQL `FLOAT64` value.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    If the JSON value is not a number, an error is produced. If the expression\nis a SQL `    NULL` , the function returns SQL `    NULL` .\n    \n    \n-  `    wide_number_mode` : A named argument with a `    STRING` value.\nDefines what happens with a number that can't be\nrepresented as a `    FLOAT64` without loss of\nprecision. This argument accepts one of the two case-sensitive values:\n    \n    \n    -  `        exact` : The function fails if the result cannot be represented as a `        FLOAT64` without loss of precision.\n    -  `        round` (default): The numeric value stored in JSON will be rounded to `        FLOAT64` . If such rounding is not possible,\nthe function fails.\n\n **Return type** \n\n `FLOAT64` \n\n **Examples** \n\nThe following examples show how invalid requests are handled:\n\n\n"
  },
  {
    "name": "FLOOR",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns the largest integral value that is not greater than X.\n\n| X | FLOOR(X) |\n| --- | --- |\n| 2.0 | 2.0 |\n| 2.3 | 2.0 |\n| 2.8 | 2.0 |\n| 2.5 | 2.0 |\n| -2.3 | -3.0 |\n| -2.8 | -3.0 |\n| -2.5 | -3.0 |\n| 0 | 0 |\n|  `+inf` |  `+inf` |\n|  `-inf` |  `-inf` |\n|  `NaN` |  `NaN` |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "FORMAT",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\n `FORMAT` formats a data type expression as a string.\n\n-  `    format_string_expression` : Can contain zero or more [format specifiers](#format_specifiers) . Each format specifier is introduced\nby the `    %` symbol, and must map to one or more of the remaining arguments.\nIn general, this is a one-to-one mapping, except when the `    *` specifier is\npresent. For example, `    %.*i` maps to two arguments—a length argument\nand a signed integer argument.  If the number of arguments related to the\nformat specifiers is not the same as the number of arguments, an error occurs.\n-  `    data_type_expression` : The value to format as a string. This can be any\nGoogleSQL data type.\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\n| Description | Statement | Result |\n| --- | --- | --- |\n| Simple integer | FORMAT('%d', 10) | 10 |\n| Integer with left blank padding | FORMAT('|%10d|', 11) | |           11| |\n| Integer with left zero padding | FORMAT('+%010d+', 12) | +0000000012+ |\n| Integer with commas | FORMAT(\"%'d\", 123456789) | 123,456,789 |\n| STRING | FORMAT('-%s-', 'abcd efg') | -abcd efg- |\n| FLOAT64 | FORMAT('%f %E', 1.1, 2.2) | 1.100000 2.200000E+00 |\n| DATE | FORMAT('%t', date '2015-09-01') | 2015-09-01 |\n| TIMESTAMP | FORMAT('%t', timestamp '2015-09-01 12:34:56\nAmerica/Los_Angeles') | 2015‑09‑01 19:34:56+00 |\n\nThe `FORMAT()` function does not provide fully customizable formatting for all\ntypes and values, nor formatting that is sensitive to locale.\n\nIf custom formatting is necessary for a type, you must first format it using\ntype-specific format functions, such as `FORMAT_DATE()` or `FORMAT_TIMESTAMP()` .\nFor example:\n\nReturns\n\n\n<span id=\"format_specifiers\"></span>\n#### Supported format specifiers\n\n\nA [format specifier](#format_specifier_list) adds formatting when casting a\nvalue to a string. It can optionally contain these sub-specifiers:\n\n-  [Flags](#flags) \n-  [Width](#width) \n-  [Precision](#precision) \n\nAdditional information about format specifiers:\n\n-  [%g and %G behavior](#g_and_g_behavior) \n-  [%p and %P behavior](#p_and_p_behavior) \n-  [%t and %T behavior](#t_and_t_behavior) \n-  [Error conditions](#error_format_specifiers) \n-  [NULL argument handling](#null_format_specifiers) \n-  [Additional semantic rules](#rules_format_specifiers) \n\n\n<span id=\"format_specifier_list\"></span>\n##### Format specifiers\n\n\n| Specifier | Description | Examples | Types |\n| --- | --- | --- | --- |\n|  `d` or `i` | Decimal integer | 392 |  `INT64` |\n|  `o` | Octal    \n    \nNote: If an `INT64` value is negative, an error is produced. | 610 |  `INT64` |\n|  `x` | Hexadecimal integer    \n    \nNote: If an `INT64` value is negative, an error is produced. | 7fa |  `INT64` |\n|  `X` | Hexadecimal integer (uppercase)    \n    \nNote: If an `INT64` value is negative, an error is produced. | 7FA |  `INT64` |\n|  `f` | Decimal notation, in [-](integer part).(fractional part) for finite\n        values, and in lowercase for non-finite values | 392.650000    \ninf    \nnan |  `NUMERIC`     \n `BIGNUMERIC`     \n `FLOAT64` |\n|  `F` | Decimal notation, in [-](integer part).(fractional part) for finite\n        values, and in uppercase for non-finite values | 392.650000    \nINF    \nNAN |  `NUMERIC`     \n `BIGNUMERIC`     \n `FLOAT64` |\n|  `e` | Scientific notation (mantissa/exponent), lowercase | 3.926500e+02    \ninf    \nnan |  `NUMERIC`     \n `BIGNUMERIC`     \n `FLOAT64` |\n|  `E` | Scientific notation (mantissa/exponent), uppercase | 3.926500E+02    \nINF    \nNAN |  `NUMERIC`     \n `BIGNUMERIC`     \n `FLOAT64` |\n|  `g` | Either decimal notation or scientific notation, depending on the input\n        value's exponent and the specified precision. Lowercase.\n        See [%g and %G behavior](#g_and_g_behavior) for details. | 392.65    \n3.9265e+07    \ninf    \nnan |  `NUMERIC`     \n `BIGNUMERIC`     \n `FLOAT64` |\n|  `G` | Either decimal notation or scientific notation, depending on the input\n      value's exponent and the specified precision. Uppercase.\n      See [%g and %G behavior](#g_and_g_behavior) for details. | 392.65    \n3.9265E+07    \nINF    \nNAN |  `NUMERIC`     \n `BIGNUMERIC`     \n `FLOAT64` |\n|  `p` | Produces a one-line printable string representing JSON.\n      \n      See [%p and %P behavior](#p_and_p_behavior) . |  |  `JSON` |\n|  `P` | Produces a multi-line printable string representing JSON.\n      \n      See [%p and %P behavior](#p_and_p_behavior) . |  |  `JSON` |\n|  `s` | String of characters | sample |  `STRING` |\n|  `t` | Returns a printable string representing the value. Often looks\n      similar to casting the argument to `STRING` .\n      See [%t and %T behavior](#t_and_t_behavior) . | sample    \n2014‑01‑01 | Any type |\n|  `T` | Produces a string that is a valid GoogleSQL constant with a\n      similar type to the value's type (maybe wider, or maybe string).\n      See [%t and %T behavior](#t_and_t_behavior) . | 'sample'    \nb'bytes sample'    \n1234    \n2.3    \ndate '2014‑01‑01' | Any type |\n|  `%` | '%%' produces a single '%' | % | n/a |\n\nThe format specifier can optionally contain the sub-specifiers identified above\nin the specifier prototype.\n\nThese sub-specifiers must comply with the following specifications.\n\n\n<span id=\"flags\"></span>\n##### Flags\n\n\n| Flags | Description |\n| --- | --- |\n|  `-` | Left-justify within the given field width; Right justification is the\ndefault (see width sub-specifier) |\n|  `+` | Forces to precede the result with a plus or minus sign ( `+` or `-` ) even for positive numbers. By default, only negative numbers\nare preceded with a `-` sign |\n| <space> | If no sign is going to be written, a blank space is inserted before the\nvalue |\n|  `#` | - For `%o`, `%x`, and `%X`, this flag means to precede the\n          value with 0, 0x or 0X respectively for values different than zero.\n- For `%f`, `%F`, `%e`, and `%E`, this flag means to add the decimal\n          point even when there is no fractional part, unless the value\n          is non-finite.\n- For `%g` and `%G`, this flag means to add the decimal point even\n          when there is no fractional part unless the value is non-finite, and\n          never remove the trailing zeros after the decimal point. |\n|  `0` | Left-pads the number with zeroes (0) instead of spaces when padding is\n      specified (see width sub-specifier) |\n|  `'` | Formats integers using the appropriating grouping character.\n       For example: |\n\nFlags may be specified in any order. Duplicate flags are not an error. When\nflags are not relevant for some element type, they are ignored.\n\n\n<span id=\"width\"></span>\n##### Width\n\n\n| Width | Description |\n| --- | --- |\n| <number> | Minimum number of characters to be printed. If the value to be printed\n      is shorter than this number, the result is padded with blank spaces.\n      The value is not truncated even if the result is larger |\n|  `*` | The width is not specified in the format string, but as an additional\n      integer value argument preceding the argument that has to be formatted |\n\n\n<span id=\"precision\"></span>\n##### Precision\n\n\n| Precision | Description |\n| --- | --- |\n|  `.` <number> | - For integer specifiers `%d`, `%i`, `%o`, `%u`, `%x`, and `%X`:\n          precision specifies the\n          minimum number of digits to be written. If the value to be written is\n          shorter than this number, the result is padded with trailing zeros.\n          The value is not truncated even if the result is longer. A precision\n          of 0 means that no character is written for the value 0.\n- For specifiers `%a`, `%A`, `%e`, `%E`, `%f`, and `%F`: this is the\n          number of digits to be printed after the decimal point. The default\n          value is 6.\n- For specifiers `%g` and `%G`: this is the number of significant digits\n          to be printed, before the removal of the trailing zeros after the\n          decimal point. The default value is 6. |\n|  `.*` | The precision is not specified in the format string, but as an\n      additional integer value argument preceding the argument that has to be\n      formatted |\n\n\n<span id=\"g_and_g_behavior\"></span>\n##### %g and %G behavior\n\n\nThe `%g` and `%G` format specifiers choose either the decimal notation (like\nthe `%f` and `%F` specifiers) or the scientific notation (like the `%e` and `%E` specifiers), depending on the input value's exponent and the specified [precision](#precision) .\n\nLet p stand for the specified [precision](#precision) (defaults to 6; 1 if the\nspecified precision is less than 1). The input value is first converted to\nscientific notation with precision = (p - 1). If the resulting exponent part x\nis less than -4 or no less than p, the scientific notation with precision =\n(p - 1) is used; otherwise the decimal notation with precision = (p - 1 - x) is\nused.\n\nUnless [# flag](#flags) is present, the trailing zeros after the decimal point\nare removed, and the decimal point is also removed if there is no digit after\nit.\n\n\n<span id=\"p_and_p_behavior\"></span>\n##### %p and %P behavior\n\n\nThe `%p` format specifier produces a one-line printable string. The `%P` format specifier produces a multi-line printable string. You can use these\nformat specifiers with the following data types:\n\n|  **Type** |  **%p** |  **%P** |\n| --- | --- | --- |\n| JSON | JSON input:\n\nProduces a one-line printable string representing JSON: | JSON input:\n\nProduces a multi-line printable string representing JSON: |\n\n\n<span id=\"t_and_t_behavior\"></span>\n##### %t and %T behavior\n\n\nThe `%t` and `%T` format specifiers are defined for all types. The [width](#width) , [precision](#precision) , and [flags](#flags) act as they do\nfor `%s` : the [width](#width) is the minimum width and the `STRING` will be\npadded to that size, and [precision](#precision) is the maximum width\nof content to show and the `STRING` will be truncated to that size, prior to\npadding to width.\n\nThe `%t` specifier is always meant to be a readable form of the value.\n\nThe `%T` specifier is always a valid SQL literal of a similar type, such as a\nwider numeric type.\nThe literal will not include casts or a type name, except for the special case\nof non-finite floating point values.\n\nThe `STRING` is formatted as follows:\n\n|  **Type** |  **%t** |  **%T** |\n| --- | --- | --- |\n|  `NULL` of any type | NULL | NULL |\n|  `INT64` | 123 | 123 |\n| NUMERIC | 123.0 *(always with .0)* | NUMERIC \"123.0\" |\n| FLOAT64 | 123.0 *(always with .0)*     \n123e+10    \n `inf`     \n `-inf`     \n `NaN` | 123.0 *(always with .0)*     \n123e+10    \nCAST(\"inf\" AS <type>)    \nCAST(\"-inf\" AS <type>)    \nCAST(\"nan\" AS <type>) |\n| STRING | unquoted string value | quoted string literal |\n| BYTES | unquoted escaped bytes    \ne.g., abc\\x01\\x02 | quoted bytes literal    \ne.g., b\"abc\\x01\\x02\" |\n| BOOL | boolean value | boolean value |\n| DATE | 2011-02-03 | DATE \"2011-02-03\" |\n| TIMESTAMP | 2011-02-03 04:05:06+00 | TIMESTAMP \"2011-02-03 04:05:06+00\" |\n| INTERVAL | 1-2 3 4:5:6.789 | INTERVAL \"1-2 3 4:5:6.789\" YEAR TO SECOND |\n| ARRAY | [value, value, ...]    \nwhere values are formatted with %t | [value, value, ...]    \nwhere values are formatted with %T |\n| STRUCT | (value, value, ...)    \nwhere fields are formatted with %t | (value, value, ...)    \nwhere fields are formatted with %T    \n    \nSpecial cases:    \nZero fields: STRUCT()    \nOne field: STRUCT(value) |\n| JSON | one-line printable string representing JSON. | one-line printable string representing a JSON literal. |\n\n\n<span id=\"error_format_specifiers\"></span>\n##### Error conditions\n\n\nIf a format specifier is invalid, or is not compatible with the related\nargument type, or the wrong number or arguments are provided, then an error is\nproduced.  For example, the following `&lt;format_string&gt;` expressions are invalid:\n\n\n<span id=\"null_format_specifiers\"></span>\n##### NULL argument handling\n\n\nA `NULL` format string results in a `NULL` output `STRING` . Any other arguments\nare ignored in this case.\n\nThe function generally produces a `NULL` value if a `NULL` argument is present.\nFor example, `FORMAT('%i', NULL_expression)` produces a `NULL STRING` as\noutput.\n\nHowever, there are some exceptions: if the format specifier is %t or %T\n(both of which produce `STRING` s that effectively match CAST and literal value\nsemantics), a `NULL` value produces 'NULL' (without the quotes) in the result `STRING` . For example, the function:\n\nReturns\n\n\n<span id=\"rules_format_specifiers\"></span>\n##### Additional semantic rules\n\n\n `FLOAT64` values can be `+/-inf` or `NaN` .\nWhen an argument has one of those values, the result of the format specifiers `%f` , `%F` , `%e` , `%E` , `%g` , `%G` , and `%t` are `inf` , `-inf` , or `nan` (or the same in uppercase) as appropriate.  This is consistent with how\nGoogleSQL casts these values to `STRING` .  For `%T` ,\nGoogleSQL returns quoted strings for `FLOAT64` values that don't have non-string literal\nrepresentations.\n\n\n\n"
  },
  {
    "name": "FORMAT_DATE",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nFormats the `date_expr` according to the specified `format_string` .\n\nSee [Supported Format Elements For DATE](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) for a list of format elements that this function supports.\n\n **Return Data Type** \n\nSTRING\n\n **Examples** \n\n\n"
  },
  {
    "name": "FORMAT_DATETIME",
    "arguments": [],
    "category": "Datetime",
    "description_markdown": " **Description** \n\nFormats a `DATETIME` object according to the specified `format_string` . See [Supported Format Elements For DATETIME](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) for a list of format elements that this function supports.\n\n **Return Data Type** \n\n `STRING` \n\n **Examples** \n\n\n"
  },
  {
    "name": "FORMAT_TIME",
    "arguments": [],
    "category": "Time",
    "description_markdown": " **Description** Formats a `TIME` object according to the specified `format_string` . See [Supported Format Elements For TIME](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) for a list of format elements that this function supports.\n\n **Return Data Type** \n\n `STRING` \n\n **Example** \n\n\n"
  },
  {
    "name": "FORMAT_TIMESTAMP",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nFormats a timestamp according to the specified `format_string` .\n\nSee [Format elements for date and time parts](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) for a list of format elements that this function supports.\n\n **Return Data Type** \n\n `STRING` \n\n **Example** \n\n\n"
  },
  {
    "name": "FROM_BASE32",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nConverts the base32-encoded input `string_expr` into `BYTES` format. To convert `BYTES` to a base32-encoded `STRING` , use [TO_BASE32](#to_base32) .\n\n **Return type** \n\n `BYTES` \n\n **Example** \n\n\n"
  },
  {
    "name": "FROM_BASE64",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nConverts the base64-encoded input `string_expr` into `BYTES` format. To convert `BYTES` to a base64-encoded `STRING` ,\nuse [TO_BASE64][string-link-to-base64].\n\nThere are several base64 encodings in common use that vary in exactly which\nalphabet of 65 ASCII characters are used to encode the 64 digits and padding.\nSee [RFC 4648](https://tools.ietf.org/html/rfc4648#section-4) for details. This\nfunction expects the alphabet `[A-Za-z0-9+/=]` .\n\n **Return type** \n\n `BYTES` \n\n **Example** \n\nTo work with an encoding using a different base64 alphabet, you might need to\ncompose `FROM_BASE64` with the `REPLACE` function. For instance, the `base64url` url-safe and filename-safe encoding commonly used in web programming\nuses `-_=` as the last characters rather than `+/=` . To decode a `base64url` -encoded string, replace `-` and `_` with `+` and `/` respectively.\n\n\n"
  },
  {
    "name": "FROM_HEX",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nConverts a hexadecimal-encoded `STRING` into `BYTES` format. Returns an error\nif the input `STRING` contains characters outside the range `(0..9, A..F, a..f)` . The lettercase of the characters does not matter. If the\ninput `STRING` has an odd number of characters, the function acts as if the\ninput has an additional leading `0` . To convert `BYTES` to a hexadecimal-encoded `STRING` , use [TO_HEX](#to_hex) .\n\n **Return type** \n\n `BYTES` \n\n **Example** \n\n\n"
  },
  {
    "name": "GAP_FILL",
    "arguments": [],
    "category": "Table",
    "description_markdown": "Finds and fills gaps in a time series.\nFor more information, see [GAP_FILL](#gap_fill) in\nTime series functions.\n\n\n\n"
  },
  {
    "name": "GENERATE_ARRAY",
    "arguments": [],
    "category": "Array",
    "description_markdown": " **Description** \n\nReturns an array of values. The `start_expression` and `end_expression` parameters determine the inclusive start and end of the array.\n\nThe `GENERATE_ARRAY` function accepts the following data types as inputs:\n\n-  `    INT64` \n-  `    NUMERIC` \n-  `    BIGNUMERIC` \n-  `    FLOAT64` \n\nThe `step_expression` parameter determines the increment used to\ngenerate array values. The default value for this parameter is `1` .\n\nThis function returns an error if `step_expression` is set to 0, or if any\ninput is `NaN` .\n\nIf any argument is `NULL` , the function will return a `NULL` array.\n\n **Return Data Type** \n\n `ARRAY` \n\n **Examples** \n\nThe following returns an array of integers, with a default step of 1.\n\nThe following returns an array using a user-specified step size.\n\nThe following returns an array using a negative value, `-3` for its step size.\n\nThe following returns an array using the same value for the `start_expression` and `end_expression` .\n\nThe following returns an empty array, because the `start_expression` is greater\nthan the `end_expression` , and the `step_expression` value is positive.\n\nThe following returns a `NULL` array because `end_expression` is `NULL` .\n\nThe following returns multiple arrays.\n\n\n"
  },
  {
    "name": "GENERATE_DATE_ARRAY",
    "arguments": [],
    "category": "Array",
    "description_markdown": " **Description** \n\nReturns an array of dates. The `start_date` and `end_date` parameters determine the inclusive start and end of the array.\n\nThe `GENERATE_DATE_ARRAY` function accepts the following data types as inputs:\n\n-  `    start_date` must be a `    DATE` .\n-  `    end_date` must be a `    DATE` .\n-  `    INT64_expr` must be an `    INT64` .\n-  `    date_part` must be either DAY, WEEK, MONTH, QUARTER, or YEAR.\n\nThe `INT64_expr` parameter determines the increment used to generate dates. The\ndefault value for this parameter is 1 day.\n\nThis function returns an error if `INT64_expr` is set to 0.\n\n **Return Data Type** \n\n `ARRAY` containing 0 or more `DATE` values.\n\n **Examples** \n\nThe following returns an array of dates, with a default step of 1.\n\nThe following returns an array using a user-specified step size.\n\nThe following returns an array using a negative value, `-3` for its step size.\n\nThe following returns an array using the same value for the `start_date` and `end_date` .\n\nThe following returns an empty array, because the `start_date` is greater\nthan the `end_date` , and the `step` value is positive.\n\nThe following returns a `NULL` array, because one of its inputs is `NULL` .\n\nThe following returns an array of dates, using MONTH as the `date_part` interval:\n\nThe following uses non-constant dates to generate an array.\n\n\n"
  },
  {
    "name": "GENERATE_RANGE_ARRAY",
    "arguments": [],
    "category": "Range",
    "description_markdown": " **Description** \n\nSplits a range into an array of subranges.\n\n **Definitions** \n\n-  `    range_to_split` : The `    RANGE&lt;T&gt;` value to split.\n-  `    step_interval` : The `    INTERVAL` value, which determines the maximum size of\neach subrange in the resulting array. An [interval single date and time part](/bigquery/docs/reference/standard-sql/data-types#single_datetime_part_interval) is supported, but an interval range of date and time parts is not.\n    \n    \n    - If `        range_to_split` is `        RANGE&lt;DATE&gt;` , these interval\ndate parts are supported: `        YEAR` to `        DAY` .\n        \n        \n    - If `        range_to_split` is `        RANGE&lt;DATETIME&gt;` , these interval\ndate and time parts are supported: `        YEAR` to `        SECOND` .\n        \n        \n    - If `        range_to_split` is `        RANGE&lt;TIMESTAMP&gt;` , these interval\ndate and time parts are supported: `        DAY` to `        SECOND` .\n        \n        \n-  `    include_last_partial_range` : A `    BOOL` value, which determines whether or\nnot to include the last subrange if it's a partial subrange.\nIf this argument is not specified, the default value is `    TRUE` .\n    \n    \n    -  `        TRUE` (default): The last subrange is included, even if it's\nsmaller than `        step_interval` .\n        \n        \n    -  `        FALSE` : Exclude the last subrange if it's smaller than `        step_interval` .\n        \n        \n\n **Details** \n\nReturns `NULL` if any input is `NULL` .\n\n **Return type** \n\n `ARRAY&lt;RANGE&lt;T&gt;&gt;` \n\n **Examples** \n\nIn the following example, a date range between `2020-01-01` and `2020-01-06` is split into an array of subranges that are one day long. There are\nno partial ranges.\n\nIn the following examples, a date range between `2020-01-01` and `2020-01-06` is split into an array of subranges that are two days long. The final subrange\nis smaller than two days:\n\nIn the following example, a date range between `2020-01-01` and `2020-01-06` is split into an array of subranges that are two days long, but the final\nsubrange is excluded because it's smaller than two days:\n\n\n"
  },
  {
    "name": "GENERATE_TIMESTAMP_ARRAY",
    "arguments": [],
    "category": "Array",
    "description_markdown": " **Description** \n\nReturns an `ARRAY` of `TIMESTAMPS` separated by a given interval. The `start_timestamp` and `end_timestamp` parameters determine the inclusive\nlower and upper bounds of the `ARRAY` .\n\nThe `GENERATE_TIMESTAMP_ARRAY` function accepts the following data types as\ninputs:\n\n-  `    start_timestamp` : `    TIMESTAMP` \n-  `    end_timestamp` : `    TIMESTAMP` \n-  `    step_expression` : `    INT64` \n- Allowed `    date_part` values are: `    MICROSECOND` , `    MILLISECOND` , `    SECOND` , `    MINUTE` , `    HOUR` , or `    DAY` .\n\nThe `step_expression` parameter determines the increment used to generate\ntimestamps.\n\n **Return Data Type** \n\nAn `ARRAY` containing 0 or more `TIMESTAMP` values.\n\n **Examples** \n\nThe following example returns an `ARRAY` of `TIMESTAMP` s at intervals of 1 day.\n\nThe following example returns an `ARRAY` of `TIMESTAMP` s at intervals of 1\nsecond.\n\nThe following example returns an `ARRAY` of `TIMESTAMPS` with a negative\ninterval.\n\nThe following example returns an `ARRAY` with a single element, because `start_timestamp` and `end_timestamp` have the same value.\n\nThe following example returns an empty `ARRAY` , because `start_timestamp` is\nlater than `end_timestamp` .\n\nThe following example returns a null `ARRAY` , because one of the inputs is `NULL` .\n\nThe following example generates `ARRAY` s of `TIMESTAMP` s from columns containing\nvalues for `start_timestamp` and `end_timestamp` .\n\n\n"
  },
  {
    "name": "GENERATE_UUID",
    "arguments": [],
    "category": "Utility",
    "description_markdown": " **Description** \n\nReturns a random universally unique identifier (UUID) as a `STRING` .\nThe returned `STRING` consists of 32 hexadecimal\ndigits in five groups separated by hyphens in the form 8-4-4-4-12. The\nhexadecimal digits represent 122 random bits and 6 fixed bits, in compliance\nwith [RFC 4122 section 4.4](https://tools.ietf.org/html/rfc4122#section-4.4) .\nThe returned `STRING` is lowercase.\n\n **Return Data Type** \n\nSTRING\n\n **Example** \n\nThe following query generates a random UUID.\n\n\n  \n"
  },
  {
    "name": "GREATEST",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns the greatest value among `X1,...,XN` . If any argument is `NULL` , returns `NULL` . Otherwise, in the case of floating-point arguments, if any argument is `NaN` , returns `NaN` . In all other cases, returns the value among `X1,...,XN` that has the greatest value according to the ordering used by the `ORDER BY` clause. The arguments `X1, ..., XN` must be coercible to a common supertype, and\nthe supertype must support ordering.\n\n| X1,...,XN | GREATEST(X1,...,XN) |\n| --- | --- |\n| 3,5,1 | 5 |\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Return Data Types** \n\nData type of the input values.\n\n\n\n"
  },
  {
    "name": "GROUPING",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nIf a groupable item in the [GROUP BY clause](/bigquery/docs/reference/standard-sql/query-syntax#group_by_clause) is aggregated\n(and thus not grouped), this function returns `1` . Otherwise,\nthis function returns `0` .\n\nDefinitions:\n\n-  `    groupable_value` : An expression that represents a value that can be grouped\nin the `    GROUP BY` clause.\n\nDetails:\n\nThe `GROUPING` function is helpful if you need to determine which rows are\nproduced by which grouping sets. A grouping set is a group of columns by which\nrows can be grouped together. So, if you need to filter rows by\na few specific grouping sets, you can use the `GROUPING` function to identify\nwhich grouping sets grouped which rows by creating a matrix of the results.\n\nIn addition, you can use the `GROUPING` function to determine the type of `NULL` produced by the `GROUP BY` clause. In some cases, the `GROUP BY` clause\nproduces a `NULL` placeholder. This placeholder represents all groupable items\nthat are aggregated (not grouped) in the current grouping set. This is different\nfrom a standard `NULL` , which can also be produced by a query.\n\nFor more information, see the following examples.\n\n **Returned Data Type** \n\n `INT64` \n\n **Examples** \n\nIn the following example, it's difficult to determine which rows are grouped by\nthe grouping value `product_type` or `product_name` . The `GROUPING` function\nmakes this easier to determine.\n\nPay close attention to what's in the `product_type_agg` and `product_name_agg` column matrix. This determines how the rows are grouped.\n\n|  `product_type_agg` |  `product_name_agg` | Notes |\n| --- | --- | --- |\n| 1 | 0 | Rows are grouped by `product_name` . |\n| 0 | 1 | Rows are grouped by `product_type` . |\n| 0 | 0 | Rows are grouped by `product_type` and `product_name` . |\n| 1 | 1 | Grand total row. |\n\nIn the following example, it's difficult to determine\nif `NULL` represents a `NULL` placeholder or a standard `NULL` value in the `product_type` column. The `GROUPING` function makes it easier to\ndetermine what type of `NULL` is being produced. If `product_type_is_aggregated` is `1` , the `NULL` value for\nthe `product_type` column is a `NULL` placeholder.\n\n\n"
  },
  {
    "name": "HLL_COUNT.EXTRACT",
    "arguments": [],
    "category": "HyperLogLog",
    "description_markdown": " **Description** \n\nA scalar function that extracts a cardinality estimate of a single [HLL++](https://research.google.com/pubs/pub40671.html) sketch.\n\nIf `sketch` is `NULL` , this function returns a cardinality estimate of `0` .\n\n **Supported input types** \n\n `BYTES` \n\n **Return type** \n\n `INT64` \n\n **Example** \n\nThe following query returns the number of distinct users for each country who\nhave at least one invoice.\n\n\n"
  },
  {
    "name": "HLL_COUNT.INIT",
    "arguments": [],
    "category": "HyperLogLog",
    "description_markdown": " **Description** \n\nAn aggregate function that takes one or more `input` values and aggregates them\ninto a [HLL++](https://research.google.com/pubs/pub40671.html) sketch. Each sketch\nis represented using the `BYTES` data type. You can then merge sketches using `HLL_COUNT.MERGE` or `HLL_COUNT.MERGE_PARTIAL` . If no merging is needed,\nyou can extract the final count of distinct values from the sketch using `HLL_COUNT.EXTRACT` .\n\nThis function supports an optional parameter, `precision` . This parameter\ndefines the accuracy of the estimate at the cost of additional memory required\nto process the sketches or store them on disk. The range for this value is `10` to `24` . The default value is `15` . For more information about precision,\nsee [Precision for sketches](/bigquery/docs/sketches#precision_hll) .\n\nIf the input is `NULL` , this function returns `NULL` .\n\nFor more information, see [HyperLogLog in Practice: Algorithmic Engineering of\na State of The Art Cardinality Estimation Algorithm](https://research.google.com/pubs/pub40671.html) .\n\n **Supported input types** \n\n-  `    INT64` \n-  `    NUMERIC` \n-  `    BIGNUMERIC` \n-  `    STRING` \n-  `    BYTES` \n\n **Return type** \n\n `BYTES` \n\n **Example** \n\nThe following query creates HLL++ sketches that count the number of distinct\nusers with at least one invoice per country.\n\n\n"
  },
  {
    "name": "HLL_COUNT.MERGE",
    "arguments": [],
    "category": "HyperLogLog",
    "description_markdown": " **Description** \n\nAn aggregate function that returns the cardinality of several [HLL++](https://research.google.com/pubs/pub40671.html) sketches by computing their union.\n\nEach `sketch` must be initialized on the same type. Attempts to merge sketches\nfor different types results in an error. For example, you cannot merge a sketch\ninitialized from `INT64` data with one initialized from `STRING` data.\n\nIf the merged sketches were initialized with different precisions, the precision\nwill be downgraded to the lowest precision involved in the merge.\n\nThis function ignores `NULL` values when merging sketches. If the merge happens\nover zero rows or only over `NULL` values, the function returns `0` .\n\n **Supported input types** \n\n `BYTES` \n\n **Return type** \n\n `INT64` \n\n **Example** \n\nThe following query counts the number of distinct users across all countries\n who have at least one invoice.\n\n\n"
  },
  {
    "name": "HLL_COUNT.MERGE_PARTIAL",
    "arguments": [],
    "category": "HyperLogLog",
    "description_markdown": " **Description** \n\nAn aggregate function that takes one or more [HLL++](https://research.google.com/pubs/pub40671.html)  `sketch` inputs and merges them into a new sketch.\n\nEach `sketch` must be initialized on the same type. Attempts to merge sketches\nfor different types results in an error. For example, you cannot merge a sketch\ninitialized from `INT64` data with one initialized from `STRING` data.\n\nIf the merged sketches were initialized with different precisions, the precision\nwill be downgraded to the lowest precision involved in the merge. For example,\nif `MERGE_PARTIAL` encounters sketches of precision 14 and 15, the returned new\nsketch will have precision 14.\n\nThis function returns `NULL` if there is no input or all inputs are `NULL` .\n\n **Supported input types** \n\n `BYTES` \n\n **Return type** \n\n `BYTES` \n\n **Example** \n\nThe following query returns an HLL++ sketch that counts the number of distinct\nusers who have at least one invoice across all countries.\n\n\n<span id=\"interval_functions\"></span>\n## Interval functions\n\n\nGoogleSQL for BigQuery supports the following interval functions.\n\n\n\n"
  },
  {
    "name": "IEEE_DIVIDE",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nDivides X by Y; this function never fails. Returns `FLOAT64` . Unlike the division operator (/),\nthis function does not generate errors for division by zero or overflow.\n\n| X | Y | IEEE_DIVIDE(X, Y) |\n| --- | --- | --- |\n| 20.0 | 4.0 | 5.0 |\n| 0.0 | 25.0 | 0.0 |\n| 25.0 | 0.0 |  `+inf` |\n| -25.0 | 0.0 |  `-inf` |\n| 0.0 | 0.0 |  `NaN` |\n| 0.0 |  `NaN` |  `NaN` |\n|  `NaN` | 0.0 |  `NaN` |\n|  `+inf` |  `+inf` |  `NaN` |\n|  `-inf` |  `-inf` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "INITCAP",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes a `STRING` and returns it with the first character in each word in\nuppercase and all other characters in lowercase. Non-alphabetic characters\nremain the same.\n\n `delimiters` is an optional string argument that is used to override the default\nset of characters used to separate words. If `delimiters` is not specified, it\ndefaults to the following characters:    \n `&lt;whitespace&gt; [ ] ( ) { } / | \\ &lt; &gt; ! ? @ \" ^ # $ &amp; ~ _ , . : ; * % + -` \n\nIf `value` or `delimiters` is `NULL` , the function returns `NULL` .\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\n\n"
  },
  {
    "name": "INSTR",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns the lowest 1-based position of `subvalue` in `value` . `value` and `subvalue` must be the same type, either `STRING` or `BYTES` .\n\nIf `position` is specified, the search starts at this position in `value` , otherwise it starts at `1` , which is the beginning of `value` . If `position` is negative, the function searches backwards\nfrom the end of `value` , with `-1` indicating the last character. `position` is of type `INT64` and cannot be `0` .\n\nIf `occurrence` is specified, the search returns the position of a specific\ninstance of `subvalue` in `value` . If not specified, `occurrence` defaults to `1` and returns the position of the first occurrence.\nFor `occurrence` > `1` , the function includes overlapping occurrences. `occurrence` is of type `INT64` and must be positive.\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\nReturns `0` if:\n\n- No match is found.\n- If `    occurrence` is greater than the number of matches found.\n- If `    position` is greater than the length of `    value` .\n\nReturns `NULL` if:\n\n- Any input argument is `    NULL` .\n\nReturns an error if:\n\n-  `    position` is `    0` .\n-  `    occurrence` is `    0` or negative.\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "INT64",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nConverts a JSON number to a SQL `INT64` value.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    If the JSON value is not a number, or the JSON number is not in the SQL `    INT64` domain, an error is produced. If the expression is SQL `    NULL` , the\nfunction returns SQL `    NULL` .\n    \n    \n\n **Return type** \n\n `INT64` \n\n **Examples** \n\nThe following examples show how invalid requests are handled:\n\n\n"
  },
  {
    "name": "IS_INF",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns `TRUE` if the value is positive or negative infinity.\n\n| X | IS_INF(X) |\n| --- | --- |\n|  `+inf` |  `TRUE` |\n|  `-inf` |  `TRUE` |\n| 25 |  `FALSE` |\n\n\n\n"
  },
  {
    "name": "IS_NAN",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns `TRUE` if the value is a `NaN` value.\n\n| X | IS_NAN(X) |\n| --- | --- |\n|  `NaN` |  `TRUE` |\n| 25 |  `FALSE` |\n\n\n\n"
  },
  {
    "name": "JSON_ARRAY",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nCreates a JSON array from zero or more SQL values.\n\nArguments:\n\n-  `    value` : A [JSON encoding-supported](#json_encodings) value to add\nto a JSON array.\n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nThe following query creates a JSON array with one value in it:\n\nYou can create a JSON array with an empty JSON array in it. For example:\n\nYou can create an empty JSON array. For example:\n\n\n"
  },
  {
    "name": "JSON_ARRAY_APPEND",
    "arguments": [],
    "category": "JSON",
    "description_markdown": "Appends JSON data to the end of a JSON array.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path_value_pair` : A value and the [JSONPath](#JSONPath_format) for\nthat value. This includes:\n    \n    \n    -  `        json_path` : Append `        value` at this [JSONPath](#JSONPath_format) in `        json_expr` .\n        \n        \n    -  `        value` : A [JSON encoding-supported](#json_encodings) value to\nappend.\n        \n        \n-  `    append_each_element` : A named argument with a `    BOOL` value.\n    \n    \n    - If `        TRUE` (default), and `        value` is a SQL array,\nappends each element individually.\n        \n        \n    - If `        FALSE,` and `        value` is a SQL array, appends\nthe array as one element.\n        \n        \n\nDetails:\n\n- Path value pairs are evaluated left to right. The JSON produced by\nevaluating one pair becomes the JSON against which the next pair\nis evaluated.\n- The operation is ignored if the path points to a JSON non-array value that\nis not a JSON null.\n- If `    json_path` points to a JSON null, the JSON null is replaced by a\nJSON array that contains `    value` .\n- If the path exists but has an incompatible type at any given path token,\nthe path value pair operation is ignored.\n- The function applies all path value pair append operations even if an\nindividual path value pair operation is invalid. For invalid operations,\nthe operation is ignored and the function continues to process the rest of\nthe path value pairs.\n- If any `    json_path` is an invalid [JSONPath](#JSONPath_format) , an error is\nproduced.\n- If `    json_expr` is SQL `    NULL` , the function returns SQL `    NULL` .\n- If `    append_each_element` is SQL `    NULL` , the function returns `    json_expr` .\n- If `    json_path` is SQL `    NULL` , the `    json_path_value_pair` operation is\nignored.\n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nIn the following example, path `$` is matched and appends `1` .\n\nIn the following example, `append_each_element` defaults to `TRUE` , so `[1, 2]` is appended as individual elements.\n\nIn the following example, `append_each_element` is `FALSE` , so `[1, 2]` is appended as one element.\n\nIn the following example, `append_each_element` is `FALSE` , so `[1, 2]` and `[3, 4]` are each appended as one element.\n\nIn the following example, the first path `$[1]` appends `[1, 2]` as single\nelements, and then the second path `$[1][1]` is not a valid path to an array,\nso the second operation is ignored.\n\nIn the following example, path `$.a` is matched and appends `2` .\n\nIn the following example, a value is appended into a JSON null.\n\nIn the following example, path `$.a` is not an array, so the operation is\nignored.\n\nIn the following example, path `$.b` does not exist, so the operation is\nignored.\n\n\n"
  },
  {
    "name": "JSON_ARRAY_INSERT",
    "arguments": [],
    "category": "JSON",
    "description_markdown": "Produces a new JSON value that is created by inserting JSON data into\na JSON array.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path_value_pair` : A value and the [JSONPath](#JSONPath_format) for\nthat value. This includes:\n    \n    \n    -  `        json_path` : Insert `        value` at this [JSONPath](#JSONPath_format) in `        json_expr` .\n        \n        \n    -  `        value` : A [JSON encoding-supported](#json_encodings) value to\ninsert.\n        \n        \n-  `    insert_each_element` : A named argument with a `    BOOL` value.\n    \n    \n    - If `        TRUE` (default), and `        value` is a SQL array,\ninserts each element individually.\n        \n        \n    - If `        FALSE,` and `        value` is a SQL array, inserts\nthe array as one element.\n        \n        \n\nDetails:\n\n- Path value pairs are evaluated left to right. The JSON produced by\nevaluating one pair becomes the JSON against which the next pair\nis evaluated.\n- The operation is ignored if the path points to a JSON non-array value that\nis not a JSON null.\n- If `    json_path` points to a JSON null, the JSON null is replaced by a\nJSON array of the appropriate size and padded on the left with JSON nulls.\n- If the path exists but has an incompatible type at any given path token,\nthe path value pair operator is ignored.\n- The function applies all path value pair append operations even if an\nindividual path value pair operation is invalid. For invalid operations,\nthe operation is ignored and the function continues to process the rest of\nthe path value pairs.\n- If the array index in `    json_path` is larger than the size of the array, the\nfunction extends the length of the array to the index, fills in\nthe array with JSON nulls, then adds `    value` at the index.\n- If any `    json_path` is an invalid [JSONPath](#JSONPath_format) , an error is\nproduced.\n- If `    json_expr` is SQL `    NULL` , the function returns SQL `    NULL` .\n- If `    insert_each_element` is SQL `    NULL` , the function returns `    json_expr` .\n- If `    json_path` is SQL `    NULL` , the `    json_path_value_pair` operation is\nignored.\n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nIn the following example, path `$[1]` is matched and inserts `1` .\n\nIn the following example, path `$[1][0]` is matched and inserts `1` .\n\nIn the following example, `insert_each_element` defaults to `TRUE` , so `[1, 2]` is inserted as individual elements.\n\nIn the following example, `insert_each_element` is `FALSE` , so `[1, 2]` is\ninserted as one element.\n\nIn the following example, path `$[7]` is larger than the length of the\nmatched array, so the array is extended with JSON nulls and `\"e\"` is inserted at\nthe end of the array.\n\nIn the following example, path `$.a` is an object, so the operation is ignored.\n\nIn the following example, path `$` does not specify a valid array position,\nso the operation is ignored.\n\nIn the following example, a value is inserted into a JSON null.\n\nIn the following example, the operation is ignored because you can't insert\ndata into a JSON number.\n\n\n"
  },
  {
    "name": "JSON_EXTRACT",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nExtracts a JSON value and converts it to a\nSQL JSON-formatted `STRING` or `JSON` value.\nThis function uses single quotes and brackets to escape invalid [JSONPath](#JSONPath_format) characters in JSON keys. For example: `['a.b']` .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    Extracts a SQL `    NULL` when a JSON-formatted string `    null` is encountered.\nFor example:\n    \n    \n-  `    json_expr` : JSON. For example:\n    \n    Extracts a JSON `    null` when a JSON `    null` is encountered.\n    \n    \n-  `    json_path` : The [JSONPath](#JSONPath_format) . This identifies the data that\nyou want to obtain from the input.\n    \n    \n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see [Differences between the JSON and JSON-formatted STRING types](#differences_json_and_string) .\n\n **Return type** \n\n-  `    json_string_expr` : A JSON-formatted `    STRING` \n-  `    json_expr` : `    JSON` \n\n **Examples** \n\nIn the following example, JSON data is extracted and returned as JSON.\n\nIn the following examples, JSON data is extracted and returned as\nJSON-formatted strings.\n\n\n"
  },
  {
    "name": "JSON_EXTRACT_ARRAY",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nExtracts a JSON array and converts it to\na SQL `ARRAY&lt;JSON-formatted STRING&gt;` or `ARRAY&lt;JSON&gt;` value.\nThis function uses single quotes and brackets to escape invalid [JSONPath](#JSONPath_format) characters in JSON keys. For example: `['a.b']` .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    \n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path` : The [JSONPath](#JSONPath_format) . This identifies the data that\nyou want to obtain from the input. If this optional parameter is not\nprovided, then the JSONPath `    $` symbol is applied, which means that all of\nthe data is analyzed.\n    \n    \n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see [Differences between the JSON and JSON-formatted STRING types](#differences_json_and_string) .\n\n **Return type** \n\n-  `    json_string_expr` : `    ARRAY&lt;JSON-formatted STRING&gt;` \n-  `    json_expr` : `    ARRAY&lt;JSON&gt;` \n\n **Examples** \n\nThis extracts items in JSON to an array of `JSON` values:\n\nThis extracts the items in a JSON-formatted string to a string array:\n\nThis extracts a string array and converts it to an integer array:\n\nThis extracts string values in a JSON-formatted string to an array:\n\nThis extracts only the items in the `fruit` property to an array:\n\nThese are equivalent:\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those\ncharacters using single quotes and brackets, `[' ']` . For example:\n\nThe following examples explore how invalid requests and empty arrays are\nhandled:\n\n- If a JSONPath is invalid, an error is thrown.\n- If a JSON-formatted string is invalid, the output is NULL.\n- It is okay to have empty arrays in the JSON-formatted string.\n\n\n"
  },
  {
    "name": "JSON_EXTRACT_SCALAR",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nExtracts a JSON scalar value and converts it to a SQL `STRING` value.\nIn addition, this function:\n\n- Removes the outermost quotes and unescapes the return values.\n- Returns a SQL `    NULL` if a non-scalar value is selected.\n- Uses single quotes and brackets to escape invalid [JSONPath](#JSONPath_format) characters in JSON keys. For example: `    ['a.b']` .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    \n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path` : The [JSONPath](#JSONPath_format) . This identifies the data that\nyou want to obtain from the input. If this optional parameter is not\nprovided, then the JSONPath `    $` symbol is applied, which means that all of\nthe data is analyzed.\n    \n    If `    json_path` returns a JSON `    null` or a non-scalar value (in other words,\nif `    json_path` refers to an object or an array), then a SQL `    NULL` is\nreturned.\n    \n    \n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see [Differences between the JSON and JSON-formatted STRING types](#differences_json_and_string) .\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\nIn the following example, `age` is extracted.\n\nThe following example compares how results are returned for the `JSON_EXTRACT` and `JSON_EXTRACT_SCALAR` functions.\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those\ncharacters using single quotes and brackets, `[' ']` . For example:\n\n\n"
  },
  {
    "name": "JSON_EXTRACT_STRING_ARRAY",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nExtracts a JSON array of scalar values and converts it to a SQL `ARRAY&lt;STRING&gt;` value. In addition, this function:\n\n- Removes the outermost quotes and unescapes the values.\n- Returns a SQL `    NULL` if the selected value is not an array or\nnot an array containing only scalar values.\n- Uses single quotes and brackets to escape invalid [JSONPath](#JSONPath_format) characters in JSON keys. For example: `    ['a.b']` .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    \n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path` : The [JSONPath](#JSONPath_format) . This identifies the data that\nyou want to obtain from the input. If this optional parameter is not\nprovided, then the JSONPath `    $` symbol is applied, which means that all of\nthe data is analyzed.\n    \n    \n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see [Differences between the JSON and JSON-formatted STRING types](#differences_json_and_string) .\n\nCaveats:\n\n- A JSON `    null` in the input array produces a SQL `    NULL` as the output for that\nJSON `    null` . If the output contains a `    NULL` array element, an error is\nproduced because the final output cannot be an array with `    NULL` values.\n- If a JSONPath matches an array that contains scalar objects and a JSON `    null` ,\nthen the output of the function must be transformed because the final output\ncannot be an array with `    NULL` values.\n\n **Return type** \n\n `ARRAY&lt;STRING&gt;` \n\n **Examples** \n\nThis extracts items in JSON to a string array:\n\nThe following example compares how results are returned for the `JSON_EXTRACT_ARRAY` and `JSON_EXTRACT_STRING_ARRAY` functions.\n\nThis extracts the items in a JSON-formatted string to a string array:\n\nThis extracts a string array and converts it to an integer array:\n\nThese are equivalent:\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those\ncharacters using single quotes and brackets: `[' ']` . For example:\n\nThe following examples explore how invalid requests and empty arrays are\nhandled:\n\n\n"
  },
  {
    "name": "JSON_KEYS",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Preview** \n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\"\n    in the General Service Terms section of the [Service Specific Terms](/terms/service-terms) .\n    Pre-GA products and features are available \"as is\" and might have\n    limited support. For more information, see the [launch stage descriptions](/products#product-launch-stages) .\n\n **Note:** To provide feedback or request support for this feature, send an email to [bigquery-sql-preview-support@google.com](mailto:bigquery-sql-preview-support@google.com) . **Description** \n\nExtracts unique JSON keys from a JSON expression.\n\nArguments:\n\n-  `    json_expr` : `    JSON` . For example:\n    \n    \n-  `    max_depth` : An `    INT64` value that represents the maximum depth of nested\nfields to search in `    json_expr` .\n    \n    \n-  `    mode` : A named argument with a `    STRING` value that can be one of the\nfollowing:\n    \n    \n    -  `        strict` (default): Ignore any key that appears in an array.\n    -  `        lax` : Also include keys contained in non-consecutively nested arrays.\n    -  `        lax recursive` : Return all keys.\n\nDetails:\n\n- Keys are de-duplicated and returned in alphabetical order.\n- Keys do not include array indices.\n- Keys containing special characters are escaped using double quotes.\n- Keys are case sensitive and not normalized.\n- If `    json_expr` or `    mode` is SQL `    NULL` , the function returns SQL `    NULL` .\n- If `    max_depth` is SQL `    NULL` , the function ignores the argument.\n- If `    max_depth` is less than or equal to 0, then an error is returned.\n\n **Return type** \n\n `ARRAY&lt;STRING&gt;` \n\n **Examples** \n\nIn the following example, there are no arrays, so all keys are returned.\n\nIn the following example, `max_depth` is set to 1 so \"a.b\" is not included.\n\nIn the following example, `json_expr` contains an array. Because the mode is `strict` , keys inside the array are excluded.\n\nIn the following example, `json_expr` contains an array. Because the mode is `lax` , keys inside the array are included.\n\nIn the following example, `json_expr` contains consecutively nested arrays.\nBecause the mode is `lax` , keys inside the consecutively nested arrays are not\nincluded.\n\nIn the following example, `json_expr` contains consecutively nested arrays.\nBecause the mode is `lax recursive` , every key is returned.\n\nIn the following example, `json_expr` contains multiple arrays. Because the\narrays are not consecutively nested and the mode is `lax` , keys inside the\narrays are included.\n\nIn the following example, `json_expr` contains both consecutively nested and\nsingle arrays. Because the mode is `lax` , keys inside the consecutively nested\narrays are excluded.\n\nIn the following example, `json_expr` contains both consecutively nested and\nsingle arrays. Because the mode is `lax recursive` , all keys are included.\n\n\n"
  },
  {
    "name": "JSON_OBJECT",
    "arguments": [],
    "category": "JSON",
    "description_markdown": "-  [Signature 1](#json_object_signature1) : `    JSON_OBJECT([json_key, json_value][, ...])` \n-  [Signature 2](#json_object_signature2) : `    JSON_OBJECT(json_key_array, json_value_array)` \n\n\n<span id=\"json_object_signature1\"></span>\n#### Signature 1\n\n\n **Description** \n\nCreates a JSON object, using key-value pairs.\n\nArguments:\n\n-  `    json_key` : A `    STRING` value that represents a key.\n-  `    json_value` : A [JSON encoding-supported](#json_encodings) value.\n\nDetails:\n\n- If two keys are passed in with the same name, only the first key-value pair\nis preserved.\n- The order of key-value pairs is not preserved.\n- If `    json_key` is `    NULL` , an error is produced.\n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nYou can create an empty JSON object by passing in no JSON keys and values.\nFor example:\n\nYou can create a JSON object by passing in key-value pairs. For example:\n\nAn error is produced if a SQL `NULL` is passed in for a JSON key.\n\nAn error is produced if the number of JSON keys and JSON values don't match:\n\n\n<span id=\"json_object_signature2\"></span>\n#### Signature 2\n\n\nCreates a JSON object, using an array of keys and values.\n\nArguments:\n\n-  `    json_key_array` : An array of zero or more `    STRING` keys.\n-  `    json_value_array` : An array of zero or more [JSON encoding-supported](#json_encodings) values.\n\nDetails:\n\n- If two keys are passed in with the same name, only the first key-value pair\nis preserved.\n- The order of key-value pairs is not preserved.\n- The number of keys must match the number of values, otherwise an error is\nproduced.\n- If any argument is `    NULL` , an error is produced.\n- If a key in `    json_key_array` is `    NULL` , an error is produced.\n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nYou can create an empty JSON object by passing in an empty array of\nkeys and values. For example:\n\nYou can create a JSON object by passing in an array of keys and an array of\nvalues. For example:\n\nThe following query groups by `id` and then creates an array of keys and\nvalues from the rows with the same `id` :\n\nAn error is produced if the size of the JSON keys and values arrays don't\nmatch:\n\nAn error is produced if the array of JSON keys or JSON values is a SQL `NULL` .\n\n\n"
  },
  {
    "name": "JSON_QUERY",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nExtracts a JSON value and converts it to a SQL\nJSON-formatted `STRING` or `JSON` value.\nThis function uses double quotes to escape invalid [JSONPath](#JSONPath_format) characters in JSON keys. For example: `\"a.b\"` .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    Extracts a SQL `    NULL` when a JSON-formatted string `    null` is encountered.\nFor example:\n    \n    \n-  `    json_expr` : JSON. For example:\n    \n    Extracts a JSON `    null` when a JSON `    null` is encountered.\n    \n    \n-  `    json_path` : The [JSONPath](#JSONPath_format) . This identifies the data that\nyou want to obtain from the input. This function\nlets you [specify a mode](#JSONPath_mode) for the JSONPath.\n    \n    \n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see [Differences between the JSON and JSON-formatted STRING types](#differences_json_and_string) .\n\n **Return type** \n\n-  `    json_string_expr` : A JSON-formatted `    STRING` \n-  `    json_expr` : `    JSON` \n\n **Examples** \n\nIn the following example, JSON data is extracted and returned as JSON.\n\nIn the following examples, JSON data is extracted and returned as\nJSON-formatted strings.\n\nIn the following examples, the JSON data is extracted in [lax mode](#JSONPath_mode) .\nBecause the keyword `lax` is included in the `JSONPath` , JSON arrays are\nautomatically unwrapped.\n\nIn the following examples, the JSON data is extracted in [lax recursive mode](#JSONPath_mode) .\nBecause the keyword `lax recursive` is included in the `JSONPath` , JSON arrays\nare unwrapped until a non-array type is found.\n\nIn the following examples, the keywords `lax` and `lax recursive` indicate that\nnon-array types should be wrapped into arrays of size 1 before matching. The\nmodes `lax` and `lax recursive` behave identically for wrapping arrays.\n\n\n"
  },
  {
    "name": "JSON_QUERY_ARRAY",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nExtracts a JSON array and converts it to\na SQL `ARRAY&lt;JSON-formatted STRING&gt;` or `ARRAY&lt;JSON&gt;` value.\nIn addition, this function uses double quotes to escape invalid [JSONPath](#JSONPath_format) characters in JSON keys. For example: `\"a.b\"` .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    \n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path` : The [JSONPath](#JSONPath_format) . This identifies the data that\nyou want to obtain from the input. If this optional parameter is not\nprovided, then the JSONPath `    $` symbol is applied, which means that all of\nthe data is analyzed.\n    \n    \n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see [Differences between the JSON and JSON-formatted STRING types](#differences_json_and_string) .\n\n **Return type** \n\n-  `    json_string_expr` : `    ARRAY&lt;JSON-formatted STRING&gt;` \n-  `    json_expr` : `    ARRAY&lt;JSON&gt;` \n\n **Examples** \n\nThis extracts items in JSON to an array of `JSON` values:\n\nThis extracts the items in a JSON-formatted string to a string array:\n\nThis extracts a string array and converts it to an integer array:\n\nThis extracts string values in a JSON-formatted string to an array:\n\nThis extracts only the items in the `fruit` property to an array:\n\nThese are equivalent:\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those\ncharacters using double quotes: `\" \"` . For example:\n\nThe following examples show how invalid requests and empty arrays are handled:\n\n\n"
  },
  {
    "name": "JSON_REMOVE",
    "arguments": [],
    "category": "JSON",
    "description_markdown": "Produces a new SQL `JSON` value with the specified JSON data removed.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path` : Remove data at this [JSONPath](#JSONPath_format) in `    json_expr` .\n    \n    \n\nDetails:\n\n- Paths are evaluated left to right. The JSON produced by evaluating the\nfirst path is the JSON for the next path.\n- The operation ignores non-existent paths and continue processing the rest\nof the paths.\n- For each path, the entire matched JSON subtree is deleted.\n- If the path matches a JSON object key, this function deletes the\nkey-value pair.\n- If the path matches an array element, this function deletes the specific\nelement from the matched array.\n- If removing the path results in an empty JSON object or empty JSON array,\nthe empty structure is preserved.\n- If `    json_path` is `    $` or an invalid [JSONPath](#JSONPath_format) , an error is\nproduced.\n- If `    json_path` is SQL `    NULL` , the path operation is ignored.\n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nIn the following example, the path `$[1]` is matched and removes `[\"b\", \"c\"]` .\n\nYou can use the field access operator to pass JSON data into this function.\nFor example:\n\nIn the following example, the first path `$[1]` is matched and removes `[\"b\", \"c\"]` . Then, the second path `$[1]` is matched and removes `\"d\"` .\n\nThe structure of an empty array is preserved when all elements are deleted\nfrom it. For example:\n\nIn the following example, the path `$.a.b.c` is matched and removes the `\"c\":\"d\"` key-value pair from the JSON object.\n\nIn the following example, the path `$.a.b` is matched and removes the `\"b\": {\"c\":\"d\"}` key-value pair from the JSON object.\n\nIn the following example, the path `$.b` is not valid, so the operation makes\nno changes.\n\nIn the following example, path `$.a.b` and `$.b` don't exist, so those\noperations are ignored, but the others are processed.\n\nIf you pass in `$` as the path, an error is produced. For example:\n\nIn the following example, the operation is ignored because you can't remove\ndata from a JSON null.\n\n\n"
  },
  {
    "name": "JSON_SET",
    "arguments": [],
    "category": "JSON",
    "description_markdown": "Produces a new SQL `JSON` value with the specified JSON data inserted\nor replaced.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path_value_pair` : A value and the [JSONPath](#JSONPath_format) for\nthat value. This includes:\n    \n    \n    -  `        json_path` : Insert or replace `        value` at this [JSONPath](#JSONPath_format) in `        json_expr` .\n        \n        \n    -  `        value` : A [JSON encoding-supported](#json_encodings) value to\ninsert.\n        \n        \n-  `    create_if_missing` : A named argument that takes a `    BOOL` value.\n    \n    \n    - If `        TRUE` (default), replaces or inserts data if the path doesn't exist.\n        \n        \n    - If `        FALSE` , only existing JSONPath values are replaced. If the path\ndoesn't exist, the set operation is ignored.\n        \n        \n\nDetails:\n\n- Path value pairs are evaluated left to right. The JSON produced by\nevaluating one pair becomes the JSON against which the next pair\nis evaluated.\n- If a matched path has an existing value, it overwrites the existing data\nwith `    value` .\n- If `    create_if_missing` is `    TRUE` :\n    \n    \n    - If a path doesn't exist, the remainder of the path is recursively\n created.\n    - If the matched path prefix points to a JSON null, the remainder of the\n path is recursively created, and `        value` is inserted.\n    - If a path token points to a JSON array and the specified index is *larger* than the size of the array, pads the JSON array with JSON\n nulls, recursively creates the remainder of the path at the specified\n index, and inserts the path value pair.\n- This function applies all path value pair set operations even if an\nindividual path value pair operation is invalid. For invalid operations,\nthe operation is ignored and the function continues to process the rest\nof the path value pairs.\n    \n    \n- If the path exists but has an incompatible type at any given path\ntoken, no update happens for that specific path value pair.\n    \n    \n- If any `    json_path` is an invalid [JSONPath](#JSONPath_format) , an error is\nproduced.\n    \n    \n- If `    json_expr` is SQL `    NULL` , the function returns SQL `    NULL` .\n    \n    \n- If `    json_path` is SQL `    NULL` , the `    json_path_value_pair` operation is\nignored.\n    \n    \n- If `    create_if_missing` is SQL `    NULL` , the set operation is ignored.\n    \n    \n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nIn the following example, the path `$` matches the entire `JSON` value\nand replaces it with `{\"b\": 2, \"c\": 3}` .\n\nIn the following example, `create_if_missing` is `FALSE` and the path `$.b` doesn't exist, so the set operation is ignored.\n\nIn the following example, `create_if_missing` is `TRUE` and the path `$.a` exists, so the value is replaced.\n\nIn the following example, the path `$.a` is matched, but `$.a.b` does not\nexist, so the new path and the value are inserted.\n\nIn the following example, the path prefix `$` points to a JSON null, so the\nremainder of the path is created for the value `100` .\n\nIn the following example, the path `$.a.c` implies that the value at `$.a` is\na JSON object but it's not. This part of the operation is ignored, but the other\nparts of the operation are completed successfully.\n\nIn the following example, the path `$.a[2]` implies that the value for `$.a` is\nan array, but it's not, so the operation is ignored for that value.\n\nIn the following example, the path `$[1]` is matched and replaces the\narray element value with `foo` .\n\nIn the following example, the path `$[1][0]` is matched and replaces the\narray element value with `foo` .\n\nIn the following example, the path prefix `$` points to a JSON null, so the\nremainder of the path is created. The resulting array is padded with\nJSON nulls and appended with `foo` .\n\nIn the following example, the path `$[1]` is matched, the matched array is\nextended since `$[1][4]` is larger than the existing array, and then `foo` is\ninserted in the array.\n\nIn the following example, the path `$[1][0][0]` implies that the value of `$[1][0]` is an array, but it is not, so the operation is ignored.\n\nIn the following example, the path `$[1][2]` is larger than the length of\nthe matched array. The array length is extended and the remainder of the path\nis recursively created. The operation continues to the path `$[1][2][1]` and inserts `foo` .\n\nIn the following example, because the `JSON` object is empty, key `b` is\ninserted, and the remainder of the path is recursively created.\n\nIn the following example, multiple values are set.\n\n\n"
  },
  {
    "name": "JSON_STRIP_NULLS",
    "arguments": [],
    "category": "JSON",
    "description_markdown": "Recursively removes JSON nulls from JSON objects and JSON arrays.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path` : Remove JSON nulls at this [JSONPath](#JSONPath_format) for `    json_expr` .\n    \n    \n-  `    include_arrays` : A named argument that's either `    TRUE` (default) or `    FALSE` . If `    TRUE` or omitted, the function removes\n JSON nulls from JSON arrays. If `    FALSE` , does not.\n    \n    \n-  `    remove_empty` : A named argument that's either `    TRUE` or `    FALSE` (default). If `    TRUE` , the function removes empty\n JSON objects after JSON nulls are removed. If `    FALSE` or omitted, does not.\n    \n    If `    remove_empty` is `    TRUE` and `    include_arrays` is `    TRUE` or omitted,\nthe function additionally removes empty JSON arrays.\n    \n    \n\nDetails:\n\n- If a value is a JSON null, the associated key-value pair is removed.\n- If `    remove_empty` is set to `    TRUE` , the function recursively removes empty\ncontainers after JSON nulls are removed.\n- If the function generates JSON with nothing in it, the function returns a\nJSON null.\n- If `    json_path` is an invalid [JSONPath](#JSONPath_format) , an error is\nproduced.\n- If `    json_expr` is SQL `    NULL` , the function returns SQL `    NULL` .\n- If `    json_path` , `    include_arrays` , or `    remove_empty` is SQL `    NULL` , the\nfunction returns `    json_expr` .\n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nIn the following example, all JSON nulls are removed.\n\nIn the following example, all JSON nulls are removed from a JSON array.\n\nIn the following example, `include_arrays` is set as `FALSE` so that JSON nulls\nare not removed from JSON arrays.\n\nIn the following example, `remove_empty` is omitted and defaults to `FALSE` , and the empty structures are retained.\n\nIn the following example, `remove_empty` is set as `TRUE` , and the\nempty structures are removed.\n\nIn the following examples, `remove_empty` is set as `TRUE` , and the\nempty structures are removed. Because no JSON data is left the function\nreturns JSON null.\n\nIn the following example, empty structures are removed for JSON objects,\nbut not JSON arrays.\n\nIn the following example, empty structures are removed for both JSON objects,\nand JSON arrays.\n\nIn the following example, because no JSON data is left, the function returns a\nJSON null.\n\n\n"
  },
  {
    "name": "JSON_TYPE",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nGets the JSON type of the outermost JSON value and converts the name of\nthis type to a SQL `STRING` value. The names of these JSON types can be\nreturned: `object` , `array` , `string` , `number` , `boolean` , `null` \n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    If this expression is SQL `    NULL` , the function returns SQL `    NULL` . If the\nextracted JSON value is not a valid JSON type, an error is produced.\n    \n    \n\n **Return type** \n\n `STRING` \n\n **Examples** \n\n\n"
  },
  {
    "name": "JSON_VALUE",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nExtracts a JSON scalar value and converts it to a SQL `STRING` value.\nIn addition, this function:\n\n- Removes the outermost quotes and unescapes the values.\n- Returns a SQL `    NULL` if a non-scalar value is selected.\n- Uses double quotes to escape invalid [JSONPath](#JSONPath_format) characters\nin JSON keys. For example: `    \"a.b\"` .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    \n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path` : The [JSONPath](#JSONPath_format) . This identifies the data that\nyou want to obtain from the input. If this optional parameter is not\nprovided, then the JSONPath `    $` symbol is applied, which means that all of\nthe data is analyzed.\n    \n    If `    json_path` returns a JSON `    null` or a non-scalar value (in other words,\nif `    json_path` refers to an object or an array), then a SQL `    NULL` is\nreturned.\n    \n    \n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see [Differences between the JSON and JSON-formatted STRING types](#differences_json_and_string) .\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\nIn the following example, JSON data is extracted and returned as a scalar value.\n\nThe following example compares how results are returned for the `JSON_QUERY` and `JSON_VALUE` functions.\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those\ncharacters using double quotes. For example:\n\n\n"
  },
  {
    "name": "JSON_VALUE_ARRAY",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nExtracts a JSON array of scalar values and converts it to a SQL `ARRAY&lt;STRING&gt;` value.\nIn addition, this function:\n\n- Removes the outermost quotes and unescapes the values.\n- Returns a SQL `    NULL` if the selected value is not an array or\nnot an array containing only scalar values.\n- Uses double quotes to escape invalid [JSONPath](#JSONPath_format) characters\nin JSON keys. For example: `    \"a.b\"` .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    \n-  `    json_expr` : JSON. For example:\n    \n    \n-  `    json_path` : The [JSONPath](#JSONPath_format) . This identifies the data that\nyou want to obtain from the input. If this optional parameter is not\nprovided, then the JSONPath `    $` symbol is applied, which means that all of\nthe data is analyzed.\n    \n    \n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see [Differences between the JSON and JSON-formatted STRING types](#differences_json_and_string) .\n\nCaveats:\n\n- A JSON `    null` in the input array produces a SQL `    NULL` as the output for\nJSON `    null` . If the output contains a `    NULL` array element, an error is\nproduced because the final output cannot be an array with `    NULL` values.\n- If a JSONPath matches an array that contains scalar objects and a JSON `    null` ,\nthen the output of the function must be transformed because the final output\ncannot be an array with `    NULL` values.\n\n **Return type** \n\n `ARRAY&lt;STRING&gt;` \n\n **Examples** \n\nThis extracts items in JSON to a string array:\n\nThe following example compares how results are returned for the `JSON_QUERY_ARRAY` and `JSON_VALUE_ARRAY` functions.\n\nThis extracts the items in a JSON-formatted string to a string array:\n\nThis extracts a string array and converts it to an integer array:\n\nThese are equivalent:\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those\ncharacters using double quotes: `\" \"` . For example:\n\nThe following examples explore how invalid requests and empty arrays are\nhandled:\n\n\n"
  },
  {
    "name": "JUSTIFY_DAYS",
    "arguments": [],
    "category": "Interval",
    "description_markdown": " **Description** \n\nNormalizes the day part of the interval to the range from -29 to 29 by\nincrementing/decrementing the month or year part of the interval.\n\n **Return Data Type** \n\n `INTERVAL` \n\n **Example** \n\n\n"
  },
  {
    "name": "JUSTIFY_HOURS",
    "arguments": [],
    "category": "Interval",
    "description_markdown": " **Description** \n\nNormalizes the time part of the interval to the range from -23:59:59.999999 to\n23:59:59.999999 by incrementing/decrementing the day part of the interval.\n\n **Return Data Type** \n\n `INTERVAL` \n\n **Example** \n\n\n"
  },
  {
    "name": "JUSTIFY_INTERVAL",
    "arguments": [],
    "category": "Interval",
    "description_markdown": " **Description** \n\nNormalizes the days and time parts of the interval.\n\n **Return Data Type** \n\n `INTERVAL` \n\n **Example** \n\n\n"
  },
  {
    "name": "KEYS.ADD_KEY_FROM_RAW_BYTES",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nReturns a serialized keyset as `BYTES` with the\naddition of a key to `keyset` based on `key_type` and `raw_key_bytes` .\n\nThe primary cryptographic key remains the same as in `keyset` . The expected\nlength of `raw_key_bytes` depends on the value of `key_type` . The following are\nsupported `key_types` :\n\n-  `    'AES_CBC_PKCS'` : Creates a key for AES decryption using cipher block chaining\nand PKCS padding. `    raw_key_bytes` is expected to be a raw key `    BYTES` value of length 16, 24, or 32; these\nlengths have sizes of 128, 192, and 256 bits, respectively. GoogleSQL\nAEAD functions do not support keys of these types for encryption; instead,\nprefer `    'AEAD_AES_GCM_256'` or `    'AES_GCM'` keys.\n-  `    'AES_GCM'` : Creates a key for AES decryption or encryption using [Galois/Counter Mode](https://en.wikipedia.org/wiki/Galois/Counter_Mode) . `    raw_key_bytes` must be a raw key `    BYTES` value of length 16 or 32; these lengths have sizes of 128 and 256 bits,\nrespectively. When keys of this type are inputs to `    AEAD.ENCRYPT` , the output\nciphertext does not have a Tink-specific prefix indicating which key was\nused as input.\n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nThe following query creates a table of customer IDs along with raw key bytes,\ncalled `CustomerRawKeys` , and a table of unique IDs, called `CustomerIds` . It\ncreates a new `'AEAD_AES_GCM_256'` keyset for each `customer_id` ; then it adds a\nnew key to each keyset, using the `raw_key_bytes` value corresponding to that `customer_id` . The output is a table where each row contains a `customer_id` and\na keyset in `BYTES` , which contains the raw key added\nusing KEYS.ADD_KEY_FROM_RAW_BYTES.\n\nThe output keysets each contain two things: the primary cryptographic key\ncreated using `KEYS.NEW_KEYSET('AEAD_AES_GCM_256')` , and the raw key added using `KEYS.ADD_KEY_FROM_RAW_BYTES` . If a keyset in the output is used with `AEAD.ENCRYPT` , GoogleSQL uses the primary cryptographic key created\nusing `KEYS.NEW_KEYSET('AEAD_AES_GCM_256')` to encrypt the input plaintext. If\nthe keyset is used with `AEAD.DECRYPT_STRING` or `AEAD.DECRYPT_BYTES` ,\nGoogleSQL returns the resulting plaintext if either key succeeds in\ndecrypting the ciphertext.\n\n\n\n"
  },
  {
    "name": "KEYS.KEYSET_CHAIN",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nCan be used in place of the `keyset` argument to the AEAD\nand deterministic\nencryption functions to pass a [Tink](https://github.com/google/tink/blob/master/docs/KEY-MANAGEMENT.md) keyset that is encrypted\nwith a [Cloud KMS key](/bigquery/docs/aead-encryption-concepts#cloud_kms_protection) . This function lets you use\nother AEAD functions without including plaintext keys in a query.\n\nThis function takes the following arguments:\n\n-  `    kms_resource_name` : A `    STRING` literal that contains the resource path to\nthe Cloud KMS key that's used to decrypt `    first_level_keyset` .\nThis key must reside in the same Cloud region where this function is executed.\nA Cloud KMS key looks like this:\n    \n    \n-  `    first_level_keyset` : A `    BYTES` literal that represents a [keyset](/bigquery/docs/aead-encryption-concepts#keysets) or [wrapped keyset](/bigquery/docs/aead-encryption-concepts#wrapped_keysets) .\n    \n    \n\n **Return Data Type** \n\n `STRUCT` \n\n **Example** \n\nThis example creates a table of example data, then shows how to encrypt that\ndata using a wrapped (encrypted) keyset. Finally it shows how to query the\nencrypted version of the data.\n\nThe following statement creates a table `RawCustomerData` containing a column of\ncustomer ids and a column of favorite animals.\n\nThe following statement creates a table `EncryptedCustomerData` containing a\ncolumn of unique IDs and a column of ciphertext. The statement encrypts the\nplaintext `favorite_animal` using the first_level_keyset provided.\n\nThe following query uses the first_level_keyset to decrypt data in the `EncryptedCustomerData` table.\n\nThe previous two steps also work with the `DETERMINISTIC_ENCRYPT` and `DETERMINISTIC_DECRYPT_BYTES` functions. The wrapped keyset must be created\nusing the `DETERMINISTIC_AEAD_AES_SIV_CMAC_256` type.\n\nThe following statement creates a table `EncryptedCustomerData` containing a\ncolumn of unique IDs and a column of ciphertext. The statement encrypts the\nplaintext `favorite_animal` using the first_level_keyset provided. You can see\nthat the ciphertext for `favorite_animal` is the same for customers 2 and 3\nsince their plaintext `favorite_animal` is the same.\n\nThe following query uses the first_level_keyset to decrypt data in the `EncryptedCustomerData` table.\n\n\n"
  },
  {
    "name": "KEYS.KEYSET_FROM_JSON",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nReturns the input `json_keyset`  `STRING` as\nserialized `BYTES` , which is a valid input for other `KEYS` and `AEAD` functions. The JSON `STRING` must\nbe compatible with the definition of the [google.crypto.tink.Keyset](https://github.com/google/tink/blob/master/proto/tink.proto) protocol buffer message: the JSON keyset should be a JSON object containing\nobjects and name-value pairs corresponding to those in the \"keyset\" message in\nthe google.crypto.tink.Keyset definition. You can convert the output serialized `BYTES` representation back to a JSON `STRING` using `KEYS.KEYSET_TO_JSON` .\n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\n `KEYS.KEYSET_FROM_JSON` takes JSON-formatted `STRING` values like the following:\n\nThe following query creates a new keyset from a JSON-formatted `STRING`  `json_keyset` :\n\nThis returns the `json_keyset` serialized as `BYTES` , like the following:\n\n\n"
  },
  {
    "name": "KEYS.KEYSET_LENGTH",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nReturns the number of keys in the provided keyset.\n\n **Return Data Type** \n\n `INT64` \n\n **Example** \n\nThis example references a JSON-formatted STRING\ncalled `json_keyset` that contains two keys:\n\nThe following query converts `json_keyset` to a keyset and then returns\nthe number of keys in the keyset:\n\n\n"
  },
  {
    "name": "KEYS.KEYSET_TO_JSON",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nReturns a JSON `STRING` representation of the input `keyset` . The returned JSON `STRING` is compatible\nwith the definition of the [google.crypto.tink.Keyset](https://github.com/google/tink/blob/master/proto/tink.proto) protocol buffer message. You can convert the JSON `STRING` representation back to `BYTES` using `KEYS.KEYSET_FROM_JSON` .\n\n **Return Data Type** \n\n `STRING` \n\n **Example** \n\nThe following query returns a new `'AEAD_AES_GCM_256'` keyset as a\nJSON-formatted `STRING` .\n\nThe result is a `STRING` like the following.\n\n\n"
  },
  {
    "name": "KEYS.NEW_KEYSET",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nReturns a serialized keyset containing a new key based on `key_type` . The\nreturned keyset is a serialized `BYTES` representation of [google.crypto.tink.Keyset](https://github.com/google/tink/blob/master/proto/tink.proto) that contains a primary cryptographic key and no additional keys. You can use\nthe keyset with the `AEAD.ENCRYPT` , `AEAD.DECRYPT_BYTES` , and `AEAD.DECRYPT_STRING` functions for encryption and decryption, as well as with\nthe `KEYS` group of key- and keyset-related functions.\n\n `key_type` is a `STRING` literal representation of the type of key to create. `key_type` cannot be `NULL` . `key_type` can be:\n\n-  `    AEAD_AES_GCM_256` : Creates a 256-bit key with the pseudo-random number\ngenerator provided by [boringSSL](https://boringssl.googlesource.com/boringssl/) . The key uses AES-GCM for\nencryption and decryption operations.\n-  `    DETERMINISTIC_AEAD_AES_SIV_CMAC_256` :\nCreates a 512-bit `    AES-SIV-CMAC` key, which contains a 256-bit `    AES-CTR` key\nand 256-bit `    AES-CMAC` key. The `    AES-SIV-CMAC` key is created with the\npseudo-random number generator provided by [boringSSL](https://boringssl.googlesource.com/boringssl/) . The key\nuses AES-SIV for encryption and decryption operations.\n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nThe following query creates a keyset for each row in `CustomerIds` , which can\nsubsequently be used to encrypt data. Each keyset contains a single encryption\nkey with randomly-generated key data. Each row in the output contains a `customer_id` and an `'AEAD_AES_GCM_256'` key in `BYTES` .\n\n\n"
  },
  {
    "name": "KEYS.NEW_WRAPPED_KEYSET",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nCreates a new keyset and encrypts it with a [Cloud KMS key](/bigquery/docs/aead-encryption-concepts#cloud_kms_protection) .\nReturns the [wrapped keyset](/bigquery/docs/aead-encryption-concepts#wrapped_keysets) as a `BYTES` representation of [google.crypto.tink.Keyset](https://github.com/google/tink/blob/master/proto/tink.proto) that contains a primary cryptographic key and no additional keys.\n\nThis function takes the following arguments:\n\n-  `    kms_resource_name` : A `    STRING` literal representation of the\nCloud KMS key. `    kms_resource_name` cannot be `    NULL` . The\nCloud KMS key must reside in the same Cloud region where this\nfunction is executed. A Cloud KMS key looks like this:\n    \n    \n-  `    key_type` : A `    STRING` literal representation of the keyset type. `    key_type` cannot be `    NULL` but can be one of the following values:\n    \n    \n    -  `        AEAD_AES_GCM_256` : Creates a 256-bit key with the pseudo-random number\ngenerator provided by [boringSSL](https://boringssl.googlesource.com/boringssl/) . The key uses AES-GCM for\nencryption and decryption operations.\n        \n        \n    -  `        DETERMINISTIC_AEAD_AES_SIV_CMAC_256` :\nCreates a 512-bit `        AES-SIV-CMAC` key, which contains a 256-bit `        AES-CTR` key\nand 256-bit `        AES-CMAC` key. The `        AES-SIV-CMAC` key is created with the\npseudo-random number generator provided by [boringSSL](https://boringssl.googlesource.com/boringssl/) . The key\nuses AES-SIV for encryption and decryption operations.\n        \n        \n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nPut the following variables above each example query that you run:\n\nThe following query creates a wrapped keyset, which contains the ciphertext\nproduced by encrypting a [Tink](https://github.com/google/tink/blob/master/proto/tink.proto) keyset\nwith the specified Cloud KMS key. If you run the query multiple times,\nit generates multiple wrapped keysets, and each wrapped keyset is unique to\neach query that is run.\n\nMultiple calls to this function with the same arguments in one query\nreturns the same value. For example, the following query only creates one\nwrapped keyset and returns it for each row in a table called `my_table` .\n\n\n"
  },
  {
    "name": "KEYS.REWRAP_KEYSET",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nRe-encrypts a [wrapped keyset](/bigquery/docs/aead-encryption-concepts#wrapped_keysets) with a new [Cloud KMS key](/bigquery/docs/aead-encryption-concepts#cloud_kms_protection) . Returns the wrapped keyset as a `BYTES` representation of [google.crypto.tink.Keyset](https://github.com/google/tink/blob/master/proto/tink.proto) that contains a primary cryptographic key and no additional keys.\n\nWhen this function is used, a wrapped keyset is decrypted by `source_kms_resource_name` and then re-encrypted by `target_kms_resource_name` .\nDuring this process, the decrypted keyset is never visible to customers.\n\nThis function takes the following arguments:\n\n-  `    source_kms_resource_name` : A `    STRING` literal representation of the\nCloud KMS key you want to replace. This key must reside in the same\nCloud region where this function is executed. A Cloud KMS key looks\nlike this:\n    \n    \n-  `    target_kms_resource_name` : A `    STRING` literal representation of the\nnew Cloud KMS key that you want to use.\n    \n    \n-  `    wrapped_keyset` : A `    BYTES` literal representation of the\nkeyset that you want to re-encrypt.\n    \n    \n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nPut the following variables above each example query that you run:\n\nThe following query rewraps a wrapped keyset. If you run the query multiple\ntimes, it generates multiple wrapped keysets, and each wrapped keyset is unique\nto each query that is run.\n\nMultiple calls to this function with the same arguments in one query\nreturns the same value. For example, the following query only creates one\nwrapped keyset and returns it for each row in a table called `my_table` .\n\n\n"
  },
  {
    "name": "KEYS.ROTATE_KEYSET",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nAdds a new key to `keyset` based on `key_type` . This new key becomes the primary\ncryptographic key of the new keyset. Returns the new keyset serialized as `BYTES` .\n\nThe old primary cryptographic key from the input `keyset` remains an additional\nkey in the returned keyset.\n\nThe new `key_type` must match the key type of existing keys in the `keyset` .\n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nThe following statement creates a table containing a column of unique `customer_id` values and `'AEAD_AES_GCM_256'` keysets. Then, it creates a new\nprimary cryptographic key within each keyset in the source table using `KEYS.ROTATE_KEYSET` . Each row in the output contains a `customer_id` and an `'AEAD_AES_GCM_256'` keyset in `BYTES` .\n\n\n"
  },
  {
    "name": "KEYS.ROTATE_WRAPPED_KEYSET",
    "arguments": [],
    "category": "AEAD_encryption",
    "description_markdown": " **Description** \n\nTakes an existing [wrapped keyset](/bigquery/docs/aead-encryption-concepts#wrapped_keysets) and returns a rotated and\nrewrapped keyset. The returned wrapped keyset is a `BYTES` representation of [google.crypto.tink.Keyset](https://github.com/google/tink/blob/master/proto/tink.proto) .\n\nWhen this function is used, the wrapped keyset is decrypted,\nthe new key is added, and then the keyset is re-encrypted. The primary\ncryptographic key from the input `wrapped_keyset` remains as an\nadditional key in the returned keyset. During this rotation process,\nthe decrypted keyset is never visible to customers.\n\nThis function takes the following arguments:\n\n-  `    kms_resource_name` : A `    STRING` literal representation of the [Cloud KMS key](/bigquery/docs/aead-encryption-concepts#cloud_kms_protection) that was used to wrap the\nwrapped keyset. The Cloud KMS key must reside in the same Cloud\nregion where this function is executed. A Cloud KMS key looks like\nthis:\n    \n    \n-  `    wrapped_keyset` : A `    BYTES` literal representation of the\nexisting keyset that you want to work with.\n    \n    \n-  `    key_type` : A `    STRING` literal representation of the keyset type. This must\nmatch the key type of existing keys in `    wrapped_keyset` .\n    \n    \n\n **Return Data Type** \n\n `BYTES` \n\n **Example** \n\nPut the following variables above each example query that you run:\n\nThe following query rotates a wrapped keyset. If you run the query multiple\ntimes, it generates multiple wrapped keysets, and each wrapped keyset is unique\nto each query that is run.\n\nMultiple calls to this function with the same arguments in one query\nreturns the same value. For example, the following query only creates one\nwrapped keyset and returns it for each row in a table called `my_table` .\n\n\n<span id=\"aggregate_functions\"></span>\n## Aggregate functions\n\n\nGoogleSQL for BigQuery supports the following general aggregate functions.\nTo learn about the syntax for aggregate function calls, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n\n\n"
  },
  {
    "name": "LAG",
    "arguments": [],
    "category": "Navigation",
    "description_markdown": " **Description** \n\nReturns the value of the `value_expression` on a preceding row. Changing the `offset` value changes which preceding row is returned; the default value is `1` , indicating the previous row in the window frame. An error occurs if `offset` is NULL or a negative value.\n\nThe optional `default_expression` is used if there isn't a row in the window\nframe at the specified offset. This expression must be a constant expression and\nits type must be implicitly coercible to the type of `value_expression` . If left\nunspecified, `default_expression` defaults to NULL.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\n-  `    value_expression` can be any data type that can be returned from an\nexpression.\n-  `    offset` must be a non-negative integer literal or parameter.\n-  `    default_expression` must be compatible with the value expression type.\n\n **Return Data Type** \n\nSame type as `value_expression` .\n\n **Examples** \n\nThe following example illustrates a basic use of the `LAG` function.\n\nThis next example uses the optional `offset` parameter.\n\nThe following example replaces NULL values with a default value.\n\n\n"
  },
  {
    "name": "LAST_DAY",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nReturns the last day from a date expression. This is commonly used to return\nthe last day of the month.\n\nYou can optionally specify the date part for which the last day is returned.\nIf this parameter is not used, the default value is `MONTH` . `LAST_DAY` supports the following values for `date_part` :\n\n-  `    YEAR` \n-  `    QUARTER` \n-  `    MONTH` \n-  `    WEEK` . Equivalent to 7 `    DAY` s.\n-  `    WEEK(&lt;WEEKDAY&gt;)` . `    &lt;WEEKDAY&gt;` represents the starting day of the week.\nValid values are `    SUNDAY` , `    MONDAY` , `    TUESDAY` , `    WEDNESDAY` , `    THURSDAY` , `    FRIDAY` , and `    SATURDAY` .\n-  `    ISOWEEK` . Uses [ISO 8601](https://en.wikipedia.org/wiki/ISO_week_date) week boundaries. ISO weeks begin\non Monday.\n-  `    ISOYEAR` . Uses the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year boundary.\nThe ISO year boundary is the Monday of the first week whose Thursday belongs\nto the corresponding Gregorian calendar year.\n\n **Return Data Type** \n\n `DATE` \n\n **Example** \n\nThese both return the last day of the month:\n\nThis returns the last day of the year:\n\nThis returns the last day of the week for a week that starts on a Sunday:\n\nThis returns the last day of the week for a week that starts on a Monday:\n\n\n"
  },
  {
    "name": "LAST_VALUE",
    "arguments": [],
    "category": "Navigation",
    "description_markdown": " **Description** \n\nReturns the value of the `value_expression` for the last row in the current\nwindow frame.\n\nThis function includes `NULL` values in the calculation unless `IGNORE NULLS` is\npresent. If `IGNORE NULLS` is present, the function excludes `NULL` values from\nthe calculation.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\n `value_expression` can be any data type that an expression can return.\n\n **Return Data Type** \n\nSame type as `value_expression` .\n\n **Examples** \n\nThe following example computes the slowest time for each division.\n\n\n"
  },
  {
    "name": "LAX_BOOL",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nAttempts to convert a JSON value to a SQL `BOOL` value.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n\nDetails:\n\n- If `    json_expr` is SQL `    NULL` , the function returns SQL `    NULL` .\n- See the conversion rules in the next section for additional `    NULL` handling.\n\n **Conversion rules** \n\n| From JSON type | To SQL `BOOL` |\n| --- | --- |\n| boolean | If the JSON boolean is `true` , returns `TRUE` .\n      Otherwise, returns `FALSE` . |\n| string | If the JSON string is `'true'` , returns `TRUE` .\n      If the JSON string is `'false'` , returns `FALSE` .\n      If the JSON string is any other value or has whitespace in it,\n      returns `NULL` .\n      This conversion is case-insensitive. |\n| number | If the JSON number is a representation of `0` ,\n      returns `FALSE` . Otherwise, returns `TRUE` . |\n| other type or null |  `NULL` |\n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\nExample with input that is a JSON boolean:\n\nExamples with inputs that are JSON strings:\n\nExamples with inputs that are JSON numbers:\n\n\n"
  },
  {
    "name": "LAX_FLOAT64",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nAttempts to convert a JSON value to a\nSQL `FLOAT64` value.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n\nDetails:\n\n- If `    json_expr` is SQL `    NULL` , the function returns SQL `    NULL` .\n- See the conversion rules in the next section for additional `    NULL` handling.\n\n **Conversion rules** \n\n| From JSON type | To SQL `FLOAT64` |\n| --- | --- |\n| boolean |  `NULL` |\n| string | If the JSON string represents a JSON number, parses it as\n      a `BIGNUMERIC` value, and then safe casts the result as a `FLOAT64` value.\n      If the JSON string can't be converted, returns `NULL` . |\n| number | Casts the JSON number as a `FLOAT64` value.\n      Large JSON numbers are rounded. |\n| other type or null |  `NULL` |\n\n **Return type** \n\n `FLOAT64` \n\n **Examples** \n\nExamples with inputs that are JSON numbers:\n\nExamples with inputs that are JSON booleans:\n\nExamples with inputs that are JSON strings:\n\n\n"
  },
  {
    "name": "LAX_INT64",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nAttempts to convert a JSON value to a SQL `INT64` value.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n\nDetails:\n\n- If `    json_expr` is SQL `    NULL` , the function returns SQL `    NULL` .\n- See the conversion rules in the next section for additional `    NULL` handling.\n\n **Conversion rules** \n\n| From JSON type | To SQL `INT64` |\n| --- | --- |\n| boolean | If the JSON boolean is `true` , returns `1` .\n      If `false` , returns `0` . |\n| string | If the JSON string represents a JSON number, parses it as\n      a `BIGNUMERIC` value, and then safe casts the results as an `INT64` value.\n      If the JSON string can't be converted, returns `NULL` . |\n| number | Casts the JSON number as an `INT64` value.\n      If the JSON number can't be converted, returns `NULL` . |\n| other type or null |  `NULL` |\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\nExamples with inputs that are JSON numbers:\n\nExamples with inputs that are JSON booleans:\n\nExamples with inputs that are JSON strings:\n\n\n"
  },
  {
    "name": "LAX_STRING",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nAttempts to convert a JSON value to a SQL `STRING` value.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    \n\nDetails:\n\n- If `    json_expr` is SQL `    NULL` , the function returns SQL `    NULL` .\n- See the conversion rules in the next section for additional `    NULL` handling.\n\n **Conversion rules** \n\n| From JSON type | To SQL `STRING` |\n| --- | --- |\n| boolean | If the JSON boolean is `true` , returns `'true'` .\n      If `false` , returns `'false'` . |\n| string | Returns the JSON string as a `STRING` value. |\n| number | Returns the JSON number as a `STRING` value. |\n| other type or null |  `NULL` |\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\nExamples with inputs that are JSON strings:\n\nExamples with inputs that are JSON booleans:\n\nExamples with inputs that are JSON numbers:\n\n\n"
  },
  {
    "name": "LEAD",
    "arguments": [],
    "category": "Navigation",
    "description_markdown": " **Description** \n\nReturns the value of the `value_expression` on a subsequent row. Changing the `offset` value changes which subsequent row is returned; the default value is `1` , indicating the next row in the window frame. An error occurs if `offset` is\nNULL or a negative value.\n\nThe optional `default_expression` is used if there isn't a row in the window\nframe at the specified offset. This expression must be a constant expression and\nits type must be implicitly coercible to the type of `value_expression` . If left\nunspecified, `default_expression` defaults to NULL.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\n-  `    value_expression` can be any data type that can be returned from an\nexpression.\n-  `    offset` must be a non-negative integer literal or parameter.\n-  `    default_expression` must be compatible with the value expression type.\n\n **Return Data Type** \n\nSame type as `value_expression` .\n\n **Examples** \n\nThe following example illustrates a basic use of the `LEAD` function.\n\nThis next example uses the optional `offset` parameter.\n\nThe following example replaces NULL values with a default value.\n\n\n"
  },
  {
    "name": "LEAST",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns the least value among `X1,...,XN` . If any argument is `NULL` , returns `NULL` . Otherwise, in the case of floating-point arguments, if any argument is `NaN` , returns `NaN` . In all other cases, returns the value among `X1,...,XN` that has the least value according to the ordering used by the `ORDER BY` clause. The arguments `X1, ..., XN` must be coercible to a common supertype, and\nthe supertype must support ordering.\n\n| X1,...,XN | LEAST(X1,...,XN) |\n| --- | --- |\n| 3,5,1 | 1 |\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Return Data Types** \n\nData type of the input values.\n\n\n\n"
  },
  {
    "name": "LEFT",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns a `STRING` or `BYTES` value that consists of the specified\nnumber of leftmost characters or bytes from `value` . The `length` is an `INT64` that specifies the length of the returned\nvalue. If `value` is of type `BYTES` , `length` is the number of leftmost bytes\nto return. If `value` is `STRING` , `length` is the number of leftmost characters\nto return.\n\nIf `length` is 0, an empty `STRING` or `BYTES` value will be\nreturned. If `length` is negative, an error will be returned. If `length` exceeds the number of characters or bytes from `value` , the original `value` will be returned.\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "LENGTH",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns the length of the `STRING` or `BYTES` value. The returned\nvalue is in characters for `STRING` arguments and in bytes for the `BYTES` argument.\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "LN",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the natural logarithm of X. Generates an error if X is less than or\nequal to zero.\n\n| X | LN(X) |\n| --- | --- |\n| 1.0 | 0.0 |\n|  `+inf` |  `+inf` |\n|  `X &lt; 0` | Error |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "LOG",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nIf only X is present, `LOG` is a synonym of `LN` . If Y is also present, `LOG` computes the logarithm of X to base Y.\n\n| X | Y | LOG(X, Y) |\n| --- | --- | --- |\n| 100.0 | 10.0 | 2.0 |\n|  `-inf` | Any value |  `NaN` |\n| Any value |  `+inf` |  `NaN` |\n|  `+inf` | 0.0 < Y < 1.0 |  `-inf` |\n|  `+inf` | Y > 1.0 |  `+inf` |\n| X <= 0 | Any value | Error |\n| Any value | Y <= 0 | Error |\n| Any value | 1.0 | Error |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n|  `INT64` |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `NUMERIC` |  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "LOG10",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nSimilar to `LOG` , but computes logarithm to base 10.\n\n| X | LOG10(X) |\n| --- | --- |\n| 100.0 | 2.0 |\n|  `-inf` |  `NaN` |\n|  `+inf` |  `+inf` |\n| X <= 0 | Error |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "LOGICAL_AND",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns the logical AND of all non- `NULL` expressions. Returns `NULL` if there\nare zero input rows or `expression` evaluates to `NULL` for all rows.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\n **Supported Argument Types** \n\n `BOOL` \n\n **Return Data Types** \n\n `BOOL` \n\n **Examples** \n\n `LOGICAL_AND` returns `FALSE` because not all of the values in the array are\nless than 3.\n\n\n"
  },
  {
    "name": "LOGICAL_OR",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns the logical OR of all non- `NULL` expressions. Returns `NULL` if there\nare zero input rows or `expression` evaluates to `NULL` for all rows.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\n **Supported Argument Types** \n\n `BOOL` \n\n **Return Data Types** \n\n `BOOL` \n\n **Examples** \n\n `LOGICAL_OR` returns `TRUE` because at least one of the values in the array is\nless than 3.\n\n\n"
  },
  {
    "name": "LOWER",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nFor `STRING` arguments, returns the original string with all alphabetic\ncharacters in lowercase. Mapping between lowercase and uppercase is done\naccording to the [Unicode Character Database](http://unicode.org/ucd/) without taking into account language-specific mappings.\n\nFor `BYTES` arguments, the argument is treated as ASCII text, with all bytes\ngreater than 127 left intact.\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "LPAD",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns a `STRING` or `BYTES` value that consists of `original_value` prepended\nwith `pattern` . The `return_length` is an `INT64` that\nspecifies the length of the returned value. If `original_value` is of type `BYTES` , `return_length` is the number of bytes. If `original_value` is\nof type `STRING` , `return_length` is the number of characters.\n\nThe default value of `pattern` is a blank space.\n\nBoth `original_value` and `pattern` must be the same data type.\n\nIf `return_length` is less than or equal to the `original_value` length, this\nfunction returns the `original_value` value, truncated to the value of `return_length` . For example, `LPAD('hello world', 7);` returns `'hello w'` .\n\nIf `original_value` , `return_length` , or `pattern` is `NULL` , this function\nreturns `NULL` .\n\nThis function returns an error if:\n\n-  `    return_length` is negative\n-  `    pattern` is empty\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "LTRIM",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nIdentical to [TRIM](#trim) , but only removes leading characters.\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "MAKE_INTERVAL",
    "arguments": [],
    "category": "Interval",
    "description_markdown": " **Description** \n\nConstructs an [INTERVAL](/bigquery/docs/reference/standard-sql/data-types#interval_type) object using `INT64` values\nrepresenting the year, month, day, hour, minute, and second. All arguments are\noptional, `0` by default, and can be [named arguments](/bigquery/docs/reference/standard-sql/functions-reference#named_arguments) .\n\n **Return Data Type** \n\n `INTERVAL` \n\n **Example** \n\n\n<span id=\"json_functions\"></span>\n## JSON functions\n\n\nGoogleSQL for BigQuery supports the following functions, which can retrieve and\ntransform JSON data.\n\n\n\n"
  },
  {
    "name": "MAX",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns the maximum non- `NULL` value in an aggregated group.\n\nCaveats:\n\n- If the aggregated group is empty or the argument is `    NULL` for all rows in\nthe group, returns `    NULL` .\n- If the argument is `    NaN` for any row in the group, returns `    NaN` .\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Supported Argument Types** \n\nAny [orderable data type](/bigquery/docs/reference/standard-sql/data-types#data_type_properties) except for `ARRAY` .\n\n **Return Data Types** \n\nThe data type of the input values.\n\n **Examples** \n\n\n"
  },
  {
    "name": "MAX_BY",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nSynonym for [ANY_VALUE(x HAVING MAX y)](#any_value) .\n\n **Return Data Types** \n\nMatches the input `x` data type.\n\n **Examples** \n\n\n"
  },
  {
    "name": "MD5",
    "arguments": [],
    "category": "Hash",
    "description_markdown": " **Description** \n\nComputes the hash of the input using the [MD5 algorithm](https://en.wikipedia.org/wiki/MD5) . The input can either be `STRING` or `BYTES` . The string version treats the input as an array of bytes.\n\nThis function returns 16 bytes.\n\n **Warning:** MD5 is no longer considered secure.\nFor increased security use another hashing function. **Return type** \n\n `BYTES` \n\n **Example** \n\n\n"
  },
  {
    "name": "MIN",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns the minimum non- `NULL` value in an aggregated group.\n\nCaveats:\n\n- If the aggregated group is empty or the argument is `    NULL` for all rows in\nthe group, returns `    NULL` .\n- If the argument is `    NaN` for any row in the group, returns `    NaN` .\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Supported Argument Types** \n\nAny [orderable data type](/bigquery/docs/reference/standard-sql/data-types#data_type_properties) except for `ARRAY` .\n\n **Return Data Types** \n\nThe data type of the input values.\n\n **Examples** \n\n\n"
  },
  {
    "name": "MIN_BY",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nSynonym for [ANY_VALUE(x HAVING MIN y)](#any_value) .\n\n **Return Data Types** \n\nMatches the input `x` data type.\n\n **Examples** \n\n\n"
  },
  {
    "name": "MOD",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nModulo function: returns the remainder of the division of X by Y. Returned\nvalue has the same sign as X. An error is generated if Y is 0.\n\n| X | Y | MOD(X, Y) |\n| --- | --- | --- |\n| 25 | 12 | 1 |\n| 25 | 0 | Error |\n\n **Return Data Type** \n\nThe return data type is determined by the argument types with the following\ntable.\n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |\n| --- | --- | --- | --- |\n|  `INT64` |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |\n|  `NUMERIC` |  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |\n\n\n\n"
  },
  {
    "name": "NET.HOST",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** \n\nTakes a URL as a `STRING` value and returns the host. For best results, URL\nvalues should comply with the format as defined by [RFC 3986](https://tools.ietf.org/html/rfc3986#appendix-A) . If the URL value does not comply\nwith RFC 3986 formatting, this function makes a best effort to parse the input\nand return a relevant result. If the function cannot parse the input, it\nreturns `NULL` .\n\n **Note:** The function does not perform any normalization. **Return Data Type** \n\n `STRING` \n\n **Example** \n\n| input | description | host | suffix | domain |\n| --- | --- | --- | --- | --- |\n| \"\" | invalid input | NULL | NULL | NULL |\n| \"http://abc.xyz\" | standard URL | \"abc.xyz\" | \"xyz\" | \"abc.xyz\" |\n| \"//user:password@a.b:80/path?query\" | standard URL with relative scheme, port, path and query, but no public suffix | \"a.b\" | NULL | NULL |\n| \"https://[::1]:80\" | standard URL with IPv6 host | \"[::1]\" | NULL | NULL |\n| \"http://例子.卷筒纸.中国\" | standard URL with internationalized domain name | \"例子.卷筒纸.中国\" | \"中国\" | \"卷筒纸.中国\" |\n| \"    www.Example.Co.UK    \" | non-standard URL with spaces, upper case letters, and without scheme | \"www.Example.Co.UK\" | \"Co.UK\" | \"Example.Co.UK\" |\n| \"mailto:?to=&subject=&body=\" | URI rather than URL--unsupported | \"mailto\" | NULL | NULL |\n\n\n\n"
  },
  {
    "name": "NET.IPV4_FROM_INT64",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** \n\nConverts an IPv4 address from integer format to binary (BYTES) format in network\nbyte order. In the integer input, the least significant bit of the IP address is\nstored in the least significant bit of the integer, regardless of host or client\narchitecture. For example, `1` means `0.0.0.1` , and `0x1FF` means `0.0.1.255` .\n\nThis function checks that either all the most significant 32 bits are 0, or all\nthe most significant 33 bits are 1 (sign-extended from a 32-bit integer).\nIn other words, the input should be in the range `[-0x80000000, 0xFFFFFFFF]` ;\notherwise, this function throws an error.\n\nThis function does not support IPv6.\n\n **Return Data Type** \n\nBYTES\n\n **Example** \n\n\n"
  },
  {
    "name": "NET.IPV4_TO_INT64",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** \n\nConverts an IPv4 address from binary (BYTES) format in network byte order to\ninteger format. In the integer output, the least significant bit of the IP\naddress is stored in the least significant bit of the integer, regardless of\nhost or client architecture. For example, `1` means `0.0.0.1` , and `0x1FF` means `0.0.1.255` . The output is in the range `[0, 0xFFFFFFFF]` .\n\nIf the input length is not 4, this function throws an error.\n\nThis function does not support IPv6.\n\n **Return Data Type** \n\nINT64\n\n **Example** \n\n\n"
  },
  {
    "name": "NET.IP_FROM_STRING",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** \n\nConverts an IPv4 or IPv6 address from text (STRING) format to binary (BYTES)\nformat in network byte order.\n\nThis function supports the following formats for `addr_str` :\n\n- IPv4: Dotted-quad format. For example, `    10.1.2.3` .\n- IPv6: Colon-separated format. For example, `    1234:5678:90ab:cdef:1234:5678:90ab:cdef` . For more examples, see the [IP Version 6 Addressing Architecture](http://www.ietf.org/rfc/rfc2373.txt) .\n\nThis function does not support [CIDR notation](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) , such as `10.1.2.3/32` .\n\nIf this function receives a `NULL` input, it returns `NULL` . If the input is\nconsidered invalid, an `OUT_OF_RANGE` error occurs.\n\n **Return Data Type** \n\nBYTES\n\n **Example** \n\n\n"
  },
  {
    "name": "NET.IP_NET_MASK",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** \n\nReturns a network mask: a byte sequence with length equal to `num_output_bytes` ,\nwhere the first `prefix_length` bits are set to 1 and the other bits are set to\n0. `num_output_bytes` and `prefix_length` are INT64.\nThis function throws an error if `num_output_bytes` is not 4 (for IPv4) or 16\n(for IPv6). It also throws an error if `prefix_length` is negative or greater\nthan `8 * num_output_bytes` .\n\n **Return Data Type** \n\nBYTES\n\n **Example** \n\n\n"
  },
  {
    "name": "NET.IP_TO_STRING",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** Converts an IPv4 or IPv6 address from binary (BYTES) format in network byte\norder to text (STRING) format.\n\nIf the input is 4 bytes, this function returns an IPv4 address as a STRING. If\nthe input is 16 bytes, it returns an IPv6 address as a STRING.\n\nIf this function receives a `NULL` input, it returns `NULL` . If the input has\na length different from 4 or 16, an `OUT_OF_RANGE` error occurs.\n\n **Return Data Type** \n\nSTRING\n\n **Example** \n\n\n"
  },
  {
    "name": "NET.IP_TRUNC",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** Takes `addr_bin` , an IPv4 or IPv6 address in binary (BYTES) format in network\nbyte order, and returns a subnet address in the same format. The result has the\nsame length as `addr_bin` , where the first `prefix_length` bits are equal to\nthose in `addr_bin` and the remaining bits are 0.\n\nThis function throws an error if `LENGTH(addr_bin)` is not 4 or 16, or if `prefix_len` is negative or greater than `LENGTH(addr_bin) * 8` .\n\n **Return Data Type** \n\nBYTES\n\n **Example** \n\n\n"
  },
  {
    "name": "NET.PUBLIC_SUFFIX",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** \n\nTakes a URL as a `STRING` value and returns the public suffix (such as `com` , `org` , or `net` ). A public suffix is an ICANN domain registered at [publicsuffix.org](https://publicsuffix.org/list/) . For best results, URL values\nshould comply with the format as defined by [RFC 3986](https://tools.ietf.org/html/rfc3986#appendix-A) . If the URL value does not comply\nwith RFC 3986 formatting, this function makes a best effort to parse the input\nand return a relevant result.\n\nThis function returns `NULL` if any of the following is true:\n\n- It cannot parse the host from the input;\n- The parsed host contains adjacent dots in the middle\n(not leading or trailing);\n- The parsed host does not contain any public suffix.\n\nBefore looking up the public suffix, this function temporarily normalizes the\nhost by converting uppercase English letters to lowercase and encoding all\nnon-ASCII characters with [Punycode](https://en.wikipedia.org/wiki/Punycode) .\nThe function then returns the public suffix as part of the original host instead\nof the normalized host.\n\n **Note:** The function does not perform [Unicode normalization](https://en.wikipedia.org/wiki/Unicode_equivalence) . **Note:** The public suffix data at [publicsuffix.org](https://publicsuffix.org/list/) also contains\nprivate domains. This function ignores the private domains. **Note:** The public suffix data may change over time. Consequently, input that\nproduces a `NULL` result now may produce a non- `NULL` value in the future. **Return Data Type** \n\n `STRING` \n\n **Example** \n\n| input | description | host | suffix | domain |\n| --- | --- | --- | --- | --- |\n| \"\" | invalid input | NULL | NULL | NULL |\n| \"http://abc.xyz\" | standard URL | \"abc.xyz\" | \"xyz\" | \"abc.xyz\" |\n| \"//user:password@a.b:80/path?query\" | standard URL with relative scheme, port, path and query, but no public suffix | \"a.b\" | NULL | NULL |\n| \"https://[::1]:80\" | standard URL with IPv6 host | \"[::1]\" | NULL | NULL |\n| \"http://例子.卷筒纸.中国\" | standard URL with internationalized domain name | \"例子.卷筒纸.中国\" | \"中国\" | \"卷筒纸.中国\" |\n| \"    www.Example.Co.UK    \" | non-standard URL with spaces, upper case letters, and without scheme | \"www.Example.Co.UK\" | \"Co.UK\" | \"Example.Co.UK |\n| \"mailto:?to=&subject=&body=\" | URI rather than URL--unsupported | \"mailto\" | NULL | NULL |\n\n\n\n"
  },
  {
    "name": "NET.REG_DOMAIN",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** \n\nTakes a URL as a string and returns the registered or registrable domain (the [public suffix](#netpublic_suffix) plus one preceding label), as a\nstring. For best results, URL values should comply with the format as defined by [RFC 3986](https://tools.ietf.org/html/rfc3986#appendix-A) . If the URL value does not comply\nwith RFC 3986 formatting, this function makes a best effort to parse the input\nand return a relevant result.\n\nThis function returns `NULL` if any of the following is true:\n\n- It cannot parse the host from the input;\n- The parsed host contains adjacent dots in the middle\n(not leading or trailing);\n- The parsed host does not contain any public suffix;\n- The parsed host contains only a public suffix without any preceding label.\n\nBefore looking up the public suffix, this function temporarily normalizes the\nhost by converting uppercase English letters to lowercase and encoding all\nnon-ASCII characters with [Punycode](https://en.wikipedia.org/wiki/Punycode) . The function then\nreturns the registered or registerable domain as part of the original host\ninstead of the normalized host.\n\n **Note:** The function does not perform [Unicode normalization](https://en.wikipedia.org/wiki/Unicode_equivalence) . **Note:** The public suffix data at [publicsuffix.org](https://publicsuffix.org/list/) also contains\nprivate domains. This function does not treat a private domain as a public\nsuffix. For example, if `us.com` is a private domain in the public suffix data, `NET.REG_DOMAIN(\"foo.us.com\")` returns `us.com` (the public suffix `com` plus\nthe preceding label `us` ) rather than `foo.us.com` (the private domain `us.com` plus the preceding label `foo` ). **Note:** The public suffix data may change over time.\nConsequently, input that produces a `NULL` result now may produce a non- `NULL` value in the future. **Return Data Type** \n\n `STRING` \n\n **Example** \n\n| input | description | host | suffix | domain |\n| --- | --- | --- | --- | --- |\n| \"\" | invalid input | NULL | NULL | NULL |\n| \"http://abc.xyz\" | standard URL | \"abc.xyz\" | \"xyz\" | \"abc.xyz\" |\n| \"//user:password@a.b:80/path?query\" | standard URL with relative scheme, port, path and query, but no public suffix | \"a.b\" | NULL | NULL |\n| \"https://[::1]:80\" | standard URL with IPv6 host | \"[::1]\" | NULL | NULL |\n| \"http://例子.卷筒纸.中国\" | standard URL with internationalized domain name | \"例子.卷筒纸.中国\" | \"中国\" | \"卷筒纸.中国\" |\n| \"    www.Example.Co.UK    \" | non-standard URL with spaces, upper case letters, and without scheme | \"www.Example.Co.UK\" | \"Co.UK\" | \"Example.Co.UK\" |\n| \"mailto:?to=&subject=&body=\" | URI rather than URL--unsupported | \"mailto\" | NULL | NULL |\n\n\n\n"
  },
  {
    "name": "NET.SAFE_IP_FROM_STRING",
    "arguments": [],
    "category": "Net",
    "description_markdown": " **Description** \n\nSimilar to [NET.IP_FROM_STRING](#netip_from_string) , but returns `NULL` instead of throwing an error if the input is invalid.\n\n **Return Data Type** \n\nBYTES\n\n **Example** \n\n\n<span id=\"numbering_functions\"></span>\n## Numbering functions\n\n\nGoogleSQL for BigQuery supports numbering functions.\nNumbering functions are a subset of window functions. To create a\nwindow function call and learn about the syntax for window functions,\nsee [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nNumbering functions assign integer values to each row based on their position\nwithin the specified window. The `OVER` clause syntax varies across\nnumbering functions.\n\n\n\n"
  },
  {
    "name": "NORMALIZE",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes a string value and returns it as a normalized string. If you do not\nprovide a normalization mode, `NFC` is used.\n\n [Normalization](https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization) is used to ensure that\ntwo strings are equivalent. Normalization is often used in situations in which\ntwo strings render the same on the screen but have different Unicode code\npoints.\n\n `NORMALIZE` supports four optional normalization modes:\n\n| Value | Name | Description |\n| --- | --- | --- |\n|  `NFC` | Normalization Form Canonical Composition | Decomposes and recomposes characters by canonical equivalence. |\n|  `NFKC` | Normalization Form Compatibility Composition | Decomposes characters by compatibility, then recomposes them by canonical equivalence. |\n|  `NFD` | Normalization Form Canonical Decomposition | Decomposes characters by canonical equivalence, and multiple combining characters are arranged in a specific order. |\n|  `NFKD` | Normalization Form Compatibility Decomposition | Decomposes characters by compatibility, and multiple combining characters are arranged in a specific order. |\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\nThe following example normalizes different language characters:\n\nThe following examples normalize different space characters:\n\n\n"
  },
  {
    "name": "NORMALIZE_AND_CASEFOLD",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes a string value and returns it as a normalized string. If you do not\nprovide a normalization mode, `NFC` is used.\n\n [Normalization](https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization) is used to ensure that\ntwo strings are equivalent. Normalization is often used in situations in which\ntwo strings render the same on the screen but have different Unicode code\npoints.\n\n [Case folding](https://en.wikipedia.org/wiki/Letter_case#Case_folding) is used for the caseless\ncomparison of strings. If you need to compare strings and case should not be\nconsidered, use `NORMALIZE_AND_CASEFOLD` , otherwise use [NORMALIZE](#normalize) .\n\n `NORMALIZE_AND_CASEFOLD` supports four optional normalization modes:\n\n| Value | Name | Description |\n| --- | --- | --- |\n|  `NFC` | Normalization Form Canonical Composition | Decomposes and recomposes characters by canonical equivalence. |\n|  `NFKC` | Normalization Form Compatibility Composition | Decomposes characters by compatibility, then recomposes them by canonical equivalence. |\n|  `NFD` | Normalization Form Canonical Decomposition | Decomposes characters by canonical equivalence, and multiple combining characters are arranged in a specific order. |\n|  `NFKD` | Normalization Form Compatibility Decomposition | Decomposes characters by compatibility, and multiple combining characters are arranged in a specific order. |\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\n\n"
  },
  {
    "name": "NTH_VALUE",
    "arguments": [],
    "category": "Navigation",
    "description_markdown": " **Description** \n\nReturns the value of `value_expression` at the Nth row of the current window\nframe, where Nth is defined by `constant_integer_expression` . Returns NULL if\nthere is no such row.\n\nThis function includes `NULL` values in the calculation unless `IGNORE NULLS` is\npresent. If `IGNORE NULLS` is present, the function excludes `NULL` values from\nthe calculation.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\n-  `    value_expression` can be any data type that can be returned from an\nexpression.\n-  `    constant_integer_expression` can be any constant expression that returns an\ninteger.\n\n **Return Data Type** \n\nSame type as `value_expression` .\n\n **Examples** \n\n\n"
  },
  {
    "name": "NTILE",
    "arguments": [],
    "category": "Numbering",
    "description_markdown": " **Description** \n\nThis function divides the rows into `constant_integer_expression` buckets based on row ordering and returns the 1-based bucket number that is\nassigned to each row. The number of rows in the buckets can differ by at most 1.\nThe remainder values (the remainder of number of rows divided by buckets) are\ndistributed one for each bucket, starting with bucket 1. If `constant_integer_expression` evaluates to NULL, 0 or negative, an\nerror is provided.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Type** \n\n `INT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "OCTET_LENGTH",
    "arguments": [],
    "category": "String",
    "description_markdown": "Alias for [BYTE_LENGTH](#byte_length) .\n\n\n\n"
  },
  {
    "name": "PARSE_BIGNUMERIC",
    "arguments": [],
    "category": "Conversion",
    "description_markdown": " **Description** \n\nConverts a `STRING` to a `BIGNUMERIC` value.\n\nThe numeric literal contained in the string must not exceed the [maximum precision or range](/bigquery/docs/reference/standard-sql/data-types#decimal_types) of the `BIGNUMERIC` type, or an\nerror occurs. If the number of digits after the decimal point exceeds 38, then\nthe resulting `BIGNUMERIC` value rounds [half away from zero](https://en.wikipedia.org/wiki/Rounding#Round_half_away_from_zero) to have 38 digits after the\ndecimal point.\n\nThis function is similar to using the [CAST AS BIGNUMERIC](#cast_bignumeric) function except that the `PARSE_BIGNUMERIC` function only accepts string inputs\nand allows the following in the string:\n\n- Spaces between the sign (+/-) and the number\n- Signs (+/-) after the number\n\nRules for valid input strings:\n\n| Rule | Example Input | Output |\n| --- | --- | --- |\n| The string can only contain digits, commas, decimal points and signs. | \"- 12,34567,89.0\" | -123456789 |\n| Whitespaces are allowed anywhere except between digits. | \"  -  12.345  \" | -12.345 |\n| Only digits and commas are allowed before the decimal point. | \" 12,345,678\" | 12345678 |\n| Only digits are allowed after the decimal point. | \"1.234 \" | 1.234 |\n| Use `E` or `e` for exponents. After the `e` , digits and a leading sign indicator are allowed. | \" 123.45e-1\" | 12.345 |\n| If the integer part is not empty, then it must contain at least one\n        digit. | \" 0,.12 -\" | -0.12 |\n| If the string contains a decimal point, then it must contain at least\n        one digit. | \" .1\" | 0.1 |\n| The string cannot contain more than one sign. | \" 0.5 +\" | 0.5 |\n\n **Return Data Type** \n\n `BIGNUMERIC` \n\n **Examples** \n\nThis example shows an input with spaces before, after, and between the\nsign and the number:\n\nThis example shows an input with an exponent as well as the sign after the\nnumber:\n\nThis example shows an input with multiple commas in the integer part of the\nnumber:\n\nThis example shows an input with a decimal point and no digits in the whole\nnumber part:\n\n **Examples of invalid inputs** \n\nThis example is invalid because the whole number part contains no digits:\n\nThis example is invalid because there are whitespaces between digits:\n\nThis example is invalid because the number is empty except for an exponent:\n\nThis example is invalid because the string contains multiple signs:\n\nThis example is invalid because the value of the number falls outside the range\nof `BIGNUMERIC` :\n\nThis example is invalid because the string contains invalid characters:\n\n\n"
  },
  {
    "name": "PARSE_DATE",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nConverts a [string representation of date](#format_date) to a `DATE` object.\n\n `format_string` contains the [format elements](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) that define how `date_string` is formatted. Each element in `date_string` must have a corresponding element in `format_string` . The\nlocation of each element in `format_string` must match the location of\neach element in `date_string` .\n\nWhen using `PARSE_DATE` , keep the following in mind:\n\n-  **Unspecified fields.** Any unspecified field is initialized from `    1970-01-01` .\n-  **Case insensitivity.** Names, such as `    Monday` , `    February` , and so on, are\ncase insensitive.\n-  **Whitespace.** One or more consecutive white spaces in the format string\nmatches zero or more consecutive white spaces in the date string. In\naddition, leading and trailing white spaces in the date string are always\nallowed -- even if they are not in the format string.\n-  **Format precedence.** When two (or more) format elements have overlapping\ninformation (for example both `    %F` and `    %Y` affect the year), the last one\ngenerally overrides any earlier ones.\n\n **Return Data Type** \n\nDATE\n\n **Examples** \n\nThis example converts a `MM/DD/YY` formatted string to a `DATE` object:\n\nThis example converts a `YYYYMMDD` formatted string to a `DATE` object:\n\n\n"
  },
  {
    "name": "PARSE_DATETIME",
    "arguments": [],
    "category": "Datetime",
    "description_markdown": " **Description** \n\nConverts a [string representation of a datetime](#format_datetime) to a `DATETIME` object.\n\n `format_string` contains the [format elements](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) that define how `datetime_string` is formatted. Each element in `datetime_string` must have a corresponding element in `format_string` . The\nlocation of each element in `format_string` must match the location of\neach element in `datetime_string` .\n\nThe format string fully supports most format elements, except for `%P` .\n\n `PARSE_DATETIME` parses `string` according to the following rules:\n\n-  **Unspecified fields.** Any unspecified field is initialized from `    1970-01-01 00:00:00.0` . For example, if the year is unspecified then it\ndefaults to `    1970` .\n-  **Case insensitivity.** Names, such as `    Monday` and `    February` ,\nare case insensitive.\n-  **Whitespace.** One or more consecutive white spaces in the format string\nmatches zero or more consecutive white spaces in the `    DATETIME` string. Leading and trailing\nwhite spaces in the `    DATETIME` string are always\nallowed, even if they are not in the format string.\n-  **Format precedence.** When two or more format elements have overlapping\ninformation, the last one generally overrides any earlier ones, with some\nexceptions. For example, both `    %F` and `    %Y` affect the year, so the earlier\nelement overrides the later. See the descriptions\nof `    %s` , `    %C` , and `    %y` in [Supported Format Elements For DATETIME](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) .\n-  **Format divergence.**  `    %p` can be used with `    am` , `    AM` , `    pm` , and `    PM` .\n\n **Return Data Type** \n\n `DATETIME` \n\n **Examples** \n\nThe following examples parse a `STRING` literal as a `DATETIME` .\n\nThe following example parses a `STRING` literal\ncontaining a date in a natural language format as a `DATETIME` .\n\n\n<span id=\"debugging_functions\"></span>\n## Debugging functions\n\n\nGoogleSQL for BigQuery supports the following debugging functions.\n\n\n\n"
  },
  {
    "name": "PARSE_JSON",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nConverts a JSON-formatted `STRING` value to a [JSON value](https://www.json.org/json-en.html) .\n\nArguments:\n\n-  `    json_string_expr` : A JSON-formatted string. For example:\n    \n    \n-  `    wide_number_mode` : A named argument with a `    STRING` value. Determines\nhow to handle numbers that can't be stored in a `    JSON` value without the\nloss of precision. If used, `    wide_number_mode` must include one of the\nfollowing values:\n    \n    \n    -  `        exact` (default): Only accept numbers that can be stored without loss\nof precision. If a number that cannot be stored without loss of\nprecision is encountered, the function throws an error.\n    -  `        round` : If a number that cannot be stored without loss of precision is\nencountered, attempt to round it to a number that can be stored without\nloss of precision. If the number cannot be rounded, the function throws\nan error.If a number appears in a JSON object or array, the `    wide_number_mode` argument is applied to the number in the object or array.\n    \n    \n\nNumbers from the following domains can be stored in JSON without loss of\nprecision:\n\n- 64-bit signed/unsigned integers, such as `    INT64` \n-  `    FLOAT64` \n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nIn the following example, a JSON-formatted string is converted to `JSON` .\n\nThe following queries fail because:\n\n- The number that was passed in cannot be stored without loss of precision.\n-  `    wide_number_mode=&gt;'exact'` is used implicitly in the first query and\nexplicitly in the second query.\n\nThe following query rounds the number to a number that can be stored in JSON.\n\nYou can also use valid JSON-formatted strings that don't represent name/value pairs. For example:\n\n\n"
  },
  {
    "name": "PARSE_NUMERIC",
    "arguments": [],
    "category": "Conversion",
    "description_markdown": " **Description** \n\nConverts a `STRING` to a `NUMERIC` value.\n\nThe numeric literal contained in the string must not exceed the [maximum precision or range](/bigquery/docs/reference/standard-sql/data-types#decimal_types) of the `NUMERIC` type, or an error\noccurs. If the number of digits after the decimal point exceeds nine, then the\nresulting `NUMERIC` value rounds [half away from zero](https://en.wikipedia.org/wiki/Rounding#Round_half_away_from_zero) to have nine digits after the\ndecimal point.\n\nThis function is similar to using the [CAST AS NUMERIC](#cast_numeric) function\nexcept that the `PARSE_NUMERIC` function only accepts string inputs and allows\nthe following in the string:\n\n- Spaces between the sign (+/-) and the number\n- Signs (+/-) after the number\n\nRules for valid input strings:\n\n| Rule | Example Input | Output |\n| --- | --- | --- |\n| The string can only contain digits, commas, decimal points and signs. | \"- 12,34567,89.0\" | -123456789 |\n| Whitespaces are allowed anywhere except between digits. | \"  -  12.345  \" | -12.345 |\n| Only digits and commas are allowed before the decimal point. | \" 12,345,678\" | 12345678 |\n| Only digits are allowed after the decimal point. | \"1.234 \" | 1.234 |\n| Use `E` or `e` for exponents. After\n        the `e` ,\n        digits and a leading sign indicator are allowed. | \" 123.45e-1\" | 12.345 |\n| If the integer part is not empty, then it must contain at least one\n        digit. | \" 0,.12 -\" | -0.12 |\n| If the string contains a decimal point, then it must contain at least\n        one digit. | \" .1\" | 0.1 |\n| The string cannot contain more than one sign. | \" 0.5 +\" | 0.5 |\n\n **Return Data Type** \n\n `NUMERIC` \n\n **Examples** \n\nThis example shows an input with spaces before, after, and between the\nsign and the number:\n\nThis example shows an input with an exponent as well as the sign after the\nnumber:\n\nThis example shows an input with multiple commas in the integer part of the\nnumber:\n\nThis example shows an input with a decimal point and no digits in the whole\nnumber part:\n\n **Examples of invalid inputs** \n\nThis example is invalid because the whole number part contains no digits:\n\nThis example is invalid because there are whitespaces between digits:\n\nThis example is invalid because the number is empty except for an exponent:\n\nThis example is invalid because the string contains multiple signs:\n\nThis example is invalid because the value of the number falls outside the range\nof `BIGNUMERIC` :\n\nThis example is invalid because the string contains invalid characters:\n\n\n"
  },
  {
    "name": "PARSE_TIME",
    "arguments": [],
    "category": "Time",
    "description_markdown": " **Description** \n\nConverts a [string representation of time](#format_time) to a `TIME` object.\n\n `format_string` contains the [format elements](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) that define how `time_string` is formatted. Each element in `time_string` must have a corresponding element in `format_string` . The\nlocation of each element in `format_string` must match the location of\neach element in `time_string` .\n\nThe format string fully supports most format elements except for `%P` .\n\nWhen using `PARSE_TIME` , keep the following in mind:\n\n-  **Unspecified fields.** Any unspecified field is initialized from `    00:00:00.0` . For instance, if `    seconds` is unspecified then it\ndefaults to `    00` , and so on.\n-  **Whitespace.** One or more consecutive white spaces in the format string\nmatches zero or more consecutive white spaces in the `    TIME` string. In\naddition, leading and trailing white spaces in the `    TIME` string are always\nallowed, even if they are not in the format string.\n-  **Format precedence.** When two (or more) format elements have overlapping\ninformation, the last one generally overrides any earlier ones.\n-  **Format divergence.**  `    %p` can be used with `    am` , `    AM` , `    pm` , and `    PM` .\n\n **Return Data Type** \n\n `TIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "PARSE_TIMESTAMP",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nConverts a [string representation of a timestamp](#format_timestamp) to a `TIMESTAMP` object.\n\n `format_string` contains the [format elements](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) that define how `timestamp_string` is formatted. Each element in `timestamp_string` must have a corresponding element in `format_string` . The\nlocation of each element in `format_string` must match the location of\neach element in `timestamp_string` .\n\nThe format string fully supports most format elements, except for `%P` .\n\nWhen using `PARSE_TIMESTAMP` , keep the following in mind:\n\n-  **Unspecified fields.** Any unspecified field is initialized from `    1970-01-01    00:00:00.0` . This initialization value uses the time zone specified by the\nfunction's time zone argument, if present. If not, the initialization value\nuses the default time zone, UTC.  For instance, if the year\nis unspecified then it defaults to `    1970` , and so on.\n-  **Case insensitivity.** Names, such as `    Monday` , `    February` , and so on, are\ncase insensitive.\n-  **Whitespace.** One or more consecutive white spaces in the format string\nmatches zero or more consecutive white spaces in the timestamp string. In\naddition, leading and trailing white spaces in the timestamp string are always\nallowed, even if they are not in the format string.\n-  **Format precedence.** When two (or more) format elements have overlapping\ninformation (for example both `    %F` and `    %Y` affect the year), the last one\ngenerally overrides any earlier ones, with some exceptions (see the\ndescriptions of `    %s` , `    %C` , and `    %y` ).\n-  **Format divergence.**  `    %p` can be used with `    am` , `    AM` , `    pm` , and `    PM` .\n\n **Return Data Type** \n\n `TIMESTAMP` \n\n **Example** \n\n\n"
  },
  {
    "name": "PERCENTILE_CONT",
    "arguments": [],
    "category": "Navigation",
    "description_markdown": " **Description** \n\nComputes the specified percentile value for the value_expression, with linear\ninterpolation.\n\nThis function ignores NULL\nvalues if `RESPECT NULLS` is absent.  If `RESPECT NULLS` is present:\n\n- Interpolation between two `    NULL` values returns `    NULL` .\n- Interpolation between a `    NULL` value and a non- `    NULL` value returns the\nnon- `    NULL` value.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n `PERCENTILE_CONT` can be used with differential privacy. To learn more, see [Differentially private aggregate functions](#aggregate-dp-functions) .\n\n **Supported Argument Types** \n\n-  `    value_expression` and `    percentile` must have one of the following types:\n    -  `        NUMERIC` \n    -  `        BIGNUMERIC` \n    -  `        FLOAT64` \n-  `    percentile` must be a literal in the range `    [0, 1]` .\n\n **Return Data Type** \n\nThe return data type is determined by the argument types with the following\ntable.\n\n| INPUT |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- |\n|  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |\n\n **Examples** \n\nThe following example computes the value for some percentiles from a column of\nvalues while ignoring nulls.\n\nThe following example computes the value for some percentiles from a column of\nvalues while respecting nulls.\n\n\n"
  },
  {
    "name": "PERCENTILE_DISC",
    "arguments": [],
    "category": "Navigation",
    "description_markdown": " **Description** \n\nComputes the specified percentile value for a discrete `value_expression` . The\nreturned value is the first sorted value of `value_expression` with cumulative\ndistribution greater than or equal to the given `percentile` value.\n\nThis function ignores `NULL` values unless `RESPECT NULLS` is present.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\n-  `    value_expression` can be any orderable type.\n-  `    percentile` must be a literal in the range `    [0, 1]` , with one of the\nfollowing types:\n    -  `        NUMERIC` \n    -  `        BIGNUMERIC` \n    -  `        FLOAT64` \n\n **Return Data Type** \n\nSame type as `value_expression` .\n\n **Examples** \n\nThe following example computes the value for some percentiles from a column of\nvalues while ignoring nulls.\n\nThe following example computes the value for some percentiles from a column of\nvalues while respecting nulls.\n\n\n<span id=\"net_functions\"></span>\n## Net functions\n\n\nGoogleSQL for BigQuery supports the following Net functions.\n\n\n\n"
  },
  {
    "name": "PERCENT_RANK",
    "arguments": [],
    "category": "Numbering",
    "description_markdown": " **Description** \n\nReturn the percentile rank of a row defined as (RK-1)/(NR-1), where RK is\nthe `RANK` of the row and NR is the number of rows in the partition.\nReturns 0 if NR=1.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "POW",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns the value of X raised to the power of Y. If the result underflows and is\nnot representable, then the function returns a  value of zero.\n\n| X | Y | POW(X, Y) |\n| --- | --- | --- |\n| 2.0 | 3.0 | 8.0 |\n| 1.0 | Any value including `NaN` | 1.0 |\n| Any value including `NaN` | 0 | 1.0 |\n| -1.0 |  `+inf` | 1.0 |\n| -1.0 |  `-inf` | 1.0 |\n| ABS(X) < 1 |  `-inf` |  `+inf` |\n| ABS(X) > 1 |  `-inf` | 0.0 |\n| ABS(X) < 1 |  `+inf` | 0.0 |\n| ABS(X) > 1 |  `+inf` |  `+inf` |\n|  `-inf` | Y < 0 | 0.0 |\n|  `-inf` | Y > 0 |  `-inf` if Y is an odd integer, `+inf` otherwise |\n|  `+inf` | Y < 0 | 0 |\n|  `+inf` | Y > 0 |  `+inf` |\n| Finite value < 0 | Non-integer | Error |\n| 0 | Finite value < 0 | Error |\n\n **Return Data Type** \n\nThe return data type is determined by the argument types with the following\ntable.\n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n|  `INT64` |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `NUMERIC` |  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "POWER",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nSynonym of [POW(X, Y)](#pow) .\n\n\n\n"
  },
  {
    "name": "RAND",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nGenerates a pseudo-random value of type `FLOAT64` in\nthe range of [0, 1), inclusive of 0 and exclusive of 1.\n\n\n\n"
  },
  {
    "name": "RANGE",
    "arguments": [],
    "category": "Range",
    "description_markdown": " **Description** \n\nConstructs a range of [DATE](/bigquery/docs/reference/standard-sql/data-types#date_type) , [DATETIME](/bigquery/docs/reference/standard-sql/data-types#datetime_type) , or [TIMESTAMP](/bigquery/docs/reference/standard-sql/data-types#timestamp_type) values.\n\n **Definitions** \n\n-  `    lower_bound` : The range starts from this value. This can be a `    DATE` , `    DATETIME` , or `    TIMESTAMP` value. If this value is `    NULL` , the range\ndoesn't include a lower bound.\n-  `    upper_bound` : The range ends before this value. This can be a `    DATE` , `    DATETIME` , or `    TIMESTAMP` value. If this value is `    NULL` , the range\ndoesn't include an upper bound.\n\n **Details** \n\n `lower_bound` and `upper_bound` must be of the same data type.\n\nProduces an error if `lower_bound` is greater than or equal to `upper_bound` .\nTo return `NULL` instead, add the `SAFE.` prefix to the function name.\n\n **Return type** \n\n `RANGE&lt;T&gt;` , where `T` is the same data type as the input.\n\n **Examples** \n\nThe following query constructs a date range:\n\nThe following query constructs a datetime range:\n\nThe following query constructs a timestamp range:\n\nThe following query constructs a date range with no lower bound:\n\nThe following query constructs a date range with no upper bound:\n\n\n"
  },
  {
    "name": "RANGE_BUCKET",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\n `RANGE_BUCKET` scans through a sorted array and returns the 0-based position\nof the point's upper bound. This can be useful if you need to group your data to\nbuild partitions, histograms, business-defined rules, and more.\n\n `RANGE_BUCKET` follows these rules:\n\n- If the point exists in the array, returns the index of the next larger value.\n    \n    \n- If the point does not exist in the array, but it falls between two values,\nreturns the index of the larger value.\n    \n    \n- If the point is smaller than the first value in the array, returns 0.\n    \n    \n- If the point is greater than or equal to the last value in the array,\nreturns the length of the array.\n    \n    \n- If the array is empty, returns 0.\n    \n    \n- If the point is `    NULL` or `    NaN` , returns `    NULL` .\n    \n    \n- The data type for the point and array must be compatible.\n    \n    \n\nExecution failure occurs when:\n\n- The array has a `    NaN` or `    NULL` value in it.\n    \n    \n- The array is not sorted in ascending order.\n    \n    \n\n **Parameters** \n\n-  `    point` : A generic value.\n-  `    boundaries_array` : A generic array of values.\n\n **Note:** The data type for `point` and the element type of `boundaries_array` must be equivalent. The data type must be [comparable](/bigquery/docs/reference/standard-sql/data-types#data_type_properties) . **Return Value** \n\n `INT64` \n\n **Examples** \n\nIn a table called `students` , check to see how many records would\nexist in each `age_group` bucket, based on a student's age:\n\n- age_group 0 (age < 10)\n- age_group 1 (age >= 10, age < 20)\n- age_group 2 (age >= 20, age < 30)\n- age_group 3 (age >= 30)\n\n\n"
  },
  {
    "name": "RANGE_CONTAINS",
    "arguments": [],
    "category": "Range",
    "description_markdown": "-  [Signature 1](#signature_1) : Checks if every value in one range is\nin another range.\n-  [Signature 2](#signature_2) : Checks if a value is in a range.\n\n\n<span id=\"signature_1_3\"></span>\n#### Signature 1\n\n\n **Description** \n\nChecks if the inner range is in the outer range.\n\n **Definitions** \n\n-  `    outer_range` : The `    RANGE&lt;T&gt;` value to search within.\n-  `    inner_range` : The `    RANGE&lt;T&gt;` value to search for in `    outer_range` .\n\n **Details** \n\nReturns `TRUE` if `inner_range` exists in `outer_range` .\nOtherwise, returns `FALSE` .\n\n `T` must be of the same type for all inputs.\n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\nIn the following query, the inner range is in the outer range:\n\nIn the following query, the inner range is not in the outer range:\n\n\n<span id=\"signature_2_3\"></span>\n#### Signature 2\n\n\n **Description** \n\nChecks if a value is in a range.\n\n **Definitions** \n\n-  `    range_to_search` : The `    RANGE&lt;T&gt;` value to search within.\n-  `    value_to_find` : The value to search for in `    range_to_search` .\n\n **Details** \n\nReturns `TRUE` if `value_to_find` exists in `range_to_search` .\nOtherwise, returns `FALSE` .\n\nThe data type for `value_to_find` must be the same data type as `T` in `range_to_search` .\n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\nIn the following query, the value `2022-04-01` is found in the range `[2022-01-01, 2023-01-01)` :\n\nIn the following query, the value `2023-04-01` is not found in the range `[2022-01-01, 2023-01-01)` :\n\n\n"
  },
  {
    "name": "RANGE_END",
    "arguments": [],
    "category": "Range",
    "description_markdown": " **Description** \n\nGets the upper bound of a range.\n\n **Definitions** \n\n-  `    range_to_check` : The `    RANGE&lt;T&gt;` value.\n\n **Details** \n\nReturns `NULL` if the upper bound in `range_value` is `UNBOUNDED` .\n\nReturns `NULL` if `range_to_check` is `NULL` .\n\n **Return type** \n\n `T` in `range_value` \n\n **Examples** \n\nIn the following query, the upper bound of the range is retrieved:\n\nIn the following query, the upper bound of the range is unbounded, so `NULL` is returned:\n\n\n"
  },
  {
    "name": "RANGE_INTERSECT",
    "arguments": [],
    "category": "Range",
    "description_markdown": " **Description** \n\nGets a segment of two ranges that intersect.\n\n **Definitions** \n\n-  `    range_a` : The first `    RANGE&lt;T&gt;` value.\n-  `    range_b` : The second `    RANGE&lt;T&gt;` value.\n\n **Details** \n\nReturns `NULL` if any input is `NULL` .\n\nProduces an error if `range_a` and `range_b` don't overlap. To return `NULL` instead, add the `SAFE.` prefix to the function name.\n\n `T` must be of the same type for all inputs.\n\n **Return type** \n\n `RANGE&lt;T&gt;` \n\n **Examples** \n\n\n"
  },
  {
    "name": "RANGE_OVERLAPS",
    "arguments": [],
    "category": "Range",
    "description_markdown": " **Description** \n\nChecks if two ranges overlap.\n\n **Definitions** \n\n-  `    range_a` : The first `    RANGE&lt;T&gt;` value.\n-  `    range_b` : The second `    RANGE&lt;T&gt;` value.\n\n **Details** \n\nReturns `TRUE` if a part of `range_a` intersects with `range_b` , otherwise\nreturns `FALSE` .\n\n `T` must be of the same type for all inputs.\n\nTo get the part of the range that overlaps, use the [RANGE_INTERSECT](#range_intersect) function.\n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\nIn the following query, the first and second ranges overlap between `2022-02-01` and `2022-04-15` :\n\nIn the following query, the first and second ranges don't overlap:\n\nIn the following query, the first and second ranges overlap between `2022-02-01` and `UNBOUNDED` :\n\n\n"
  },
  {
    "name": "RANGE_SESSIONIZE",
    "arguments": [],
    "category": "Range",
    "description_markdown": " **Description** \n\nProduces a table of sessionized ranges.\n\n **Definitions** \n\n-  `    table_name` : A table expression that represents the name of the table to\nconstruct. This can represent any relation with `    range_column` .\n-  `    range_column` : A `    STRING` literal that indicates which `    RANGE` column\nin a table contains the data to sessionize.\n-  `    partitioning_columns` : An `    ARRAY&lt;STRING&gt;` literal that indicates which\ncolumns should partition the data before the data is sessionized.\n-  `    sessionize_option` : A `    STRING` value that describes how order-adjacent\nranges are sessionized. Your choices are as follows:\n    \n    \n    -  `        MEETS` (default): Ranges that meet or overlap are sessionized.\n        \n        \n    -  `        OVERLAPS` : Only a range that is overlapped by another range is\nsessionized.\n        \n        If this argument is not provided, `    MEETS` is used by default.\n    \n    \n\n **Details** \n\nThis function produces a table that includes all columns in the\ninput table and an additional `RANGE` column called `session_range` , which indicates the start and end of a session. The\nstart and end of each session is determined by the `sessionize_option` argument.\n\n **Return type** \n\n `TABLE` \n\n **Examples** \n\nThe examples in this section reference the following table called `my_sessionized_range_table` in a dataset called `mydataset` :\n\nIn the following query, a table of sessionized data is produced for `my_sessionized_range_table` , and only ranges that meet or overlap are\nsessionized:\n\nIn the following query, a table of sessionized data is produced for `my_sessionized_range_table` , and only a range that is overlapped by another\nrange is sessionized:\n\nIf you need to normalize sessionized data, you can use a query similar to the\nfollowing:\n\n\n"
  },
  {
    "name": "RANGE_START",
    "arguments": [],
    "category": "Range",
    "description_markdown": " **Description** \n\nGets the lower bound of a range.\n\n **Definitions** \n\n-  `    range_to_check` : The `    RANGE&lt;T&gt;` value.\n\n **Details** \n\nReturns `NULL` if the lower bound of `range_value` is `UNBOUNDED` .\n\nReturns `NULL` if `range_to_check` is `NULL` .\n\n **Return type** \n\n `T` in `range_value` \n\n **Examples** \n\nIn the following query, the lower bound of the range is retrieved:\n\nIn the following query, the lower bound of the range is unbounded, so `NULL` is returned:\n\n\n<span id=\"search_functions\"></span>\n## Search functions\n\n\nGoogleSQL for BigQuery supports the following search functions.\n\n\n\n"
  },
  {
    "name": "RANK",
    "arguments": [],
    "category": "Numbering",
    "description_markdown": " **Description** \n\nReturns the ordinal (1-based) rank of each row within the ordered partition.\nAll peer rows receive the same rank value. The next row or set of peer rows\nreceives a rank value which increments by the number of peers with the previous\nrank value, instead of `DENSE_RANK` , which always increments by 1.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "REGEXP_CONTAINS",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns `TRUE` if `value` is a partial match for the regular expression, `regexp` .\n\nIf the `regexp` argument is invalid, the function returns an error.\n\nYou can search for a full match by using `^` (beginning of text) and `$` (end of\ntext). Due to regular expression operator precedence, it is good practice to use\nparentheses around everything between `^` and `$` .\n\n **Note:** GoogleSQL provides regular expression support using the [re2](https://github.com/google/re2/wiki/Syntax) library; see that documentation for its\nregular expression syntax. **Return type** \n\n `BOOL` \n\n **Examples** \n\nThe following queries check to see if an email is valid:\n\n\n"
  },
  {
    "name": "REGEXP_EXTRACT",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns the substring in `value` that matches the [re2 regular expression](https://github.com/google/re2/wiki/Syntax) , `regexp` .\nReturns `NULL` if there is no match.\n\nIf the regular expression contains a capturing group ( `(...)` ), and there is a\nmatch for that capturing group, that match is returned. If there\nare multiple matches for a capturing group, the first match is returned.\n\nIf `position` is specified, the search starts at this\nposition in `value` , otherwise it starts at the beginning of `value` . The `position` must be a positive integer and cannot be 0. If `position` is greater\nthan the length of `value` , `NULL` is returned.\n\nIf `occurrence` is specified, the search returns a specific occurrence of the `regexp` in `value` , otherwise returns the first match. If `occurrence` is\ngreater than the number of matches found, `NULL` is returned. For `occurrence` > 1, the function searches for additional occurrences beginning\nwith the character following the previous occurrence.\n\nReturns an error if:\n\n- The regular expression is invalid\n- The regular expression has more than one capturing group\n- The `    position` is not a positive integer\n- The `    occurrence` is not a positive integer\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "REGEXP_EXTRACT_ALL",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns an array of all substrings of `value` that match the [re2 regular expression](https://github.com/google/re2/wiki/Syntax) , `regexp` . Returns an empty array\nif there is no match.\n\nIf the regular expression contains a capturing group ( `(...)` ), and there is a\nmatch for that capturing group, that match is added to the results.\n\nThe `REGEXP_EXTRACT_ALL` function only returns non-overlapping matches. For\nexample, using this function to extract `ana` from `banana` returns only one\nsubstring, not two.\n\nReturns an error if:\n\n- The regular expression is invalid\n- The regular expression has more than one capturing group\n\n **Return type** \n\n `ARRAY&lt;STRING&gt;` or `ARRAY&lt;BYTES&gt;` \n\n **Examples** \n\n\n"
  },
  {
    "name": "REGEXP_INSTR",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns the lowest 1-based position of a regular expression, `regexp` , in `source_value` . `source_value` and `regexp` must be the same type, either `STRING` or `BYTES` .\n\nIf `position` is specified, the search starts at this position in `source_value` , otherwise it starts at `1` , which is the beginning of `source_value` . `position` is of type `INT64` and must be positive.\n\nIf `occurrence` is specified, the search returns the position of a specific\ninstance of `regexp` in `source_value` . If not specified, `occurrence` defaults\nto `1` and returns the position of the first occurrence.  For `occurrence` > 1,\nthe function searches for the next, non-overlapping occurrence. `occurrence` is of type `INT64` and must be positive.\n\nYou can optionally use `occurrence_position` to specify where a position\nin relation to an `occurrence` starts. Your choices are:\n\n-  `    0` : Returns the start position of `    occurrence` .\n-  `    1` : Returns the end position of `    occurrence` + `    1` . If the\nend of the occurrence is at the end of `    source_value` , `    LENGTH(source_value) + 1` is returned.\n\nReturns `0` if:\n\n- No match is found.\n- If `    occurrence` is greater than the number of matches found.\n- If `    position` is greater than the length of `    source_value` .\n- The regular expression is empty.\n\nReturns `NULL` if:\n\n-  `    position` is `    NULL` .\n-  `    occurrence` is `    NULL` .\n\nReturns an error if:\n\n-  `    position` is `    0` or negative.\n-  `    occurrence` is `    0` or negative.\n-  `    occurrence_position` is neither `    0` nor `    1` .\n- The regular expression is invalid.\n- The regular expression has more than one capturing group.\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "REGEXP_REPLACE",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns a `STRING` where all substrings of `value` that\nmatch regular expression `regexp` are replaced with `replacement` .\n\nYou can use backslashed-escaped digits (\\1 to \\9) within the `replacement` argument to insert text matching the corresponding parenthesized group in the `regexp` pattern. Use \\0 to refer to the entire matching text.\n\nTo add a backslash in your regular expression, you must first escape it. For\nexample, `SELECT REGEXP_REPLACE('abc', 'b(.)', 'X\\\\1');` returns `aXc` . You can\nalso use [raw strings](/bigquery/docs/reference/standard-sql/lexical#string_and_bytes_literals) to remove one layer of\nescaping, for example `SELECT REGEXP_REPLACE('abc', 'b(.)', r'X\\1');` .\n\nThe `REGEXP_REPLACE` function only replaces non-overlapping matches. For\nexample, replacing `ana` within `banana` results in only one replacement, not\ntwo.\n\nIf the `regexp` argument is not a valid regular expression, this function\nreturns an error.\n\n **Note:** GoogleSQL provides regular expression support using the [re2](https://github.com/google/re2/wiki/Syntax) library; see that documentation for its\nregular expression syntax. **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "REGEXP_SUBSTR",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nSynonym for [REGEXP_EXTRACT](#regexp_extract) .\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "REPEAT",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns a `STRING` or `BYTES` value that consists of `original_value` , repeated.\nThe `repetitions` parameter specifies the number of times to repeat `original_value` . Returns `NULL` if either `original_value` or `repetitions` are `NULL` .\n\nThis function returns an error if the `repetitions` value is negative.\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "REPLACE",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReplaces all occurrences of `from_pattern` with `to_pattern` in `original_value` . If `from_pattern` is empty, no replacement is made.\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "REVERSE",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns the reverse of the input `STRING` or `BYTES` .\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "RIGHT",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns a `STRING` or `BYTES` value that consists of the specified\nnumber of rightmost characters or bytes from `value` . The `length` is an `INT64` that specifies the length of the returned\nvalue. If `value` is `BYTES` , `length` is the number of rightmost bytes to\nreturn. If `value` is `STRING` , `length` is the number of rightmost characters\nto return.\n\nIf `length` is 0, an empty `STRING` or `BYTES` value will be\nreturned. If `length` is negative, an error will be returned. If `length` exceeds the number of characters or bytes from `value` , the original `value` will be returned.\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ROUND",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nIf only X is present, rounds X to the nearest integer. If N is present,\nrounds X to N decimal places after the decimal point. If N is negative,\nrounds off digits to the left of the decimal point. Rounds halfway cases\naway from zero. Generates an error if overflow occurs.\n\nIf X is a `NUMERIC` or `BIGNUMERIC` type, then you can\nexplicitly set `rounding_mode` to one of the following:\n\n-  [\"ROUND_HALF_AWAY_FROM_ZERO\"](https://en.wikipedia.org/wiki/Rounding#Rounding_half_away_from_zero) : (Default) Rounds\nhalfway cases away from zero.\n-  [\"ROUND_HALF_EVEN\"](https://en.wikipedia.org/wiki/Rounding#Rounding_half_to_even) : Rounds halfway cases\ntowards the nearest even digit.\n\nIf you set the `rounding_mode` and X is not a `NUMERIC` or `BIGNUMERIC` type,\nthen the function generates an error.\n\n| Expression | Return Value |\n| --- | --- |\n|  `ROUND(2.0)` | 2.0 |\n|  `ROUND(2.3)` | 2.0 |\n|  `ROUND(2.8)` | 3.0 |\n|  `ROUND(2.5)` | 3.0 |\n|  `ROUND(-2.3)` | -2.0 |\n|  `ROUND(-2.8)` | -3.0 |\n|  `ROUND(-2.5)` | -3.0 |\n|  `ROUND(0)` | 0 |\n|  `ROUND(+inf)` |  `+inf` |\n|  `ROUND(-inf)` |  `-inf` |\n|  `ROUND(NaN)` |  `NaN` |\n|  `ROUND(123.7, -1)` | 120.0 |\n|  `ROUND(1.235, 2)` | 1.24 |\n|  `ROUND(NUMERIC \"2.25\", 1, \"ROUND_HALF_EVEN\")` | 2.2 |\n|  `ROUND(NUMERIC \"2.35\", 1, \"ROUND_HALF_EVEN\")` | 2.4 |\n|  `ROUND(NUMERIC \"2.251\", 1, \"ROUND_HALF_EVEN\")` | 2.3 |\n|  `ROUND(NUMERIC \"-2.5\", 0, \"ROUND_HALF_EVEN\")` | -2 |\n|  `ROUND(NUMERIC \"2.5\", 0, \"ROUND_HALF_AWAY_FROM_ZERO\")` | 3 |\n|  `ROUND(NUMERIC \"-2.5\", 0, \"ROUND_HALF_AWAY_FROM_ZERO\")` | -3 |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "ROW_NUMBER",
    "arguments": [],
    "category": "Numbering",
    "description_markdown": " **Description** \n\nDoes not require the `ORDER BY` clause. Returns the sequential\nrow ordinal (1-based) of each row for each ordered partition. If the `ORDER BY` clause is unspecified then the result is\nnon-deterministic.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Type** \n\n `INT64` \n\n **Examples** \n\n\n<span id=\"range_functions\"></span>\n## Range functions\n\n\nGoogleSQL for BigQuery supports the following range functions.\n\n\n\n"
  },
  {
    "name": "RPAD",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns a `STRING` or `BYTES` value that consists of `original_value` appended\nwith `pattern` . The `return_length` parameter is an `INT64` that specifies the length of the\nreturned value. If `original_value` is `BYTES` , `return_length` is the number of bytes. If `original_value` is `STRING` , `return_length` is the number of characters.\n\nThe default value of `pattern` is a blank space.\n\nBoth `original_value` and `pattern` must be the same data type.\n\nIf `return_length` is less than or equal to the `original_value` length, this\nfunction returns the `original_value` value, truncated to the value of `return_length` . For example, `RPAD('hello world', 7);` returns `'hello w'` .\n\nIf `original_value` , `return_length` , or `pattern` is `NULL` , this function\nreturns `NULL` .\n\nThis function returns an error if:\n\n-  `    return_length` is negative\n-  `    pattern` is empty\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "RTRIM",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nIdentical to [TRIM](#trim) , but only removes trailing characters.\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "S2_CELLIDFROMPOINT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the [S2 cell ID](https://s2geometry.io/devguide/s2cell_hierarchy) covering a point `GEOGRAPHY` .\n\n- The optional `    INT64` parameter `    level` specifies the S2 cell level for the\nreturned cell. Naming this argument is optional.\n\nThis is advanced functionality for interoperability with systems utilizing the [S2 Geometry Library](https://s2geometry.io/) .\n\n **Constraints** \n\n- Returns the cell ID as a signed `    INT64` bit-equivalent to [unsigned 64-bit integer representation](https://s2geometry.io/devguide/s2cell_hierarchy) .\n- Can return negative cell IDs.\n- Valid S2 cell levels are 0 to 30.\n-  `    level` defaults to 30 if not explicitly specified.\n- The function only supports a single point GEOGRAPHY. Use the `    SAFE` prefix if\nthe input can be multipoint, linestring, polygon, or an empty `    GEOGRAPHY` .\n- To compute the covering of a complex `    GEOGRAPHY` , use [S2_COVERINGCELLIDS](#s2_coveringcellids) .\n\n **Return type** \n\n `INT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "S2_COVERINGCELLIDS",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns an array of [S2 cell IDs](https://s2geometry.io/devguide/s2cell_hierarchy) that cover the input `GEOGRAPHY` . The function returns at most `max_cells` cells. The optional\narguments `min_level` and `max_level` specify minimum and maximum levels for\nreturned S2 cells. The array size is limited by the optional `max_cells` argument. The optional `buffer` argument specifies a buffering factor in\nmeters; the region being covered is expanded from the extent of the\ninput geography by this amount.\n\nThis is advanced functionality for interoperability with systems utilizing the [S2 Geometry Library](https://s2geometry.io/) .\n\n **Constraints** \n\n- Returns the cell ID as a signed `    INT64` bit-equivalent to [unsigned 64-bit integer representation](https://s2geometry.io/devguide/s2cell_hierarchy) .\n- Can return negative cell IDs.\n- Valid S2 cell levels are 0 to 30.\n-  `    max_cells` defaults to 8 if not explicitly specified.\n-  `    buffer` should be nonnegative. It defaults to 0.0 meters if not explicitly\nspecified.\n\n **Return type** \n\n `ARRAY&lt;INT64&gt;` \n\n **Example** \n\n\n"
  },
  {
    "name": "SAFE_ADD",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nEquivalent to the addition operator ( `+` ), but returns `NULL` if overflow occurs.\n\n| X | Y | SAFE_ADD(X, Y) |\n| --- | --- | --- |\n| 5 | 4 | 9 |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n|  `INT64` |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `NUMERIC` |  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "SAFE_CAST",
    "arguments": [],
    "category": "Conversion",
    "description_markdown": " **Description** \n\nWhen using `CAST` , a query can fail if GoogleSQL is unable to perform\nthe cast. For example, the following query generates an error:\n\nIf you want to protect your queries from these types of errors, you can use `SAFE_CAST` . `SAFE_CAST` replaces runtime errors with `NULL` s.  However, during\nstatic analysis, impossible casts between two non-castable types still produce\nan error because the query is invalid.\n\nSome casts can include a [format clause](/bigquery/docs/reference/standard-sql/format-elements#formatting_syntax) , which provides\ninstructions for how to conduct the\ncast. For example, you could\ninstruct a cast to convert a sequence of bytes to a BASE64-encoded string\ninstead of a UTF-8-encoded string.\n\nThe structure of the format clause is unique to each type of cast and more\ninformation is available in the section for that cast.\n\nIf you are casting from bytes to strings, you can also use the\nfunction, [SAFE_CONVERT_BYTES_TO_STRING](#safe_convert_bytes_to_string) . Any invalid UTF-8 characters\nare replaced with the unicode replacement character, `U+FFFD` .\n\n\n\n"
  },
  {
    "name": "SAFE_CONVERT_BYTES_TO_STRING",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nConverts a sequence of `BYTES` to a `STRING` . Any invalid UTF-8 characters are\nreplaced with the Unicode replacement character, `U+FFFD` .\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\nThe following statement returns the Unicode replacement character, �.\n\n\n"
  },
  {
    "name": "SAFE_DIVIDE",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nEquivalent to the division operator ( `X / Y` ), but returns `NULL` if an error occurs, such as a division by zero error.\n\n| X | Y | SAFE_DIVIDE(X, Y) |\n| --- | --- | --- |\n| 20 | 4 | 5 |\n| 0 | 20 |  `0` |\n| 20 | 0 |  `NULL` |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n|  `INT64` |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `NUMERIC` |  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "SAFE_MULTIPLY",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nEquivalent to the multiplication operator ( `*` ), but returns `NULL` if overflow occurs.\n\n| X | Y | SAFE_MULTIPLY(X, Y) |\n| --- | --- | --- |\n| 20 | 4 | 80 |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n|  `INT64` |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `NUMERIC` |  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "SAFE_NEGATE",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nEquivalent to the unary minus operator ( `-` ), but returns `NULL` if overflow occurs.\n\n| X | SAFE_NEGATE(X) |\n| --- | --- |\n| +1 | -1 |\n| -1 | +1 |\n| 0 | 0 |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "SAFE_SUBTRACT",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns the result of Y subtracted from X.\nEquivalent to the subtraction operator ( `-` ), but returns `NULL` if overflow occurs.\n\n| X | Y | SAFE_SUBTRACT(X, Y) |\n| --- | --- | --- |\n| 5 | 4 | 1 |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n|  `INT64` |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `NUMERIC` |  `NUMERIC` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n|  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "SEARCH",
    "arguments": [],
    "category": "Search",
    "description_markdown": " **Description** \n\nThe `SEARCH` function checks to see whether a BigQuery table or other\nsearch data contains a set of search terms (tokens). It returns `TRUE` if all\nsearch terms appear in the data, based on the [rules for search_query](#search_query_rules) and text analysis described in the [text analyzer](/bigquery/docs/reference/standard-sql/text-analysis) . Otherwise,\nthis function returns `FALSE` .\n\n **Definitions** \n\n<span id=\"data_to_search_arg\"></span>\n\n-  `    data_to_search` : The data to search over. The value can be:\n    \n    \n    - Any GoogleSQL data type literal\n    - A list of columns\n    - A table reference\n    - A column of any typeA table reference is evaluated as a `    STRUCT` whose fields are the columns of\nthe table. `    data_to_search` can be any type, but `    SEARCH` will return `    FALSE` for all types except those listed here:\n    \n    \n    -  `        ARRAY&lt;STRING&gt;` \n    -  `        ARRAY&lt;STRUCT&gt;` \n    -  `        JSON` \n    -  `        STRING` \n    -  `        STRUCT` You can search for string literals in columns of the preceding types.\nFor additional rules, see [Search data rules](#data_to_search_rules) .\n    \n    \n\n<span id=\"search_query_arg\"></span>\n\n-  `    search_query` : A `    STRING` literal, or a `    STRING` constant expression that\nrepresents the terms of the search query. If `    search_query` is `    NULL` , an\nerror is returned. If `    search_query` produces no search tokens,\nand the text analyzer is `    LOG_ANALYZER` or `    PATTERN_ANALYZER` , an error is\nproduced.\n-  `    json_scope` : A named argument with a `    STRING` value.\nTakes one of the following values to indicate the scope of JSON data to be\nsearched. It has no effect if `    data_to_search` isn't a JSON value or\ndoesn't contain a JSON field.\n    \n    \n    -  `        'JSON_VALUES'` (default): Only the JSON values are searched. If `        json_scope` isn't provided, this is used by default.\n        \n        \n    -  `        'JSON_KEYS'` : Only the JSON keys are searched.\n        \n        \n    -  `        'JSON_KEYS_AND_VALUES'` : The JSON keys and values are searched.\n        \n        \n-  `    analyzer` : A named argument with a `    STRING` value. Takes\none of the following values to indicate the text analyzer to use:\n    \n    \n    -  `        'LOG_ANALYZER'` (default): Breaks the input into tokens when delimiters\nare encountered and then normalizes the tokens.\nFor more information, see [LOG_ANALYZER](/bigquery/docs/reference/standard-sql/text-analysis#log_analyzer) .\n        \n        \n    -  `        'NO_OP_ANALYZER'` : Extracts the text as a single token, but\ndoesn't apply normalization. For more information about this analyzer,\nsee [NO_OP_ANALYZER](/bigquery/docs/reference/standard-sql/text-analysis#no_op_analyzer) .\n        \n        \n    -  `        'PATTERN_ANALYZER'` : Breaks the input into tokens that match a\nregular expression. For more information, see [PATTERN_ANALYZER text analyzer](/bigquery/docs/reference/standard-sql/text-analysis#pattern_analyzer) .\n        \n        \n-  `    analyzer_options` : A named argument with a JSON-formatted `    STRING` value.\nTakes a list of text analysis rules. For more information,\nsee [Text analyzer options](/bigquery/docs/reference/standard-sql/text-analysis#text_analyzer_options) .\n    \n    \n\n **Details** \n\nThe `SEARCH` function is designed to work with [search indexes](/bigquery/docs/search-index) to\noptimize point lookups. Although the `SEARCH` function works for\ntables that aren't indexed, its performance will be greatly improved with a\nsearch index. If both the analyzer and analyzer options match the one used\nto create the index, the search index will be used.\n\n<span id=\"text_analyzer\"></span>\n\n<span id=\"search_query_rules\"></span>\n\n **Rules for `search_query` ** \n\nA search query is initially broken down into terms (subqueries) using the\nwhite spaces in  the search query. Each term is then further broken down into\nzero or more searchable tokens based on the text analyzer. This section contains\nthe rules for how different types of terms are analyzed and evaluated.\n\nRules for backticks in [search_query](#search_query_arg) :\n\n- If the `    LOG_ANALYZER` text analyzer is used, text enclosed in backticks\nforces an exact match.\n    \n    For example, `    `Hello World` happy days` becomes `    Hello World` , `    happy` ,\nand `    days` .\n    \n    \n- Search terms enclosed in backticks must match exactly in `    data_to_search` ,\nsubject to the following conditions:\n    \n    \n    - It appears at the start of `        data_to_search` or is immediately preceded\nby a delimiter.\n        \n        \n    - It appears at the end of `        data_to_search` or is immediately followed by\na delimiter.\n        \n        For example, `    SEARCH('foo.bar', '`foo.`')` returns `    FALSE` because the\ntext enclosed in the backticks `    foo.` is immediately followed by the\ncharacter `    b` in the search data `    foo.bar` , rather than by a delimiter or\nthe end of the string. However, `    SEARCH('foo..bar', '`foo.`')` returns `    TRUE` because `    foo.` is immediately followed by the delimiter `    .` in the\nsearch data.\n    \n    \n- The backtick itself can be escaped using a backslash,\nas in `    \\`foobar\\`` .\n    \n    \n- The following are reserved words and must be enclosed\nin backticks:\n    \n     `    AND` , `    NOT` , `    OR` , `    IN` , and `    NEAR` \n    \n    \n\nRules for reserved characters in [search_query](#search_query_arg) :\n\n- Text not enclosed in backticks requires the following\nreserved characters to be escaped by a double backslash `    \\\\` :\n    \n    \n    -  `        [ ] &lt; &gt; ( ) { } | ! ' \" * &amp; ? + / : = - \\ ~ ^` \n        \n        \n    - If the quoted string is preceded by the character `        r` or `        R` , such as `        r\"my\\+string\"` , then it is treated as a raw string and only a single\nbackslash is required to escape the reserved characters. For more\ninformation about raw strings and escape\nsequences, see [String and byte literals](/bigquery/docs/reference/standard-sql/lexical#literals) .\n        \n        \n\nRules for phrases in [search_query](#search_query_arg) :\n\n- A phrase is a type of term. If text is enclosed in double quotes and the `    analyzer` is `    LOG_ANALYZER` , `    PATTERN_ANALYZER` , or not set\n( `    LOG_ANALYZER` by default), the term represents a phrase.\n- When a phrase is analyzed, a subset of tokens is created for that phrase.\nFor example, from the phrase `    \"foo baz.bar\"` , the analyzer called `    LOG_ANALYZER` generates the phrase-specific tokens `    foo` , `    baz` , and `    bar` .\n- The order of terms in a phrase matters. A match is only returned if\nthe tokens that were produced for the phrase are next to each other and in\nthe same order as the tokens for [data_to_search](#data_to_search_arg) .\n    \n    For example:\n    \n    \n- A single quote inside of the phrase is analyzed as a special character.\n    \n    \n- An escaped double quote (double quote after a backslash) is analyzed\nas a double quote character.\n    \n    \n\n<span id=\"search_query_to_search_query\"></span>\n\n **How `search_query` is broken into searchable tokens** \n\nThe following table shows how [search_query](#search_query_arg) is broken into\nsearchable tokens by the `LOG_ANALYZER` text analyzer. All entries are strings.\n\n| search_query | searchable tokens |\n| --- | --- |\n| 127.0.0.1 | 127    \n0    \n1    \n127.0.0.1    \n.\n127.0.0    \n127.0    \n0.0    \n0.0.1    \n0.1 |\n| foobar@example.com | foobar    \nexample    \ncom    \nfoobar@example    \nexample.com    \nfoobar@example.com |\n| The fox. | the    \nfox    \nThe    \nThe fox    \nThe fox.    \nfox    \nfox. |\n\nThe following table shows how `search_query` is broken into searchable tokens\nby the `LOG_ANALYZER` text analyzer. All entries are strings.\n\n| search_query | searchable tokens |\n| --- | --- |\n| 127.0.0.1 | 127    \n0    \n1 |\n| `127.0.0.1` | 127.0.0.1 |\n| foobar@example.com | foobar    \nexample    \ncom |\n| `foobar@example.com` | foobar@example.com |\n\n<span id=\"data_to_search_rules\"></span>\n\n **Rules for `data_to_search` ** \n\nGeneral rules for [data_to_search](#data_to_search_arg) :\n\n-  `    data_to_search` must contain all tokens produced for `    search_query` for the function to return `    TRUE` .\n- To perform a cross-field search, `    data_to_search` must be a `    STRUCT` , `    ARRAY` , or `    JSON` data type.\n- Each `    STRING` field in a compound data type is individually\nsearched for terms.\n- If at least one field in `    data_to_search` includes all search terms\nproduced by `    search_query` , `    SEARCH` returns `    TRUE` . Otherwise it has the\nfollowing behavior:\n    \n    \n    - If at least one `        STRING` field is `        NULL` , `        SEARCH` returns `        NULL` .\n        \n        \n    - Otherwise, `        SEARCH` returns `        FALSE` .\n        \n        \n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\nThe following queries show how tokens in `search_query` are analyzed\nby a `SEARCH` function call using the default analyzer, `LOG_ANALYZER` :\n\nThe following queries show how phrases in `search_query` are analyzed\nby a `SEARCH` function call:\n\nThe following query shows examples of calls to the `SEARCH` function using the `NO_OP_ANALYZER` text analyzer and reasons for various return values:\n\nConsider the following table called `meals` with columns `breakfast` , `lunch` ,\nand `dinner` :\n\nThe following query shows how to search single columns, multiple columns, and\nwhole tables, using the default [LOG_ANALYZER](/bigquery/docs/reference/standard-sql/text-analysis#log_analyzer) text analyzer\nwith the default analyzer options:\n\nThe following query shows additional ways to search, using the\ndefault [LOG_ANALYZER](/bigquery/docs/reference/standard-sql/text-analysis#log_analyzer) text analyzer with\ndefault analyzer options:\n\nThe following query shows additional ways to search, using the\ndefault [LOG_ANALYZER](/bigquery/docs/reference/standard-sql/text-analysis#log_analyzer) text analyzer with custom\nanalyzer options. Terms are only split when a space or `@` symbol is\nencountered.\n\nThe following query shows how to search, using the [NO_OP_ANALYZER](/bigquery/docs/reference/standard-sql/text-analysis#no_op_analyzer) text analyzer:\n\nThe following query shows how to use the [PATTERN_ANALYZER](/bigquery/docs/reference/standard-sql/text-analysis#pattern_analyzer) text analyzer with default analyzer options:\n\nThe following query shows additional ways to search, using the [PATTERN_ANALYZER](/bigquery/docs/reference/standard-sql/text-analysis#pattern_analyzer) text analyzer with\ncustom analyzer options:\n\nFor additional examples that include analyzer options,\nsee the [Text analysis](/bigquery/docs/reference/standard-sql/text-analysis) reference guide.\n\nFor helpful analyzer recipes that you can use to enhance\nanalyzer-supported queries, see the [Search with text analyzers](/bigquery/docs/text-analysis-search) user guide.\n\n\n\n"
  },
  {
    "name": "SEC",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the secant for the angle of `X` , where `X` is specified in radians. `X` can be any data type\nthat [coerces to FLOAT64](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\n\n| X | SEC(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n|  `NULL` |  `NULL` |\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "SECH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the hyperbolic secant for the angle of `X` , where `X` is specified\nin radians. `X` can be any data type\nthat [coerces to FLOAT64](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nNever produces an error.\n\n| X | SECH(X) |\n| --- | --- |\n|  `+inf` |  `0` |\n|  `-inf` |  `0` |\n|  `NaN` |  `NaN` |\n|  `NULL` |  `NULL` |\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "SESSION_USER",
    "arguments": [],
    "category": "Security",
    "description_markdown": " **Description** \n\nFor first-party users, returns the email address of the user that is running the\nquery.\nFor third-party users, returns the [principal identifier](https://cloud.google.com/iam/docs/principal-identifiers) of the user that is running the query.\nFor more information about identities, see [Principals](https://cloud.google.com/docs/authentication#principal) .\n\n **Return Data Type** \n\n `STRING` \n\n **Example** \n\n\n<span id=\"statistical_aggregate_functions\"></span>\n## Statistical aggregate functions\n\n\nGoogleSQL for BigQuery supports statistical aggregate functions.\nTo learn about the syntax for aggregate function calls, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\n\n\n"
  },
  {
    "name": "SHA1",
    "arguments": [],
    "category": "Hash",
    "description_markdown": " **Description** \n\nComputes the hash of the input using the [SHA-1 algorithm](https://en.wikipedia.org/wiki/SHA-1) . The input can either be `STRING` or `BYTES` . The string version treats the input as an array of bytes.\n\nThis function returns 20 bytes.\n\n **Warning:** SHA1 is no longer considered secure.\nFor increased security, use another hashing function. **Return type** \n\n `BYTES` \n\n **Example** \n\n\n"
  },
  {
    "name": "SHA256",
    "arguments": [],
    "category": "Hash",
    "description_markdown": " **Description** \n\nComputes the hash of the input using the [SHA-256 algorithm](https://en.wikipedia.org/wiki/SHA-2) . The input can either be `STRING` or `BYTES` . The string version treats the input as an array of bytes.\n\nThis function returns 32 bytes.\n\n **Return type** \n\n `BYTES` \n\n **Example** \n\n\n"
  },
  {
    "name": "SHA512",
    "arguments": [],
    "category": "Hash",
    "description_markdown": " **Description** \n\nComputes the hash of the input using the [SHA-512 algorithm](https://en.wikipedia.org/wiki/SHA-2) . The input can either be `STRING` or `BYTES` . The string version treats the input as an array of bytes.\n\nThis function returns 64 bytes.\n\n **Return type** \n\n `BYTES` \n\n **Example** \n\n\n<span id=\"hll_functions\"></span>\n## HyperLogLog++ functions\n\n\nThe [HyperLogLog++ algorithm (HLL++)](/bigquery/docs/sketches#sketches_hll) estimates [cardinality](https://en.wikipedia.org/wiki/Cardinality) from [sketches](/bigquery/docs/sketches#sketches_hll) .\n\nHLL++ functions are approximate aggregate functions.\nApproximate aggregation typically requires less\nmemory than exact aggregation functions,\nlike [COUNT(DISTINCT)](#count) , but also introduces statistical error.\nThis makes HLL++ functions appropriate for large data streams for\nwhich linear memory usage is impractical, as well as for data that is\nalready approximate.\n\nIf you do not need materialized sketches, you can alternatively use an [approximate aggregate function with system-defined precision](#approximate_aggregate_functions) ,\nsuch as [APPROX_COUNT_DISTINCT](#approx-count-distinct) . However, `APPROX_COUNT_DISTINCT` does not allow partial aggregations, re-aggregations,\nand custom precision.\n\nGoogleSQL for BigQuery supports the following HLL++ functions:\n\n\n\n"
  },
  {
    "name": "SIGN",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nReturns `-1` , `0` , or `+1` for negative, zero and positive arguments\nrespectively. For floating point arguments, this function does not distinguish\nbetween positive and negative zero.\n\n| X | SIGN(X) |\n| --- | --- |\n| 25 | +1 |\n| 0 | 0 |\n| -25 | -1 |\n| NaN | NaN |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "SIN",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the sine of X where X is specified in radians. Never fails.\n\n| X | SIN(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "SINH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the hyperbolic sine of X where X is specified in radians. Generates\nan error if overflow occurs.\n\n| X | SINH(X) |\n| --- | --- |\n|  `+inf` |  `+inf` |\n|  `-inf` |  `-inf` |\n|  `NaN` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "SOUNDEX",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns a `STRING` that represents the [Soundex](https://en.wikipedia.org/wiki/Soundex) code for `value` .\n\nSOUNDEX produces a phonetic representation of a string. It indexes words by\nsound, as pronounced in English. It is typically used to help determine whether\ntwo strings, such as the family names *Levine* and *Lavine* , or the words *to* and *too* , have similar English-language pronunciation.\n\nThe result of the SOUNDEX consists of a letter followed by 3 digits. Non-latin\ncharacters are ignored. If the remaining string is empty after removing\nnon-Latin characters, an empty `STRING` is returned.\n\n **Return type** \n\n `STRING` \n\n **Examples** \n\n\n"
  },
  {
    "name": "SPLIT",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nSplits a `STRING` or `BYTES` value, using a delimiter. The `delimiter` argument\nmust be a literal character or sequence of characters. You can't split with a\nregular expression.\n\nFor `STRING` , the default delimiter is the comma `,` .\n\nFor `BYTES` , you must specify a delimiter.\n\nSplitting on an empty delimiter produces an array of UTF-8 characters for `STRING` values, and an array of `BYTES` for `BYTES` values.\n\nSplitting an empty `STRING` returns an `ARRAY` with a single empty `STRING` .\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Return type** \n\n `ARRAY&lt;STRING&gt;` or `ARRAY&lt;BYTES&gt;` \n\n **Examples** \n\n\n"
  },
  {
    "name": "SQRT",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the square root of X. Generates an error if X is less than 0.\n\n| X | SQRT(X) |\n| --- | --- |\n|  `25.0` |  `5.0` |\n|  `+inf` |  `+inf` |\n|  `X &lt; 0` | Error |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n\n"
  },
  {
    "name": "STARTS_WITH",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes two `STRING` or `BYTES` values. Returns `TRUE` if `prefix` is a\nprefix of `value` .\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Return type** \n\n `BOOL` \n\n **Examples** \n\n\n"
  },
  {
    "name": "STDDEV",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nAn alias of [STDDEV_SAMP](#stddev_samp) .\n\n\n\n"
  },
  {
    "name": "STDDEV_POP",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nReturns the population (biased) standard deviation of the values. The return\nresult is between `0` and `+Inf` .\n\nAll numeric types are supported. If the\ninput is `NUMERIC` or `BIGNUMERIC` then the internal aggregation is\nstable with the final output converted to a `FLOAT64` .\nOtherwise the input is converted to a `FLOAT64` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any `NULL` inputs. If all inputs are ignored, this\nfunction returns `NULL` . If this function receives a single non- `NULL` input,\nit returns `0` .\n\n `NaN` is produced if:\n\n- Any input value is `    NaN` \n- Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nIf this function is used with the `OVER` clause, it's part of a\nwindow function call. In a window function call,\naggregate function clauses can't be used.\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "STDDEV_SAMP",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nReturns the sample (unbiased) standard deviation of the values. The return\nresult is between `0` and `+Inf` .\n\nAll numeric types are supported. If the\ninput is `NUMERIC` or `BIGNUMERIC` then the internal aggregation is\nstable with the final output converted to a `FLOAT64` .\nOtherwise the input is converted to a `FLOAT64` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any `NULL` inputs. If there are fewer than two non- `NULL` inputs, this function returns `NULL` .\n\n `NaN` is produced if:\n\n- Any input value is `    NaN` \n- Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nIf this function is used with the `OVER` clause, it's part of a\nwindow function call. In a window function call,\naggregate function clauses can't be used.\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "STRING",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nConverts a JSON string to a SQL `STRING` value.\n\nArguments:\n\n-  `    json_expr` : JSON. For example:\n    \n    If the JSON value is not a string, an error is produced. If the expression\nis SQL `    NULL` , the function returns SQL `    NULL` .\n    \n    \n\n **Return type** \n\n `STRING` \n\n **Examples** \n\nThe following examples show how invalid requests are handled:\n\n\n"
  },
  {
    "name": "STRING_AGG",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns a value (either `STRING` or `BYTES` ) obtained by concatenating\nnon- `NULL` values. Returns `NULL` if there are zero input rows or `expression` evaluates to `NULL` for all rows.\n\nIf a `delimiter` is specified, concatenated values are separated by that\ndelimiter; otherwise, a comma is used as a delimiter.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nIf this function is used with the `OVER` clause, it's part of a\nwindow function call. In a window function call,\naggregate function clauses can't be used.\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Supported Argument Types** \n\nEither `STRING` or `BYTES` .\n\n **Return Data Types** \n\nEither `STRING` or `BYTES` .\n\n **Examples** \n\n\n"
  },
  {
    "name": "STRPOS",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes two `STRING` or `BYTES` values. Returns the 1-based position of the first\noccurrence of `subvalue` inside `value` . Returns `0` if `subvalue` is not found.\n\nThis function supports specifying [collation](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ST_ANGLE",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nTakes three point `GEOGRAPHY` values, which represent two intersecting lines.\nReturns the angle between these lines. Point 2 and point 1 represent the first\nline and point 2 and point 3 represent the second line. The angle between\nthese lines is in radians, in the range `[0, 2pi)` . The angle is measured\nclockwise from the first line to the second line.\n\n `ST_ANGLE` has the following edge cases:\n\n- If points 2 and 3 are the same, returns `    NULL` .\n- If points 2 and 1 are the same, returns `    NULL` .\n- If points 2 and 3 are exactly antipodal, returns `    NULL` .\n- If points 2 and 1 are exactly antipodal, returns `    NULL` .\n- If any of the input geographies are not single points or are the empty\ngeography, then throws an error.\n\n **Return type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "ST_AREA",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the area in square meters covered by the polygons in the input `GEOGRAPHY` .\n\nIf `geography_expression` is a point or a line, returns zero. If `geography_expression` is a collection, returns the area of the polygons in the\ncollection; if the collection does not contain polygons, returns zero.\n\nThe optional `use_spheroid` parameter determines how this function measures\ndistance. If `use_spheroid` is `FALSE` , the function measures distance on the\nsurface of a perfect sphere.\n\nThe `use_spheroid` parameter currently only supports\nthe value `FALSE` . The default value of `use_spheroid` is `FALSE` .\n\n **Return type** \n\n `FLOAT64` \n\n\n\n"
  },
  {
    "name": "ST_ASBINARY",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the [WKB](https://en.wikipedia.org/wiki/Well-known_text#Well-known_binary) representation of an input `GEOGRAPHY` .\n\nSee [ST_GEOGFROMWKB](#st_geogfromwkb) to construct a `GEOGRAPHY` from WKB.\n\n **Return type** \n\n `BYTES` \n\n\n\n"
  },
  {
    "name": "ST_ASGEOJSON",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the [RFC 7946](https://tools.ietf.org/html/rfc7946) compliant [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON) representation of the input `GEOGRAPHY` .\n\nA GoogleSQL `GEOGRAPHY` has spherical\ngeodesic edges, whereas a GeoJSON `Geometry` object explicitly has planar edges.\nTo convert between these two types of edges, GoogleSQL adds additional\npoints to the line where necessary so that the resulting sequence of edges\nremains within 10 meters of the original edge.\n\nSee [ST_GEOGFROMGEOJSON](#st_geogfromgeojson) to construct a `GEOGRAPHY` from GeoJSON.\n\n **Return type** \n\n `STRING` \n\n\n\n"
  },
  {
    "name": "ST_ASTEXT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the [WKT](https://en.wikipedia.org/wiki/Well-known_text) representation of an input `GEOGRAPHY` .\n\nSee [ST_GEOGFROMTEXT](#st_geogfromtext) to construct a `GEOGRAPHY` from WKT.\n\n **Return type** \n\n `STRING` \n\n\n\n"
  },
  {
    "name": "ST_AZIMUTH",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nTakes two point `GEOGRAPHY` values, and returns the azimuth of the line segment\nformed by points 1 and 2. The azimuth is the angle in radians measured between\nthe line from point 1 facing true North to the line segment from point 1 to\npoint 2.\n\nThe positive angle is measured clockwise on the surface of a sphere. For\nexample, the azimuth for a line segment:\n\n- Pointing North is `    0` \n- Pointing East is `    PI/2` \n- Pointing South is `    PI` \n- Pointing West is `    3PI/2` \n\n `ST_AZIMUTH` has the following edge cases:\n\n- If the two input points are the same, returns `    NULL` .\n- If the two input points are exactly antipodal, returns `    NULL` .\n- If either of the input geographies are not single points or are the empty\ngeography, throws an error.\n\n **Return type** \n\n `FLOAT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "ST_BOUNDARY",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a single `GEOGRAPHY` that contains the union\nof the boundaries of each component in the given input `GEOGRAPHY` .\n\nThe boundary of each component of a `GEOGRAPHY` is\ndefined as follows:\n\n- The boundary of a point is empty.\n- The boundary of a linestring consists of the endpoints of the linestring.\n- The boundary of a polygon consists of the linestrings that form the polygon\nshell and each of the polygon's holes.\n\n **Return type** \n\n `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_BOUNDINGBOX",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `STRUCT` that represents the bounding box for the specified geography.\nThe bounding box is the minimal rectangle that encloses the geography. The edges\nof the rectangle follow constant lines of longitude and latitude.\n\nCaveats:\n\n- Returns `    NULL` if the input is `    NULL` or an empty geography.\n- The bounding box might cross the antimeridian if this allows for a smaller\nrectangle. In this case, the bounding box has one of its longitudinal bounds\noutside of the [-180, 180] range, so that `    xmin` is smaller than the eastmost\nvalue `    xmax` .\n\n **Return type** \n\n `STRUCT&lt;xmin FLOAT64, ymin FLOAT64, xmax FLOAT64, ymax FLOAT64&gt;` .\n\nBounding box parts:\n\n-  `    xmin` : The westmost constant longitude line that bounds the rectangle.\n-  `    xmax` : The eastmost constant longitude line that bounds the rectangle.\n-  `    ymin` : The minimum constant latitude line that bounds the rectangle.\n-  `    ymax` : The maximum constant latitude line that bounds the rectangle.\n\n **Example** \n\nSee [ST_EXTENT](#st_extent) for the aggregate version of `ST_BOUNDINGBOX` .\n\n\n\n"
  },
  {
    "name": "ST_BUFFER",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `GEOGRAPHY` that represents the buffer around the input `GEOGRAPHY` .\nThis function is similar to [ST_BUFFERWITHTOLERANCE](#st_bufferwithtolerance) ,\nbut you specify the number of segments instead of providing tolerance to\ndetermine how much the resulting geography can deviate from the ideal\nbuffer radius.\n\n-  `    geography` : The input `    GEOGRAPHY` to encircle with the buffer radius.\n-  `    buffer_radius` : `    FLOAT64` that represents the radius of the\nbuffer around the input geography. The radius is in meters. Note that\npolygons contract when buffered with a negative `    buffer_radius` . Polygon\nshells and holes that are contracted to a point are discarded.\n-  `    num_seg_quarter_circle` : (Optional) `    FLOAT64` specifies the\nnumber of segments that are used to approximate a quarter circle. The\ndefault value is `    8.0` . Naming this argument is optional.\n-  `    endcap` : (Optional) `    STRING` allows you to specify one of two endcap\nstyles: `    ROUND` and `    FLAT` . The default value is `    ROUND` . This option only\naffects the endcaps of buffered linestrings.\n-  `    side` : (Optional) `    STRING` allows you to specify one of three possibilities\nfor lines: `    BOTH` , `    LEFT` , and `    RIGHT` . The default is `    BOTH` . This option\nonly affects how linestrings are buffered.\n-  `    use_spheroid` : (Optional) `    BOOL` determines how this function measures\ndistance. If `    use_spheroid` is `    FALSE` , the function measures distance on\nthe surface of a perfect sphere. The `    use_spheroid` parameter\ncurrently only supports the value `    FALSE` . The default value of `    use_spheroid` is `    FALSE` .\n\n **Return type** \n\nPolygon `GEOGRAPHY` \n\n **Example** \n\nThe following example shows the result of `ST_BUFFER` on a point. A buffered\npoint is an approximated circle. When `num_seg_quarter_circle = 2` , there are\ntwo line segments in a quarter circle, and therefore the buffered circle has\neight sides and [ST_NUMPOINTS](#st_numpoints) returns nine vertices. When `num_seg_quarter_circle = 8` , there are eight line segments in a quarter circle,\nand therefore the buffered circle has thirty-two sides and [ST_NUMPOINTS](#st_numpoints) returns thirty-three vertices.\n\n\n"
  },
  {
    "name": "ST_BUFFERWITHTOLERANCE",
    "arguments": [],
    "category": "Geography",
    "description_markdown": "Returns a `GEOGRAPHY` that represents the buffer around the input `GEOGRAPHY` .\nThis function is similar to [ST_BUFFER](#st_buffer) ,\nbut you provide tolerance instead of segments to determine how much the\nresulting geography can deviate from the ideal buffer radius.\n\n-  `    geography` : The input `    GEOGRAPHY` to encircle with the buffer radius.\n-  `    buffer_radius` : `    FLOAT64` that represents the radius of the\nbuffer around the input geography. The radius is in meters. Note that\npolygons contract when buffered with a negative `    buffer_radius` . Polygon\nshells and holes that are contracted to a point are discarded.\n-  `    tolerance_meters` : `    FLOAT64` specifies a tolerance in\nmeters with which the shape is approximated. Tolerance determines how much a\npolygon can deviate from the ideal radius. Naming this argument is optional.\n-  `    endcap` : (Optional) `    STRING` allows you to specify one of two endcap\nstyles: `    ROUND` and `    FLAT` . The default value is `    ROUND` . This option only\naffects the endcaps of buffered linestrings.\n-  `    side` : (Optional) `    STRING` allows you to specify one of three possible line\nstyles: `    BOTH` , `    LEFT` , and `    RIGHT` . The default is `    BOTH` . This option only\naffects the endcaps of buffered linestrings.\n-  `    use_spheroid` : (Optional) `    BOOL` determines how this function measures\ndistance. If `    use_spheroid` is `    FALSE` , the function measures distance on\nthe surface of a perfect sphere. The `    use_spheroid` parameter\ncurrently only supports the value `    FALSE` . The default value of `    use_spheroid` is `    FALSE` .\n\n **Return type** \n\nPolygon `GEOGRAPHY` \n\n **Example** \n\nThe following example shows the results of `ST_BUFFERWITHTOLERANCE` on a point,\ngiven two different values for tolerance but with the same buffer radius of `100` . A buffered point is an approximated circle. When `tolerance_meters=25` ,\nthe tolerance is a large percentage of the buffer radius, and therefore only\nfive segments are used to approximate a circle around the input point. When `tolerance_meters=1` , the tolerance is a much smaller percentage of the buffer\nradius, and therefore twenty-four edges are used to approximate a circle around\nthe input point.\n\n\n"
  },
  {
    "name": "ST_CENTROID",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the *centroid* of the input `GEOGRAPHY` as a single point `GEOGRAPHY` .\n\nThe *centroid* of a `GEOGRAPHY` is the weighted average of the centroids of the\nhighest-dimensional components in the `GEOGRAPHY` . The centroid for components\nin each dimension is defined as follows:\n\n- The centroid of points is the arithmetic mean of the input coordinates.\n- The centroid of linestrings is the centroid of all the edges weighted by\nlength. The centroid of each edge is the geodesic midpoint of the edge.\n- The centroid of a polygon is its center of mass.\n\nIf the input `GEOGRAPHY` is empty, an empty `GEOGRAPHY` is returned.\n\n **Constraints** \n\nIn the unlikely event that the centroid of a `GEOGRAPHY` cannot be defined by a\nsingle point on the surface of the Earth, a deterministic but otherwise\narbitrary point is returned. This can only happen if the centroid is exactly at\nthe center of the Earth, such as the centroid for a pair of antipodal points,\nand the likelihood of this happening is vanishingly small.\n\n **Return type** \n\nPoint `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_CENTROID_AGG",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nComputes the centroid of the set of input `GEOGRAPHY` s as a single point `GEOGRAPHY` .\n\nThe *centroid* over the set of input `GEOGRAPHY` s is the weighted average of the\ncentroid of each individual `GEOGRAPHY` . Only the `GEOGRAPHY` s with the highest\ndimension present in the input contribute to the centroid of the entire set. For\nexample, if the input contains both `GEOGRAPHY` s with lines and `GEOGRAPHY` s\nwith only points, `ST_CENTROID_AGG` returns the weighted average of the `GEOGRAPHY` s with lines, since a line has more dimensions than a point. In this\nexample, `ST_CENTROID_AGG` ignores `GEOGRAPHY` s with only points when\ncalculating the aggregate centroid.\n\n `ST_CENTROID_AGG` ignores `NULL` input `GEOGRAPHY` values.\n\nSee [ST_CENTROID](#st_centroid) for the non-aggregate version of `ST_CENTROID_AGG` and the definition of centroid for an individual `GEOGRAPHY` value.\n\n **Return type** \n\nPoint `GEOGRAPHY` \n\n **Example** \n\nThe following queries compute the aggregate centroid over a set of `GEOGRAPHY` values. The input to the first query\ncontains only points, and therefore each value contribute to the aggregate\ncentroid. Also notice that `ST_CENTROID_AGG` is *not* equivalent to calling `ST_CENTROID` on the result of `ST_UNION_AGG` ; duplicates are removed by the\nunion, unlike `ST_CENTROID_AGG` . The input to the second query has mixed\ndimensions, and only values with the highest dimension in the set, the lines,\naffect the aggregate centroid.\n\n\n"
  },
  {
    "name": "ST_CLOSESTPOINT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `GEOGRAPHY` containing a point on `geography_1` with the smallest possible distance to `geography_2` . This implies\nthat the distance between the point returned by `ST_CLOSESTPOINT` and `geography_2` is less than or equal to the distance between any other point on `geography_1` and `geography_2` .\n\nIf either of the input `GEOGRAPHY` s is empty, `ST_CLOSESTPOINT` returns `NULL` .\n\nThe optional `use_spheroid` parameter determines how this function measures\ndistance. If `use_spheroid` is `FALSE` , the function measures distance on the\nsurface of a perfect sphere.\n\nThe `use_spheroid` parameter currently only supports\nthe value `FALSE` . The default value of `use_spheroid` is `FALSE` .\n\n **Return type** \n\nPoint `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_CLUSTERDBSCAN",
    "arguments": [],
    "category": "Geography",
    "description_markdown": "Performs [DBSCAN clustering](https://en.wikipedia.org/wiki/DBSCAN) on a column of geographies. Returns a\n0-based cluster number.\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Input parameters** \n\n-  `    geography_column` : A column of `    GEOGRAPHY` s that\nis clustered.\n-  `    epsilon` : The epsilon that specifies the radius, measured in meters, around\na core value. Non-negative `    FLOAT64` value.\n-  `    minimum_geographies` : Specifies the minimum number of geographies in a\nsingle cluster. Only dense input forms a cluster, otherwise it is classified\nas noise. Non-negative `    INT64` value.\n\n **Geography types and the DBSCAN algorithm** \n\nThe DBSCAN algorithm identifies high-density clusters of data and marks outliers\nin low-density areas of noise. Geographies passed in through `geography_column` are classified in one of three ways by the DBSCAN algorithm:\n\n- Core value: A geography is a core value if it is within `    epsilon` distance\nof `    minimum_geographies` geographies, including itself. The core value\nstarts a new cluster, or is added to the same cluster as a core value within `    epsilon` distance. Core values are grouped in a cluster together with all\nother core and border values that are within `    epsilon` distance.\n- Border value: A geography is a border value if it is within epsilon distance\nof a core value. It is added to the same cluster as a core value within `    epsilon` distance. A border value may be within `    epsilon` distance of more\nthan one cluster. In this case, it may be arbitrarily assigned to either\ncluster and the function will produce the same result in subsequent calls.\n- Noise: A geography is noise if it is neither a core nor a border value.\nNoise values are assigned to a `    NULL` cluster. An empty `    GEOGRAPHY` is always classified as noise.\n\n **Constraints** \n\n- The argument `    minimum_geographies` is a non-negative `    INT64` and `    epsilon` is a non-negative `    FLOAT64` .\n- An empty geography cannot join any cluster.\n- Multiple clustering assignments could be possible for a border value. If a\ngeography is a border value, `    ST_CLUSTERDBSCAN` will assign it to an\narbitrary valid cluster.\n\n **Return type** \n\n `INT64` for each geography in the geography column.\n\n **Examples** \n\nThis example performs DBSCAN clustering with a radius of 100,000 meters with a `minimum_geographies` argument of 1. The geographies being analyzed are a\nmixture of points, lines, and polygons.\n\n\n"
  },
  {
    "name": "ST_CONTAINS",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if no point of `geography_2` is outside `geography_1` , and\nthe interiors intersect; returns `FALSE` otherwise.\n\nNOTE: A `GEOGRAPHY`  *does not* contain its own\nboundary. Compare with [ST_COVERS](#st_covers) .\n\n **Return type** \n\n `BOOL` \n\n **Example** \n\nThe following query tests whether the polygon `POLYGON((1 1, 20 1, 10 20, 1 1))` contains each of the three points `(0, 0)` , `(1, 1)` , and `(10, 10)` , which lie\non the exterior, the boundary, and the interior of the polygon respectively.\n\n\n"
  },
  {
    "name": "ST_CONVEXHULL",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the convex hull for the input `GEOGRAPHY` . The convex hull is the\nsmallest convex `GEOGRAPHY` that covers the input. A `GEOGRAPHY` is convex if\nfor every pair of points in the `GEOGRAPHY` , the geodesic edge connecting the\npoints are also contained in the same `GEOGRAPHY` .\n\nIn most cases, the convex hull consists of a single polygon. Notable edge cases\ninclude the following:\n\n- The convex hull of a single point is also a point.\n- The convex hull of two or more collinear points is a linestring as long as\nthat linestring is convex.\n- If the input `    GEOGRAPHY` spans more than a\nhemisphere, the convex hull is the full globe. This includes any input that\ncontains a pair of antipodal points.\n-  `    ST_CONVEXHULL` returns `    NULL` if the input is either `    NULL` or the empty `    GEOGRAPHY` .\n\n **Return type** \n\n `GEOGRAPHY` \n\n **Examples** \n\nThe convex hull returned by `ST_CONVEXHULL` can be a point, linestring, or a\npolygon, depending on the input.\n\n\n"
  },
  {
    "name": "ST_COVEREDBY",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `FALSE` if `geography_1` or `geography_2` is empty. Returns `TRUE` if no\npoints of `geography_1` lie in the exterior of `geography_2` .\n\nGiven two `GEOGRAPHY` s `a` and `b` , `ST_COVEREDBY(a, b)` returns the same result as [ST_COVERS](#st_covers)  `(b, a)` . Note the opposite order of arguments.\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_COVERS",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `FALSE` if `geography_1` or `geography_2` is empty.\nReturns `TRUE` if no points of `geography_2` lie in the exterior of `geography_1` .\n\n **Return type** \n\n `BOOL` \n\n **Example** \n\nThe following query tests whether the polygon `POLYGON((1 1, 20 1, 10 20, 1 1))` covers each of the three points `(0, 0)` , `(1, 1)` , and `(10, 10)` , which lie\non the exterior, the boundary, and the interior of the polygon respectively.\n\n\n"
  },
  {
    "name": "ST_DIFFERENCE",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `GEOGRAPHY` that represents the point set\ndifference of `geography_1` and `geography_2` . Therefore, the result consists of\nthe part of `geography_1` that does not intersect with `geography_2` .\n\nIf `geometry_1` is completely contained in `geometry_2` , then `ST_DIFFERENCE` returns an empty `GEOGRAPHY` .\n\n **Constraints** \n\nThe underlying geometric objects that a GoogleSQL `GEOGRAPHY` represents correspond to a *closed* point\nset. Therefore, `ST_DIFFERENCE` is the closure of the point set difference of `geography_1` and `geography_2` . This implies that if `geography_1` and `geography_2` intersect, then a portion of the boundary of `geography_2` could\nbe in the difference.\n\n **Return type** \n\n `GEOGRAPHY` \n\n **Example** \n\nThe following query illustrates the difference between `geog1` , a larger polygon `POLYGON((0 0, 10 0, 10 10, 0 0))` and `geog1` , a smaller polygon `POLYGON((4 2, 6 2, 8 6, 4 2))` that intersects with `geog1` . The result is `geog1` with a hole where `geog2` intersects with it.\n\n\n"
  },
  {
    "name": "ST_DIMENSION",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the dimension of the highest-dimensional element in the input `GEOGRAPHY` .\n\nThe dimension of each possible element is as follows:\n\n- The dimension of a point is `    0` .\n- The dimension of a linestring is `    1` .\n- The dimension of a polygon is `    2` .\n\nIf the input `GEOGRAPHY` is empty, `ST_DIMENSION` returns `-1` .\n\n **Return type** \n\n `INT64` \n\n\n\n"
  },
  {
    "name": "ST_DISJOINT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if the intersection of `geography_1` and `geography_2` is empty,\nthat is, no point in `geography_1` also appears in `geography_2` .\n\n `ST_DISJOINT` is the logical negation of [ST_INTERSECTS](#st_intersects) .\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_DISTANCE",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the shortest distance in meters between two non-empty `GEOGRAPHY` s.\n\nIf either of the input `GEOGRAPHY` s is empty, `ST_DISTANCE` returns `NULL` .\n\nThe optional `use_spheroid` parameter determines how this function measures\ndistance. If `use_spheroid` is `FALSE` , the function measures distance on the\nsurface of a perfect sphere. If `use_spheroid` is `TRUE` , the function measures\ndistance on the surface of the [WGS84](https://en.wikipedia.org/wiki/World_Geodetic_System) spheroid. The default value\nof `use_spheroid` is `FALSE` .\n\n **Return type** \n\n `FLOAT64` \n\n\n\n"
  },
  {
    "name": "ST_DUMP",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns an `ARRAY` of simple `GEOGRAPHY` s where each element is a component of\nthe input `GEOGRAPHY` . A simple `GEOGRAPHY` consists of a single point, linestring,\nor polygon. If the input `GEOGRAPHY` is simple, the\nresult is a single element. When the input `GEOGRAPHY` is a collection, `ST_DUMP` returns an `ARRAY` with one simple `GEOGRAPHY` for each component in the collection.\n\nIf `dimension` is provided, the function only returns `GEOGRAPHY` s of the corresponding dimension. A\ndimension of -1 is equivalent to omitting `dimension` .\n\n **Return Type** \n\n `ARRAY&lt;GEOGRAPHY&gt;` \n\n **Examples** \n\nThe following example shows how `ST_DUMP` returns the simple geographies within\na complex geography.\n\nThe following example shows how `ST_DUMP` with the dimension argument only\nreturns simple geographies of the given dimension.\n\n\n"
  },
  {
    "name": "ST_DWITHIN",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if the distance between at least one point in `geography_1` and\none point in `geography_2` is less than or equal to the distance given by the `distance` argument; otherwise, returns `FALSE` . If either input `GEOGRAPHY` is empty, `ST_DWithin` returns `FALSE` . The\ngiven `distance` is in meters on the surface of the Earth.\n\nThe optional `use_spheroid` parameter determines how this function measures\ndistance. If `use_spheroid` is `FALSE` , the function measures distance on the\nsurface of a perfect sphere.\n\nThe `use_spheroid` parameter currently only supports\nthe value `FALSE` . The default value of `use_spheroid` is `FALSE` .\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_ENDPOINT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the last point of a linestring geography as a point geography. Returns\nan error if the input is not a linestring or if the input is empty. Use the `SAFE` prefix to obtain `NULL` for invalid input instead of an error.\n\n **Return Type** \n\nPoint `GEOGRAPHY` \n\n **Example** \n\n\n"
  },
  {
    "name": "ST_EQUALS",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if `geography_1` and `geography_2` represent the same\n\n `GEOGRAPHY` value. More precisely, this means that\none of the following conditions holds:\n+ `ST_COVERS(geography_1, geography_2) = TRUE` and `ST_COVERS(geography_2,\n    geography_1) = TRUE` +   Both `geography_1` and `geography_2` are empty.\n\nTherefore, two `GEOGRAPHY` s may be equal even if the\nordering of points or vertices differ, as long as they still represent the same\ngeometric structure.\n\n **Constraints** \n\n `ST_EQUALS` is not guaranteed to be a transitive function.\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_EXTENT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `STRUCT` that represents the bounding box for the set of input `GEOGRAPHY` values. The bounding box is the minimal rectangle that encloses the\ngeography. The edges of the rectangle follow constant lines of longitude and\nlatitude.\n\nCaveats:\n\n- Returns `    NULL` if all the inputs are `    NULL` or empty geographies.\n- The bounding box might cross the antimeridian if this allows for a smaller\nrectangle. In this case, the bounding box has one of its longitudinal bounds\noutside of the [-180, 180] range, so that `    xmin` is smaller than the eastmost\nvalue `    xmax` .\n- If the longitude span of the bounding box is larger than or equal to 180\ndegrees, the function returns the bounding box with the longitude range of\n[-180, 180].\n\n **Return type** \n\n `STRUCT&lt;xmin FLOAT64, ymin FLOAT64, xmax FLOAT64, ymax FLOAT64&gt;` .\n\nBounding box parts:\n\n-  `    xmin` : The westmost constant longitude line that bounds the rectangle.\n-  `    xmax` : The eastmost constant longitude line that bounds the rectangle.\n-  `    ymin` : The minimum constant latitude line that bounds the rectangle.\n-  `    ymax` : The maximum constant latitude line that bounds the rectangle.\n\n **Example** \n\n [ST_BOUNDINGBOX](#st_boundingbox) for the non-aggregate version of `ST_EXTENT` .\n\n\n\n"
  },
  {
    "name": "ST_EXTERIORRING",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a linestring geography that corresponds to the outermost ring of a\npolygon geography.\n\n- If the input geography is a polygon, gets the outermost ring of the polygon\ngeography and returns the corresponding linestring.\n- If the input is the full `    GEOGRAPHY` , returns an empty geography.\n- Returns an error if the input is not a single polygon.\n\nUse the `SAFE` prefix to return `NULL` for invalid input instead of an error.\n\n **Return type** \n\n- Linestring `    GEOGRAPHY` \n- Empty `    GEOGRAPHY` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ST_GEOGFROM",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nConverts an expression for a `STRING` or `BYTES` value into a `GEOGRAPHY` value.\n\nIf `expression` represents a `STRING` value, it must be a valid `GEOGRAPHY` representation in one of the following formats:\n\n- WKT format. To learn more about this format and the requirements to use it,\nsee [ST_GEOGFROMTEXT](#st_geogfromtext) .\n- WKB in hexadecimal text format. To learn more about this format and the\nrequirements to use it, see [ST_GEOGFROMWKB](#st_geogfromwkb) .\n- GeoJSON format. To learn more about this format and the\nrequirements to use it, see [ST_GEOGFROMGEOJSON](#st_geogfromgeojson) .\n\nIf `expression` represents a `BYTES` value, it must be a valid `GEOGRAPHY` binary expression in WKB format. To learn more about this format and the\nrequirements to use it, see [ST_GEOGFROMWKB](#st_geogfromwkb) .\n\nIf `expression` is `NULL` , the output is `NULL` .\n\n **Return type** \n\n `GEOGRAPHY` \n\n **Examples** \n\nThis takes a WKT-formatted string and returns a `GEOGRAPHY` polygon:\n\nThis takes a WKB-formatted hexadecimal-encoded string and returns a `GEOGRAPHY` point:\n\nThis takes WKB-formatted bytes and returns a `GEOGRAPHY` point:\n\nThis takes a GeoJSON-formatted string and returns a `GEOGRAPHY` polygon:\n\n\n"
  },
  {
    "name": "ST_GEOGFROMGEOJSON",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `GEOGRAPHY` value that corresponds to the\ninput [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON) representation.\n\n `ST_GEOGFROMGEOJSON` accepts input that is [RFC 7946](https://tools.ietf.org/html/rfc7946) compliant.\n\nIf the named argument `make_valid` is set to `TRUE` , the function attempts to\nrepair polygons that don't conform to [Open Geospatial Consortium](https://www.ogc.org/standards/sfa) semantics.\n\nA GoogleSQL `GEOGRAPHY` has spherical\ngeodesic edges, whereas a GeoJSON `Geometry` object explicitly has planar edges.\nTo convert between these two types of edges, GoogleSQL adds additional\npoints to the line where necessary so that the resulting sequence of edges\nremains within 10 meters of the original edge.\n\nSee [ST_ASGEOJSON](#st_asgeojson) to format a `GEOGRAPHY` as GeoJSON.\n\n **Constraints** \n\nThe JSON input is subject to the following constraints:\n\n-  `    ST_GEOGFROMGEOJSON` only accepts JSON geometry fragments and cannot be used\nto ingest a whole JSON document.\n- The input JSON fragment must consist of a GeoJSON geometry type, which\nincludes `    Point` , `    MultiPoint` , `    LineString` , `    MultiLineString` , `    Polygon` , `    MultiPolygon` , and `    GeometryCollection` . Any other GeoJSON type such as `    Feature` or `    FeatureCollection` will result in an error.\n- A position in the `    coordinates` member of a GeoJSON geometry type must\nconsist of exactly two elements. The first is the longitude and the second\nis the latitude. Therefore, `    ST_GEOGFROMGEOJSON` does not support the\noptional third element for a position in the `    coordinates` member.\n\n **Return type** \n\n `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_GEOGFROMTEXT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": "<span id=\"st_geogfromtext_signature1\"></span><span id=\"st_geogfromtext_signature2\"></span>\n\n **Description** \n\nConverts a `STRING`  [WKT](https://en.wikipedia.org/wiki/Well-known_text) geometry value into a `GEOGRAPHY` value.\n\nTo format `GEOGRAPHY` value as WKT, use [ST_ASTEXT](#st_astext) .\n\n **Definitions** \n\n-  `    wkt_string` : A `    STRING` value that contains the [WKT](https://en.wikipedia.org/wiki/Well-known_text) format.\n-  `    oriented` : A named argument with a `    BOOL` literal.\n    \n    \n    - If the value is `        TRUE` , any polygons in the input are assumed to be\noriented as follows: when traveling along the boundary of the polygon\nin the order of the input vertices, the interior of the polygon is on\nthe left. This allows WKT to represent polygons larger than a\nhemisphere. See also [ST_MAKEPOLYGONORIENTED](#st_makepolygonoriented) ,\nwhich is similar to `        ST_GEOGFROMTEXT` with `        oriented=TRUE` .\n        \n        \n    - If the value is `        FALSE` or omitted, this function returns the polygon\nwith the smaller area.\n        \n        \n-  `    planar` : A named argument with a `    BOOL` literal. If the value\nis `    TRUE` , the edges of the linestrings and polygons are assumed to use\nplanar map semantics, rather than GoogleSQL default spherical\ngeodesics semantics. For more information about the\ndifferences between spherical geodesics and planar lines, see [Coordinate systems and edges](/bigquery/docs/gis-data#coordinate_systems_and_edges) .\n    \n    \n-  `    make_valid` : A named argument with a `    BOOL` literal. If the\nvalue is `    TRUE` , the function attempts to repair polygons that don't\nconform to [Open Geospatial Consortium](https://www.ogc.org/standards/sfa) semantics.\n    \n    \n\n **Details** \n\n- The function does not support three-dimensional geometries that have a `    Z` suffix, nor does it support linear referencing system geometries with an `    M` suffix.\n-  `    oriented` and `    planar` can't be `    TRUE` at the same time.\n-  `    oriented` and `    make_valid` can't be `    TRUE` at the same time.\n\n **Example** \n\nThe following query reads the WKT string `POLYGON((0 0, 0 2, 2 2, 0 2, 0 0))` both as a non-oriented polygon and as an oriented polygon, and checks whether\neach result contains the point `(1, 1)` .\n\nThe following query converts a WKT string with an invalid polygon to `GEOGRAPHY` . The WKT string violates two properties\nof a valid polygon - the loop describing the polygon is not closed, and it\ncontains self-intersection. With the `make_valid` option, `ST_GEOGFROMTEXT` successfully converts it to a multipolygon shape.\n\n\n"
  },
  {
    "name": "ST_GEOGFROMWKB",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nConverts an expression from a hexadecimal-text `STRING` or `BYTES` value into a `GEOGRAPHY` value. The expression must be in [WKB](https://en.wikipedia.org/wiki/Well-known_text#Well-known_binary) format.\n\nTo format `GEOGRAPHY` as WKB, use [ST_ASBINARY](#st_asbinary) .\n\n **Definitions** \n\n-  `    wkb_bytes_expression` : A `    BYTES` value that contains the [WKB](https://en.wikipedia.org/wiki/Well-known_text#Well-known_binary) format.\n-  `    wkb_hex_string_expression` : A `    STRING` value that contains the\nhexadecimal-encoded [WKB](https://en.wikipedia.org/wiki/Well-known_text#Well-known_binary) format.\n-  `    oriented` : A named argument with a `    BOOL` literal.\n    \n    \n    - If the value is `        TRUE` , any polygons in the input are assumed to be\noriented as follows: when traveling along the boundary of the polygon\nin the order of the input vertices, the interior of the polygon is on\nthe left. This allows WKB to represent polygons larger than a\nhemisphere. See also [ST_MAKEPOLYGONORIENTED](#st_makepolygonoriented) ,\nwhich is similar to `        ST_GEOGFROMWKB` with `        oriented=TRUE` .\n        \n        \n    - If the value is `        FALSE` or omitted, this function returns the polygon\nwith the smaller area.\n        \n        \n-  `    planar` : A named argument with a `    BOOL` literal. If the value\nis `    TRUE` , the edges of the linestrings and polygons are assumed to use\nplanar map semantics, rather than GoogleSQL default spherical\ngeodesics semantics. For more information about the\ndifferences between spherical geodesics and planar lines, see [Coordinate systems and edges](/bigquery/docs/gis-data#coordinate_systems_and_edges) .\n    \n    \n-  `    make_valid` : A named argument with a `    BOOL` literal. If the\nvalue is `    TRUE` , the function attempts to repair polygons that\ndon't conform to [Open Geospatial Consortium](https://www.ogc.org/standards/sfa) semantics.\n    \n    \n\n **Details** \n\n- The function does not support three-dimensional geometries that have a `    Z` suffix, nor does it support linear referencing system geometries with an `    M` suffix.\n-  `    oriented` and `    planar` can't be `    TRUE` at the same time.\n-  `    oriented` and `    make_valid` can't be `    TRUE` at the same time.\n\n **Return type** \n\n `GEOGRAPHY` \n\n **Example** \n\nThe following query reads the hex-encoded WKB data containing `LINESTRING(1 1, 3 2)` and uses it with planar and geodesic semantics. When\nplanar is used, the function approximates the planar input line using\nline that contains a chain of geodesic segments.\n\n\n"
  },
  {
    "name": "ST_GEOGPOINT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nCreates a `GEOGRAPHY` with a single point. `ST_GEOGPOINT` creates a point from\nthe specified `FLOAT64` longitude (in degrees,\nnegative west of the Prime Meridian, positive east) and latitude (in degrees,\npositive north of the Equator, negative south) parameters and returns that point\nin a `GEOGRAPHY` value.\n\nNOTE: Some systems present latitude first; take care with argument order.\n\n **Constraints** \n\n- Longitudes outside the range [-180, 180] are allowed; `    ST_GEOGPOINT` uses\nthe input longitude modulo 360 to obtain a longitude within [-180, 180].\n- Latitudes must be in the range [-90, 90]. Latitudes outside this range\nwill result in an error.\n\n **Return type** \n\nPoint `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_GEOGPOINTFROMGEOHASH",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `GEOGRAPHY` value that corresponds to a\npoint in the middle of a bounding box defined in the [GeoHash](https://en.wikipedia.org/wiki/Geohash) .\n\n **Return type** \n\nPoint `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_GEOHASH",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nTakes a single-point `GEOGRAPHY` and returns a [GeoHash](https://en.wikipedia.org/wiki/Geohash) representation of that `GEOGRAPHY` object.\n\n-  `    geography_expression` : Represents a `    GEOGRAPHY` object. Only a `    GEOGRAPHY` object that represents a single point is supported. If `    ST_GEOHASH` is used\nover an empty `    GEOGRAPHY` object, returns `    NULL` .\n-  `    maxchars` : This optional `    INT64` parameter specifies the maximum number of\ncharacters the hash will contain. Fewer characters corresponds to lower\nprecision (or, described differently, to a bigger bounding box). `    maxchars` defaults to 20 if not explicitly specified. A valid `    maxchars` value is 1\nto 20. Any value below or above is considered unspecified and the default of\n20 is used.\n\n **Return type** \n\n `STRING` \n\n **Example** \n\nReturns a GeoHash of the Seattle Center with 10 characters of precision.\n\n\n"
  },
  {
    "name": "ST_GEOMETRYTYPE",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the [Open Geospatial Consortium](https://www.ogc.org/standards/sfa) (OGC) geometry type that\ndescribes the input `GEOGRAPHY` . The OGC geometry type matches the\ntypes that are used in [WKT](https://en.wikipedia.org/wiki/Well-known_text) and [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON) formats and\nprinted for [ST_ASTEXT](#st_astext) and [ST_ASGEOJSON](#st_asgeojson) . `ST_GEOMETRYTYPE` returns the OGC geometry type with the \"ST_\" prefix.\n\n `ST_GEOMETRYTYPE` returns the following given the type on the input:\n\n- Single point geography: Returns `    ST_Point` .\n- Collection of only points: Returns `    ST_MultiPoint` .\n- Single linestring geography: Returns `    ST_LineString` .\n- Collection of only linestrings: Returns `    ST_MultiLineString` .\n- Single polygon geography: Returns `    ST_Polygon` .\n- Collection of only polygons: Returns `    ST_MultiPolygon` .\n- Collection with elements of different dimensions, or the input is the empty\ngeography: Returns `    ST_GeometryCollection` .\n\n **Return type** \n\n `STRING` \n\n **Example** \n\nThe following example shows how `ST_GEOMETRYTYPE` takes geographies and returns\nthe names of their OGC geometry types.\n\n\n"
  },
  {
    "name": "ST_HAUSDORFFDISTANCE",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nGets the discrete [Hausdorff distance](http://en.wikipedia.org/wiki/Hausdorff_distance) , which is the greatest of all\nthe distances from a discrete point in one geography to the closest\ndiscrete point in another geography.\n\n **Definitions** \n\n-  `    geography_1` : A `    GEOGRAPHY` value that represents the first geography.\n-  `    geography_2` : A `    GEOGRAPHY` value that represents the second geography.\n-  `    directed` : A named argument with a `    BOOL` value. Represents the type of\ncomputation to use on the input geographies. If this argument isn't\nspecified, `    directed =&gt; FALSE` is used by default.\n    \n    \n    -  `        FALSE` : The largest Hausdorff distance found in\n( `        geography_1` , `        geography_2` ) and\n( `        geography_2` , `        geography_1` ).\n        \n        \n    -  `        TRUE` (default): The Hausdorff distance for\n( `        geography_1` , `        geography_2` ).\n        \n        \n\n **Details** \n\nIf an input geography is `NULL` , the function returns `NULL` .\n\n **Return type** \n\n `FLOAT64` \n\n **Example** \n\nThe following query gets the Hausdorff distance between `geo1` and `geo2` :\n\nThe following query gets the Hausdorff distance between `geo2` and `geo1` :\n\nThe following query gets the largest Hausdorff distance between\n( `geo1` and `geo2` ) and ( `geo2` and `geo1` ):\n\nThe following query produces the same results as the previous query because `ST_HAUSDORFFDISTANCE` uses `directed=&gt;FALSE` by default.\n\n\n"
  },
  {
    "name": "ST_INTERIORRINGS",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns an array of linestring geographies that corresponds to the interior\nrings of a polygon geography. Each interior ring is the border of a hole within\nthe input polygon.\n\n- If the input geography is a polygon, excludes the outermost ring of the\npolygon geography and returns the linestrings corresponding to the interior\nrings.\n- If the input is the full `    GEOGRAPHY` , returns an empty array.\n- If the input polygon has no holes, returns an empty array.\n- Returns an error if the input is not a single polygon.\n\nUse the `SAFE` prefix to return `NULL` for invalid input instead of an error.\n\n **Return type** \n\n `ARRAY&lt;LineString GEOGRAPHY&gt;` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ST_INTERSECTION",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `GEOGRAPHY` that represents the point set\nintersection of the two input `GEOGRAPHY` s. Thus,\nevery point in the intersection appears in both `geography_1` and `geography_2` .\n\nIf the two input `GEOGRAPHY` s are disjoint, that is,\nthere are no points that appear in both input `geometry_1` and `geometry_2` ,\nthen an empty `GEOGRAPHY` is returned.\n\nSee [ST_INTERSECTS](#st_intersects) , [ST_DISJOINT](#st_disjoint) for related\npredicate functions.\n\n **Return type** \n\n `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_INTERSECTS",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if the point set intersection of `geography_1` and `geography_2` is non-empty. Thus, this function returns `TRUE` if there is at least one point\nthat appears in both input `GEOGRAPHY` s.\n\nIf `ST_INTERSECTS` returns `TRUE` , it implies that [ST_DISJOINT](#st_disjoint) returns `FALSE` .\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_INTERSECTSBOX",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if `geography` intersects the rectangle between `[lng1, lng2]` and `[lat1, lat2]` . The edges of the rectangle follow constant lines of\nlongitude and latitude. `lng1` and `lng2` specify the westmost and eastmost\nconstant longitude lines that bound the rectangle, and `lat1` and `lat2` specify\nthe minimum and maximum constant latitude lines that bound the rectangle.\n\nSpecify all longitude and latitude arguments in degrees.\n\n **Constraints** \n\nThe input arguments are subject to the following constraints:\n\n- Latitudes should be in the `    [-90, 90]` degree range.\n- Longitudes should follow either of the following rules:\n    - Both longitudes are in the `        [-180, 180]` degree range.\n    - One of the longitudes is in the `        [-180, 180]` degree range, and `        lng2 - lng1` is in the `        [0, 360]` interval.\n\n **Return type** \n\n `BOOL` \n\n **Example** \n\n\n"
  },
  {
    "name": "ST_ISCLOSED",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` for a non-empty Geography, where each element in the Geography\nhas an empty boundary. The boundary for each element can be defined with [ST_BOUNDARY](#st_boundary) .\n\n- A point is closed.\n- A linestring is closed if the start and end points of the linestring are\nthe same.\n- A polygon is closed only if it is a full polygon.\n- A collection is closed if and only if every element in the collection is\nclosed.\n\nAn empty `GEOGRAPHY` is not closed.\n\n **Return type** \n\n `BOOL` \n\n **Example** \n\n\n"
  },
  {
    "name": "ST_ISCOLLECTION",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if the total number of points, linestrings, and polygons is\ngreater than one.\n\nAn empty `GEOGRAPHY` is not a collection.\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_ISEMPTY",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if the given `GEOGRAPHY` is empty; that is, the `GEOGRAPHY` does\nnot contain any points, lines, or polygons.\n\nNOTE: An empty `GEOGRAPHY` is not associated with a particular geometry shape.\nFor example, the results of expressions `ST_GEOGFROMTEXT('POINT EMPTY')` and `ST_GEOGFROMTEXT('GEOMETRYCOLLECTION EMPTY')` are identical.\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_ISRING",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if the input `GEOGRAPHY` is a linestring and if the\nlinestring is both [ST_ISCLOSED](#st_isclosed) and\nsimple. A linestring is considered simple if it does not pass through the\nsame point twice (with the exception of the start and endpoint, which may\noverlap to form a ring).\n\nAn empty `GEOGRAPHY` is not a ring.\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_LENGTH",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the total length in meters of the lines in the input `GEOGRAPHY` .\n\nIf `geography_expression` is a point or a polygon, returns zero. If `geography_expression` is a collection, returns the length of the lines in the\ncollection; if the collection does not contain lines, returns zero.\n\nThe optional `use_spheroid` parameter determines how this function measures\ndistance. If `use_spheroid` is `FALSE` , the function measures distance on the\nsurface of a perfect sphere.\n\nThe `use_spheroid` parameter currently only supports\nthe value `FALSE` . The default value of `use_spheroid` is `FALSE` .\n\n **Return type** \n\n `FLOAT64` \n\n\n\n"
  },
  {
    "name": "ST_LINEINTERPOLATEPOINT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nGets a point at a specific fraction in a linestring `GEOGRAPHY` value.\n\n **Definitions** \n\n-  `    linestring_geography` : A linestring `    GEOGRAPHY` on which the target point\nis located.\n-  `    fraction` : A `    FLOAT64` value that represents a fraction\nalong the linestring `    GEOGRAPHY` where the target point is located.\nThis should be an inclusive value between `    0` (start of the\nlinestring) and `    1` (end of the linestring).\n\n **Details** \n\n- Returns `    NULL` if any input argument is `    NULL` .\n- Returns an empty geography if `    linestring_geography` is an empty geography.\n- Returns an error if `    linestring_geography` is not a linestring or an empty\ngeography, or if `    fraction` is outside the `    [0, 1]` range.\n\n **Return Type** \n\n `GEOGRAPHY` \n\n **Example** \n\nThe following query returns a few points on a linestring. Notice that the\n midpoint of the linestring `LINESTRING(1 1, 5 5)` is slightly different from `POINT(3 3)` because the `GEOGRAPHY` type uses geodesic line segments.\n\n\n"
  },
  {
    "name": "ST_LINELOCATEPOINT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nGets a section of a linestring between the start point and a selected point (a\npoint on the linestring closest to the `point_geography` argument). Returns the\npercentage that this section represents in the linestring.\n\nDetails:\n\n- To select a point on the linestring `    GEOGRAPHY` ( `    linestring_geography` ),\nthis function takes a point `    GEOGRAPHY` ( `    point_geography` ) and finds the [closest point](#st_closestpoint) to it on the linestring.\n- If two points on `    linestring_geography` are an equal distance away from `    point_geography` , it is not guaranteed which one will be selected.\n- The return value is an inclusive value between 0 and 1 (0-100%).\n- If the selected point is the start point on the linestring, function returns\n0 (0%).\n- If the selected point is the end point on the linestring, function returns 1\n(100%).\n\n `NULL` and error handling:\n\n- Returns `    NULL` if any input argument is `    NULL` .\n- Returns an error if `    linestring_geography` is not a linestring or if `    point_geography` is not a point. Use the `    SAFE` prefix\nto obtain `    NULL` for invalid input instead of an error.\n\n **Return Type** \n\n `FLOAT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "ST_LINESUBSTRING",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nGets a segment of a linestring at a specific starting and ending fraction.\n\n **Definitions** \n\n-  `    linestring_geography` : The LineString `    GEOGRAPHY` value that represents the\nlinestring from which to extract a segment.\n-  `    start_fraction` : `    FLOAT64` value that represents\nthe starting fraction of the total length of `    linestring_geography` .\nThis must be an inclusive value between 0 and 1 (0-100%).\n-  `    end_fraction` : `    FLOAT64` value that represents\nthe ending fraction of the total length of `    linestring_geography` .\nThis must be an inclusive value between 0 and 1 (0-100%).\n\n **Details** \n\n `end_fraction` must be greater than or equal to `start_fraction` .\n\nIf `start_fraction` and `end_fraction` are equal, a linestring with only\none point is produced.\n\n **Return type** \n\n- LineString `    GEOGRAPHY` if the resulting geography has more than one point.\n- Point `    GEOGRAPHY` if the resulting geography has only one point.\n\n **Example** \n\nThe following query returns the second half of the linestring:\n\nThe following query returns a linestring that only contains one point:\n\n\n"
  },
  {
    "name": "ST_MAKELINE",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nCreates a `GEOGRAPHY` with a single linestring by\nconcatenating the point or line vertices of each of the input `GEOGRAPHY` s in the order they are given.\n\n `ST_MAKELINE` comes in two variants. For the first variant, input must be two `GEOGRAPHY` s. For the second, input must be an `ARRAY` of type `GEOGRAPHY` . In\neither variant, each input `GEOGRAPHY` must consist of one of the following\nvalues:\n\n- Exactly one point.\n- Exactly one linestring.\n\nFor the first variant of `ST_MAKELINE` , if either input `GEOGRAPHY` is `NULL` , `ST_MAKELINE` returns `NULL` . For the second variant, if input `ARRAY` or any\nelement in the input `ARRAY` is `NULL` , `ST_MAKELINE` returns `NULL` .\n\n **Constraints** \n\nEvery edge must span strictly less than 180 degrees.\n\nNOTE: The GoogleSQL snapping process may discard sufficiently short\nedges and snap the two endpoints together. For instance, if two input `GEOGRAPHY` s each contain a point and the two points are separated by a distance\nless than the snap radius, the points will be snapped together. In such a case\nthe result will be a `GEOGRAPHY` with exactly one point.\n\n **Return type** \n\nLineString `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_MAKEPOLYGON",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nCreates a `GEOGRAPHY` containing a single polygon\nfrom linestring inputs, where each input linestring is used to construct a\npolygon ring.\n\n `ST_MAKEPOLYGON` comes in two variants. For the first variant, the input\nlinestring is provided by a single `GEOGRAPHY` containing exactly one\nlinestring. For the second variant, the input consists of a single `GEOGRAPHY` and an array of `GEOGRAPHY` s, each containing exactly one linestring.\n\nThe first `GEOGRAPHY` in either variant is used to construct the polygon shell.\nAdditional `GEOGRAPHY` s provided in the input `ARRAY` specify a polygon hole.\nFor every input `GEOGRAPHY` containing exactly one linestring, the following\nmust be true:\n\n- The linestring must consist of at least three distinct vertices.\n- The linestring must be closed: that is, the first and last vertex have to be\nthe same. If the first and last vertex differ, the function constructs a\nfinal edge from the first vertex to the last.\n\nFor the first variant of `ST_MAKEPOLYGON` , if either input `GEOGRAPHY` is `NULL` , `ST_MAKEPOLYGON` returns `NULL` . For the second variant, if\ninput `ARRAY` or any element in the `ARRAY` is `NULL` , `ST_MAKEPOLYGON` returns `NULL` .\n\nNOTE: `ST_MAKEPOLYGON` accepts an empty `GEOGRAPHY` as input. `ST_MAKEPOLYGON` interprets an empty `GEOGRAPHY` as having an empty linestring, which will\ncreate a full loop: that is, a polygon that covers the entire Earth.\n\n **Constraints** \n\nTogether, the input rings must form a valid polygon:\n\n- The polygon shell must cover each of the polygon holes.\n- There can be only one polygon shell (which has to be the first input ring).\nThis implies that polygon holes cannot be nested.\n- Polygon rings may only intersect in a vertex on the boundary of both rings.\n\nEvery edge must span strictly less than 180 degrees.\n\nEach polygon ring divides the sphere into two regions. The first input linesting\nto `ST_MAKEPOLYGON` forms the polygon shell, and the interior is chosen to be\nthe smaller of the two regions. Each subsequent input linestring specifies a\npolygon hole, so the interior of the polygon is already well-defined. In order\nto define a polygon shell such that the interior of the polygon is the larger of\nthe two regions, see [ST_MAKEPOLYGONORIENTED](#st_makepolygonoriented) .\n\nNOTE: The GoogleSQL snapping process may discard sufficiently\nshort edges and snap the two endpoints together. Hence, when vertices are\nsnapped together, it is possible that a polygon hole that is sufficiently small\nmay disappear, or the output `GEOGRAPHY` may contain only a line or a\npoint.\n\n **Return type** \n\n `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_MAKEPOLYGONORIENTED",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nLike `ST_MAKEPOLYGON` , but the vertex ordering of each input linestring\ndetermines the orientation of each polygon ring. The orientation of a polygon\nring defines the interior of the polygon as follows: if someone walks along the\nboundary of the polygon in the order of the input vertices, the interior of the\npolygon is on the left. This applies for each polygon ring provided.\n\nThis variant of the polygon constructor is more flexible since `ST_MAKEPOLYGONORIENTED` can construct a polygon such that the interior is on\neither side of the polygon ring. However, proper orientation of polygon rings is\ncritical in order to construct the desired polygon.\n\nIf the input `ARRAY` or any element in the `ARRAY` is `NULL` , `ST_MAKEPOLYGONORIENTED` returns `NULL` .\n\nNOTE: The input argument for `ST_MAKEPOLYGONORIENTED` may contain an empty `GEOGRAPHY` . `ST_MAKEPOLYGONORIENTED` interprets an empty `GEOGRAPHY` as having\nan empty linestring, which will create a full loop: that is, a polygon that\ncovers the entire Earth.\n\n **Constraints** \n\nTogether, the input rings must form a valid polygon:\n\n- The polygon shell must cover each of the polygon holes.\n- There must be only one polygon shell, which must to be the first input ring.\nThis implies that polygon holes cannot be nested.\n- Polygon rings may only intersect in a vertex on the boundary of both rings.\n\nEvery edge must span strictly less than 180 degrees.\n\n `ST_MAKEPOLYGONORIENTED` relies on the ordering of the input vertices of each\nlinestring to determine the orientation of the polygon. This applies to the\npolygon shell and any polygon holes. `ST_MAKEPOLYGONORIENTED` expects all\npolygon holes to have the opposite orientation of the shell. See [ST_MAKEPOLYGON](#st_makepolygon) for an alternate polygon constructor, and\nother constraints on building a valid polygon.\n\nNOTE: Due to the GoogleSQL snapping process, edges with a sufficiently\nshort length will be discarded and the two endpoints will be snapped to a single\npoint. Therefore, it is possible that vertices in a linestring may be snapped\ntogether such that one or more edge disappears. Hence, it is possible that a\npolygon hole that is sufficiently small may disappear, or the resulting `GEOGRAPHY` may contain only a line or a point.\n\n **Return type** \n\n `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_MAXDISTANCE",
    "arguments": [],
    "category": "Geography",
    "description_markdown": "Returns the longest distance in meters between two non-empty `GEOGRAPHY` s; that is, the distance between two\nvertices where the first vertex is in the first `GEOGRAPHY` , and the second vertex is in the second `GEOGRAPHY` . If `geography_1` and `geography_2` are the\nsame `GEOGRAPHY` , the function returns the distance\nbetween the two most distant vertices in that `GEOGRAPHY` .\n\nIf either of the input `GEOGRAPHY` s is empty, `ST_MAXDISTANCE` returns `NULL` .\n\nThe optional `use_spheroid` parameter determines how this function measures\ndistance. If `use_spheroid` is `FALSE` , the function measures distance on the\nsurface of a perfect sphere.\n\nThe `use_spheroid` parameter currently only supports\nthe value `FALSE` . The default value of `use_spheroid` is `FALSE` .\n\n **Return type** \n\n `FLOAT64` \n\n\n\n"
  },
  {
    "name": "ST_NPOINTS",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nAn alias of [ST_NUMPOINTS](#st_numpoints) .\n\n\n\n"
  },
  {
    "name": "ST_NUMGEOMETRIES",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the number of geometries in the input `GEOGRAPHY` . For a single point,\nlinestring, or polygon, `ST_NUMGEOMETRIES` returns `1` . For any collection of\ngeometries, `ST_NUMGEOMETRIES` returns the number of geometries making up the\ncollection. `ST_NUMGEOMETRIES` returns `0` if the input is the empty `GEOGRAPHY` .\n\n **Return type** \n\n `INT64` \n\n **Example** \n\nThe following example computes `ST_NUMGEOMETRIES` for a single point geography,\ntwo collections, and an empty geography.\n\n\n"
  },
  {
    "name": "ST_NUMPOINTS",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the number of vertices in the input `GEOGRAPHY` . This includes the number of points, the\nnumber of linestring vertices, and the number of polygon vertices.\n\nNOTE: The first and last vertex of a polygon ring are counted as distinct\nvertices.\n\n **Return type** \n\n `INT64` \n\n\n\n"
  },
  {
    "name": "ST_PERIMETER",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the length in meters of the boundary of the polygons in the input `GEOGRAPHY` .\n\nIf `geography_expression` is a point or a line, returns zero. If `geography_expression` is a collection, returns the perimeter of the polygons\nin the collection; if the collection does not contain polygons, returns zero.\n\nThe optional `use_spheroid` parameter determines how this function measures\ndistance. If `use_spheroid` is `FALSE` , the function measures distance on the\nsurface of a perfect sphere.\n\nThe `use_spheroid` parameter currently only supports\nthe value `FALSE` . The default value of `use_spheroid` is `FALSE` .\n\n **Return type** \n\n `FLOAT64` \n\n\n\n"
  },
  {
    "name": "ST_POINTN",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the Nth point of a linestring geography as a point geography, where N is\nthe index. The index is 1-based. Negative values are counted backwards from the\nend of the linestring, so that -1 is the last point. Returns an error if the\ninput is not a linestring, if the input is empty, or if there is no vertex at\nthe given index. Use the `SAFE` prefix to obtain `NULL` for invalid input\ninstead of an error.\n\n **Return Type** \n\nPoint `GEOGRAPHY` \n\n **Example** \n\nThe following example uses `ST_POINTN` , [ST_STARTPOINT](#st_startpoint) and [ST_ENDPOINT](#st_endpoint) to extract points from a linestring.\n\n\n"
  },
  {
    "name": "ST_SIMPLIFY",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a simplified version of `geography` , the given input `GEOGRAPHY` . The input `GEOGRAPHY` is simplified by replacing nearly straight\nchains of short edges with a single long edge. The input `geography` will not\nchange by more than the tolerance specified by `tolerance_meters` . Thus,\nsimplified edges are guaranteed to pass within `tolerance_meters` of the *original* positions of all vertices that were removed from that edge. The given `tolerance_meters` is in meters on the surface of the Earth.\n\nNote that `ST_SIMPLIFY` preserves topological relationships, which means that\nno new crossing edges will be created and the output will be valid. For a large\nenough tolerance, adjacent shapes may collapse into a single object, or a shape\ncould be simplified to a shape with a smaller dimension.\n\n **Constraints** \n\nFor `ST_SIMPLIFY` to have any effect, `tolerance_meters` must be non-zero.\n\n `ST_SIMPLIFY` returns an error if the tolerance specified by `tolerance_meters` is one of the following:\n\n- A negative tolerance.\n- Greater than ~7800 kilometers.\n\n **Return type** \n\n `GEOGRAPHY` \n\n **Examples** \n\nThe following example shows how `ST_SIMPLIFY` simplifies the input line `GEOGRAPHY` by removing intermediate vertices.\n\nThe following example illustrates how the result of `ST_SIMPLIFY` can have a\nlower dimension than the original shape.\n\n\n"
  },
  {
    "name": "ST_SNAPTOGRID",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the input `GEOGRAPHY` , where each vertex has\nbeen snapped to a longitude/latitude grid. The grid size is determined by the `grid_size` parameter which is given in degrees.\n\n **Constraints** \n\nArbitrary grid sizes are not supported. The `grid_size` parameter is rounded so\nthat it is of the form `10^n` , where `-10 &lt; n &lt; 0` .\n\n **Return type** \n\n `GEOGRAPHY` \n\n\n\n"
  },
  {
    "name": "ST_STARTPOINT",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the first point of a linestring geography as a point geography. Returns\nan error if the input is not a linestring or if the input is empty. Use the `SAFE` prefix to obtain `NULL` for invalid input instead of an error.\n\n **Return Type** \n\nPoint `GEOGRAPHY` \n\n **Example** \n\n\n"
  },
  {
    "name": "ST_TOUCHES",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` provided the following two conditions are satisfied:\n\n1.  `    geography_1` intersects `    geography_2` .\n1. The interior of `    geography_1` and the interior of `    geography_2` are\ndisjoint.\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_UNION",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `GEOGRAPHY` that represents the point set\nunion of all input `GEOGRAPHY` s.\n\n `ST_UNION` comes in two variants. For the first variant, input must be two `GEOGRAPHY` s. For the second, the input is an `ARRAY` of type `GEOGRAPHY` .\n\nFor the first variant of `ST_UNION` , if an input `GEOGRAPHY` is `NULL` , `ST_UNION` returns `NULL` .\nFor the second variant, if the input `ARRAY` value\nis `NULL` , `ST_UNION` returns `NULL` .\nFor a non- `NULL` input `ARRAY` , the union is computed\nand `NULL` elements are ignored so that they do not affect the output.\n\nSee [ST_UNION_AGG](#st_union_agg) for the aggregate version of `ST_UNION` .\n\n **Return type** \n\n `GEOGRAPHY` \n\n **Example** \n\n\n"
  },
  {
    "name": "ST_UNION_AGG",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns a `GEOGRAPHY` that represents the point set\nunion of all input `GEOGRAPHY` s.\n\n `ST_UNION_AGG` ignores `NULL` input `GEOGRAPHY` values.\n\nSee [ST_UNION](#st_union) for the non-aggregate version of `ST_UNION_AGG` .\n\n **Return type** \n\n `GEOGRAPHY` \n\n **Example** \n\n\n"
  },
  {
    "name": "ST_WITHIN",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns `TRUE` if no point of `geography_1` is outside of `geography_2` and\nthe interiors of `geography_1` and `geography_2` intersect.\n\nGiven two geographies `a` and `b` , `ST_WITHIN(a, b)` returns the same result\nas [ST_CONTAINS](#st_contains)  `(b, a)` . Note the opposite order of arguments.\n\n **Return type** \n\n `BOOL` \n\n\n\n"
  },
  {
    "name": "ST_X",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the longitude in degrees of the single-point input `GEOGRAPHY` .\n\nFor any input `GEOGRAPHY` that is not a single point,\nincluding an empty `GEOGRAPHY` , `ST_X` returns an\nerror. Use the `SAFE.` prefix to obtain `NULL` .\n\n **Return type** \n\n `FLOAT64` \n\n **Example** \n\nThe following example uses `ST_X` and `ST_Y` to extract coordinates from\nsingle-point geographies.\n\n\n"
  },
  {
    "name": "ST_Y",
    "arguments": [],
    "category": "Geography",
    "description_markdown": " **Description** \n\nReturns the latitude in degrees of the single-point input `GEOGRAPHY` .\n\nFor any input `GEOGRAPHY` that is not a single point,\nincluding an empty `GEOGRAPHY` , `ST_Y` returns an\nerror. Use the `SAFE.` prefix to return `NULL` instead.\n\n **Return type** \n\n `FLOAT64` \n\n **Example** \n\nSee [ST_X](#st_x) for example usage.\n\n\n<span id=\"hash_functions\"></span>\n## Hash functions\n\n\nGoogleSQL for BigQuery supports the following hash functions.\n\n\n\n"
  },
  {
    "name": "SUBSTR",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nGets a portion (substring) of the supplied `STRING` or `BYTES` value.\n\nThe `position` argument is an integer specifying the starting position of the\nsubstring.\n\n- If `    position` is `    1` , the substring starts from the first character or byte.\n- If `    position` is `    0` or less than `    -LENGTH(value)` , `    position` is set to `    1` ,\nand the substring starts from the first character or byte.\n- If `    position` is greater than the length of `    value` , the function produces\nan empty substring.\n- If `    position` is negative, the function counts from the end of `    value` ,\nwith `    -1` indicating the last character or byte.\n\nThe `length` argument specifies the maximum number of characters or bytes to\nreturn.\n\n- If `    length` is not specified, the function produces a substring that starts\nat the specified position and ends at the last character or byte of `    value` .\n- If `    length` is `    0` , the function produces an empty substring.\n- If `    length` is negative, the function produces an error.\n- The returned substring may be shorter than `    length` , for example, when `    length` exceeds the length of `    value` , or when the starting position of the\nsubstring plus `    length` is greater than the length of `    value` .\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "SUBSTRING",
    "arguments": [],
    "category": "String",
    "description_markdown": "Alias for [SUBSTR](#substr) .\n\n\n\n"
  },
  {
    "name": "SUM",
    "arguments": [],
    "category": "Aggregate",
    "description_markdown": " **Description** \n\nReturns the sum of non- `NULL` values in an aggregated group.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n `SUM` can be used with differential privacy. For more information, see [Differentially private aggregate functions](#aggregate-dp-functions) .\n\nCaveats:\n\n- If the aggregated group is empty or the argument is `    NULL` for all rows in\nthe group, returns `    NULL` .\n- If the argument is `    NaN` for any row in the group, returns `    NaN` .\n- If the argument is `    [+|-]Infinity` for any row in the group, returns either `    [+|-]Infinity` or `    NaN` .\n- If there is numeric overflow, produces an error.\n- If a [floating-point type](/bigquery/docs/reference/standard-sql/data-types#floating_point_types) is returned, the result is [non-deterministic](/bigquery/docs/reference/standard-sql/data-types#floating-point-semantics) , which means you might receive a\ndifferent result each time you use this function.\n\n **Supported Argument Types** \n\n- Any supported numeric data type\n-  `    INTERVAL` \n\n **Return Data Types** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |  `INTERVAL` |\n| --- | --- | --- | --- | --- | --- |\n| OUTPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |  `INTERVAL` |\n\n **Examples** \n\n\n<span id=\"approximate_aggregate_functions\"></span>\n## Approximate aggregate functions\n\n\nGoogleSQL for BigQuery supports approximate aggregate functions.\nTo learn about the syntax for aggregate function calls, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nApproximate aggregate functions are scalable in terms of memory usage and time,\nbut produce approximate results instead of exact results. These functions\ntypically require less memory than [exact aggregation functions](#aggregate_functions) like `COUNT(DISTINCT ...)` , but also introduce statistical uncertainty.\nThis makes approximate aggregation appropriate for large data streams for\nwhich linear memory usage is impractical, as well as for data that is\nalready approximate.\n\nThe approximate aggregate functions in this section work directly on the\ninput data, rather than an intermediate estimation of the data. These functions *do not allow* users to specify the precision for the estimation with\nsketches. If you would like to specify precision with sketches, see:\n\n-  [HyperLogLog++ functions](#hyperloglog_functions) to estimate cardinality.\n\n\n\n"
  },
  {
    "name": "TAN",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the tangent of X where X is specified in radians. Generates an error if\noverflow occurs.\n\n| X | TAN(X) |\n| --- | --- |\n|  `+inf` |  `NaN` |\n|  `-inf` |  `NaN` |\n|  `NaN` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "TANH",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nComputes the hyperbolic tangent of X where X is specified in radians. Does not\nfail.\n\n| X | TANH(X) |\n| --- | --- |\n|  `+inf` | 1.0 |\n|  `-inf` | -1.0 |\n|  `NaN` |  `NaN` |\n\n\n\n"
  },
  {
    "name": "TIME",
    "arguments": [],
    "category": "Time",
    "description_markdown": " **Description** \n\n1. Constructs a `    TIME` object using `    INT64` values representing the hour, minute, and second.\n1. Constructs a `    TIME` object using a `    TIMESTAMP` object. It supports an\noptional\nparameter to [specify a time zone](#timezone_definitions) . If no\ntime zone is specified, the default time zone, UTC, is\nused.\n1. Constructs a `    TIME` object using a `    DATETIME` object.\n\n **Return Data Type** \n\n `TIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIMESTAMP",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\n-  `    string_expression[, time_zone]` : Converts a string to a\ntimestamp. `    string_expression` must include a\ntimestamp literal.\nIf `    string_expression` includes a time zone in the timestamp literal, do\nnot include an explicit `    time_zone` argument.\n-  `    date_expression[, time_zone]` : Converts a date to a timestamp.\nThe value returned is the earliest timestamp that falls within\nthe given date.\n-  `    datetime_expression[, time_zone]` : Converts a\ndatetime to a timestamp.\n\nThis function supports an optional\nparameter to [specify a time zone](#timezone_definitions) . If\nno time zone is specified, the default time zone, UTC,\nis used.\n\n **Return Data Type** \n\n `TIMESTAMP` \n\n **Examples** \n\n\n"
  },
  {
    "name": "TIMESTAMP_ADD",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nAdds `int64_expression` units of `date_part` to the timestamp, independent of\nany time zone.\n\n `TIMESTAMP_ADD` supports the following values for `date_part` :\n\n-  `    MICROSECOND` \n-  `    MILLISECOND` \n-  `    SECOND` \n-  `    MINUTE` \n-  `    HOUR` . Equivalent to 60 `    MINUTE` parts.\n-  `    DAY` . Equivalent to 24 `    HOUR` parts.\n\n **Return Data Types** \n\n `TIMESTAMP` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIMESTAMP_BUCKET",
    "arguments": [],
    "category": "Time_series",
    "description_markdown": " **Description** \n\nGets the lower bound of the timestamp bucket that contains a timestamp.\n\n **Definitions** \n\n-  `    timestamp_in_bucket` : A `    TIMESTAMP` value that you can use to look up a\ntimestamp bucket.\n-  `    bucket_width` : An `    INTERVAL` value that represents the width of\na timestamp bucket. A [single interval](/bigquery/docs/reference/standard-sql/data-types#single_datetime_part_interval) with [date and time parts](/bigquery/docs/reference/standard-sql/data-types#interval_datetime_parts) is supported.\n-  `    bucket_origin_timestamp` : A `    TIMESTAMP` value that represents a point in\ntime. All buckets expand left and right from this point. If this argument\nis not set, `    1950-01-01 00:00:00` is used by default.\n\n **Return type** \n\n `TIMESTAMP` \n\n **Examples** \n\nIn the following example, the origin is omitted and the default origin, `1950-01-01 00:00:00` is used. All buckets expand in both directions from the\norigin, and the size of each bucket is 12 hours. The lower bound of the bucket\nin which `my_timestamp` belongs is returned:\n\nIn the following example, the origin has been changed to `2000-12-24 12:00:00` ,\nand all buckets expand in both directions from this point. The size of each\nbucket is seven days. The lower bound of the bucket in which `my_timestamp` belongs is returned:\n\n\n<span id=\"timestamp_functions\"></span>\n## Timestamp functions\n\n\nGoogleSQL for BigQuery supports the following timestamp functions.\n\nIMPORTANT: Before working with these functions, you need to understand\nthe difference between the formats in which timestamps are stored and displayed,\nand how time zones are used for the conversion between these formats.\nTo learn more, see [How time zones work with timestamp functions](#timezone_definitions) .\n\nNOTE: These functions return a runtime error if overflow occurs; result\nvalues are bounded by the defined [DATE range](/bigquery/docs/reference/standard-sql/data-types#date_type) and [TIMESTAMP range](/bigquery/docs/reference/standard-sql/data-types#timestamp_type) .\n\n\n\n"
  },
  {
    "name": "TIMESTAMP_DIFF",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nGets the number of unit boundaries between two `TIMESTAMP` values\n( `end_timestamp` - `start_timestamp` ) at a particular time granularity.\n\n **Definitions** \n\n-  `    start_timestamp` : The starting `    TIMESTAMP` value.\n-  `    end_timestamp` : The ending `    TIMESTAMP` value.\n-  `    granularity` : The timestamp part that represents the granularity. If\nyou passed in `    TIMESTAMP` values for the first arguments, `    granularity` can\nbe:\n    \n    \n    -  `        MICROSECOND` \n    -  `        MILLISECOND` \n    -  `        SECOND` \n    -  `        MINUTE` \n    -  `        HOUR` . Equivalent to 60 `        MINUTE` s.\n    -  `        DAY` . Equivalent to 24 `        HOUR` s.\n\n **Details** \n\nIf `end_timestamp` is earlier than `start_timestamp` , the output is negative.\nProduces an error if the computation overflows, such as if the difference\nin microseconds\nbetween the two `TIMESTAMP` values overflows.\n\n **Note:** The behavior of the this function follows the type of arguments passed in.\nFor example, `TIMESTAMP_DIFF(DATE, DATE, PART)` behaves like `DATE_DIFF(DATE, DATE, PART)` . **Return Data Type** \n\n `INT64` \n\n **Example** \n\nIn the following example, the first timestamp occurs before the\nsecond timestamp, resulting in a negative output.\n\nIn this example, the result is 0 because only the number of whole specified `HOUR` intervals are included.\n\n\n"
  },
  {
    "name": "TIMESTAMP_MICROS",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nInterprets `int64_expression` as the number of microseconds since 1970-01-01\n00:00:00 UTC and returns a timestamp.\n\n **Return Data Type** \n\n `TIMESTAMP` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIMESTAMP_MILLIS",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nInterprets `int64_expression` as the number of milliseconds since 1970-01-01\n00:00:00 UTC and returns a timestamp.\n\n **Return Data Type** \n\n `TIMESTAMP` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIMESTAMP_SECONDS",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nInterprets `int64_expression` as the number of seconds since 1970-01-01 00:00:00\nUTC and returns a timestamp.\n\n **Return Data Type** \n\n `TIMESTAMP` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIMESTAMP_SUB",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nSubtracts `int64_expression` units of `date_part` from the timestamp,\nindependent of any time zone.\n\n `TIMESTAMP_SUB` supports the following values for `date_part` :\n\n-  `    MICROSECOND` \n-  `    MILLISECOND` \n-  `    SECOND` \n-  `    MINUTE` \n-  `    HOUR` . Equivalent to 60 `    MINUTE` parts.\n-  `    DAY` . Equivalent to 24 `    HOUR` parts.\n\n **Return Data Type** \n\n `TIMESTAMP` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIMESTAMP_TRUNC",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nTruncates a `TIMESTAMP` value at a particular time granularity. The `TIMESTAMP` value is always rounded to the beginning of `granularity` .\n\n **Definitions** \n\n-  `    timestamp_expression` : The `    TIMESTAMP` value to truncate.\n-  `    granularity` : The datetime part that represents the granularity. If\nyou passed in a `    TIMESTAMP` value for the first argument, `    granularity` can\nbe:\n    \n    \n    -  `        MICROSECOND` : If used, nothing is truncated from the value.\n        \n        \n    -  `        MILLISECOND` : The nearest lesser than or equal millisecond.\n        \n        \n    -  `        SECOND` : The nearest lesser than or equal second.\n        \n        \n    -  `        MINUTE` : The nearest lesser than or equal minute.\n        \n        \n    -  `        HOUR` : The nearest lesser than or equal hour.\n        \n        \n    -  `        DAY` : The day in the Gregorian calendar year that contains the `        TIMESTAMP` value.\n        \n        \n    -  `        WEEK` : The first day in the week that contains the `        TIMESTAMP` value. Weeks begin on Sundays. `        WEEK` is equivalent to `        WEEK(SUNDAY)` .\n        \n        \n    -  `        WEEK(WEEKDAY)` : The first day in the week that contains the `        TIMESTAMP` value. Weeks begin on `        WEEKDAY` . `        WEEKDAY` must be one of the\nfollowing: `        SUNDAY` , `        MONDAY` , `        TUESDAY` , `        WEDNESDAY` , `        THURSDAY` , `        FRIDAY` ,\nor `        SATURDAY` .\n        \n        \n    -  `        ISOWEEK` : The first day in the [ISO 8601 week](https://en.wikipedia.org/wiki/ISO_week_date) that contains\nthe `        TIMESTAMP` value. The ISO week begins on\nMonday. The first ISO week of each ISO year contains the first Thursday of the\ncorresponding Gregorian calendar year.\n        \n        \n    -  `        MONTH` : The first day in the month that contains the `        TIMESTAMP` value.\n        \n        \n    -  `        QUARTER` : The first day in the quarter that contains the `        TIMESTAMP` value.\n        \n        \n    -  `        YEAR` : The first day in the year that contains the `        TIMESTAMP` value.\n        \n        \n    -  `        ISOYEAR` : The first day in the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year\nthat contains the `        TIMESTAMP` value. The ISO year is the\nMonday of the first week where Thursday belongs to the corresponding\nGregorian calendar year.\n        \n        \n\n-  `    time_zone` : Use this parameter if you want to use a time zone other than\nthe default time zone, UTC, as part of the\ntruncate operation. This can be:\n    \n    \n    -  `        MINUTE` \n    -  `        HOUR` \n    -  `        DAY` \n    -  `        WEEK` \n    -  `        WEEK(&lt;WEEKDAY&gt;)` \n    -  `        ISOWEEK` \n    -  `        MONTH` \n    -  `        QUARTER` \n    -  `        YEAR` \n    -  `        ISOYEAR` \n\nWhen truncating a timestamp to `MINUTE` or `HOUR` parts, `TIMESTAMP_TRUNC` determines the civil time of the\ntimestamp in the specified (or default) time zone\nand subtracts the minutes and seconds (when truncating to `HOUR` ) or the seconds\n(when truncating to `MINUTE` ) from that timestamp.\nWhile this provides intuitive results in most cases, the result is\nnon-intuitive near daylight savings transitions that are not hour-aligned.\n\n **Return Data Type** \n\n `TIMESTAMP` \n\n **Examples** \n\nIn the following example, `timestamp_expression` has a time zone offset of +12.\nThe first column shows the `timestamp_expression` in UTC time. The second\ncolumn shows the output of `TIMESTAMP_TRUNC` using weeks that start on Monday.\nBecause the `timestamp_expression` falls on a Sunday in UTC, `TIMESTAMP_TRUNC` truncates it to the preceding Monday. The third column shows the same function\nwith the optional [Time zone definition](#timezone_definitions) argument 'Pacific/Auckland'. Here, the function truncates the `timestamp_expression` using New Zealand Daylight Time, where it falls on a\nMonday.\n\nIn the following example, the original `timestamp_expression` is in the\nGregorian calendar year 2015. However, `TIMESTAMP_TRUNC` with the `ISOYEAR` date\npart truncates the `timestamp_expression` to the beginning of the ISO year, not\nthe Gregorian calendar year. The first Thursday of the 2015 calendar year was\n2015-01-01, so the ISO year 2015 begins on the preceding Monday, 2014-12-29.\nTherefore the ISO year boundary preceding the `timestamp_expression` 2015-06-15 00:00:00+00 is 2014-12-29.\n\n\n"
  },
  {
    "name": "TIME_ADD",
    "arguments": [],
    "category": "Time",
    "description_markdown": " **Description** \n\nAdds `int64_expression` units of `part` to the `TIME` object.\n\n `TIME_ADD` supports the following values for `part` :\n\n-  `    MICROSECOND` \n-  `    MILLISECOND` \n-  `    SECOND` \n-  `    MINUTE` \n-  `    HOUR` \n\nThis function automatically adjusts when values fall outside of the 00:00:00 to\n24:00:00 boundary. For example, if you add an hour to `23:30:00` , the returned\nvalue is `00:30:00` .\n\n **Return Data Types** \n\n `TIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIME_DIFF",
    "arguments": [],
    "category": "Time",
    "description_markdown": " **Description** \n\nGets the number of unit boundaries between two `TIME` values ( `end_time` - `start_time` ) at a particular time granularity.\n\n **Definitions** \n\n-  `    start_time` : The starting `    TIME` value.\n-  `    end_time` : The ending `    TIME` value.\n-  `    granularity` : The time part that represents the granularity. If\nyou passed in `    TIME` values for the first arguments, `    granularity` can\nbe:\n    \n    \n    -  `        MICROSECOND` \n    -  `        MILLISECOND` \n    -  `        SECOND` \n    -  `        MINUTE` \n    -  `        HOUR` \n\n **Details** \n\nIf `end_time` is earlier than `start_time` , the output is negative.\nProduces an error if the computation overflows, such as if the difference\nin microseconds\nbetween the two `TIME` values overflows.\n\n **Note:** The behavior of the this function follows the type of arguments passed in.\nFor example, `TIME_DIFF(TIMESTAMP, TIMESTAMP, PART)` behaves like `TIMESTAMP_DIFF(TIMESTAMP, TIMESTAMP, PART)` . **Return Data Type** \n\n `INT64` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIME_SUB",
    "arguments": [],
    "category": "Time",
    "description_markdown": " **Description** \n\nSubtracts `int64_expression` units of `part` from the `TIME` object.\n\n `TIME_SUB` supports the following values for `part` :\n\n-  `    MICROSECOND` \n-  `    MILLISECOND` \n-  `    SECOND` \n-  `    MINUTE` \n-  `    HOUR` \n\nThis function automatically adjusts when values fall outside of the 00:00:00 to\n24:00:00 boundary. For example, if you subtract an hour from `00:30:00` , the\nreturned value is `23:30:00` .\n\n **Return Data Type** \n\n `TIME` \n\n **Example** \n\n\n"
  },
  {
    "name": "TIME_TRUNC",
    "arguments": [],
    "category": "Time",
    "description_markdown": " **Description** \n\nTruncates a `TIME` value at a particular time granularity. The `TIME` value\nis always rounded to the beginning of `granularity` .\n\n **Definitions** \n\n-  `    time_expression` : The `    TIME` value to truncate.\n-  `    granularity` : The time part that represents the granularity. If\nyou passed in a `    TIME` value for the first argument, `    granularity` can\nbe:\n    \n    \n    -  `        MICROSECOND` : If used, nothing is truncated from the value.\n        \n        \n    -  `        MILLISECOND` : The nearest lesser than or equal millisecond.\n        \n        \n    -  `        SECOND` : The nearest lesser than or equal second.\n        \n        \n    -  `        MINUTE` : The nearest lesser than or equal minute.\n        \n        \n    -  `        HOUR` : The nearest lesser than or equal hour.\n        \n        \n\n **Return Data Type** \n\n `TIME` \n\n **Example** \n\n\n<span id=\"time_series_functions\"></span>\n## Time series functions\n\n\nGoogleSQL for BigQuery supports the following time series functions.\n\n\n\n"
  },
  {
    "name": "TO_BASE32",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nConverts a sequence of `BYTES` into a base32-encoded `STRING` . To convert a\nbase32-encoded `STRING` into `BYTES` , use [FROM_BASE32](#from_base32) .\n\n **Return type** \n\n `STRING` \n\n **Example** \n\n\n"
  },
  {
    "name": "TO_BASE64",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nConverts a sequence of `BYTES` into a base64-encoded `STRING` . To convert a\nbase64-encoded `STRING` into `BYTES` , use [FROM_BASE64](#from_base64) .\n\nThere are several base64 encodings in common use that vary in exactly which\nalphabet of 65 ASCII characters are used to encode the 64 digits and padding.\nSee [RFC 4648](https://tools.ietf.org/html/rfc4648#section-4) for details. This\nfunction adds padding and uses the alphabet `[A-Za-z0-9+/=]` .\n\n **Return type** \n\n `STRING` \n\n **Example** \n\nTo work with an encoding using a different base64 alphabet, you might need to\ncompose `TO_BASE64` with the `REPLACE` function. For instance, the `base64url` url-safe and filename-safe encoding commonly used in web programming\nuses `-_=` as the last characters rather than `+/=` . To encode a `base64url` -encoded string, replace `+` and `/` with `-` and `_` respectively.\n\n\n"
  },
  {
    "name": "TO_CODE_POINTS",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes a `STRING` or `BYTES` value and returns an array of `INT64` values that\nrepresent code points or extended ASCII character values.\n\n- If `    value` is a `    STRING` , each element in the returned array represents a [code point](https://en.wikipedia.org/wiki/Code_point) . Each code point falls\nwithin the range of [0, 0xD7FF] and [0xE000, 0x10FFFF].\n- If `    value` is `    BYTES` , each element in the array is an extended ASCII\ncharacter value in the range of [0, 255].\n\nTo convert from an array of code points to a `STRING` or `BYTES` , see [CODE_POINTS_TO_STRING](#code_points_to_string) or [CODE_POINTS_TO_BYTES](#code_points_to_bytes) .\n\n **Return type** \n\n `ARRAY&lt;INT64&gt;` \n\n **Examples** \n\nThe following examples get the code points for each element in an array of\nwords.\n\nThe following examples convert integer representations of `BYTES` to their\ncorresponding ASCII character values.\n\nThe following example demonstrates the difference between a `BYTES` result and a `STRING` result. Notice that the character `Ā` is represented as a two-byte\nUnicode sequence. As a result, the `BYTES` version of `TO_CODE_POINTS` returns\nan array with two elements, while the `STRING` version returns an array with a\nsingle element.\n\n\n"
  },
  {
    "name": "TO_HEX",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nConverts a sequence of `BYTES` into a hexadecimal `STRING` . Converts each byte\nin the `STRING` as two hexadecimal characters in the range `(0..9, a..f)` . To convert a hexadecimal-encoded `STRING` to `BYTES` , use [FROM_HEX](#from_hex) .\n\n **Return type** \n\n `STRING` \n\n **Example** \n\n\n"
  },
  {
    "name": "TO_JSON",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nConverts a SQL value to a JSON value.\n\nArguments:\n\n-  `    sql_value` : The SQL value to convert to a JSON value. You can review the\nGoogleSQL data types that this function supports and their\nJSON encodings [here](#json_encodings) .\n-  `    stringify_wide_numbers` : A named argument that's either `    TRUE` or `    FALSE` (default).\n    \n    \n    - If `        TRUE` , numeric values outside of the `        FLOAT64` type domain are encoded as strings.\n    - If `        FALSE` (default), numeric values outside of the `        FLOAT64` type domain are not encoded as strings,\nbut are stored as JSON numbers. If a numerical value cannot be stored in\nJSON without loss of precision, an error is thrown.The following numerical data types are affected by the `    stringify_wide_numbers` argument:\n    \n    \n-  `    INT64` \n    \n    \n-  `    NUMERIC` \n    \n    \n-  `    BIGNUMERIC` \n    \n    If one of these numerical data types appears in a container data type\nsuch as an `    ARRAY` or `    STRUCT` , the `    stringify_wide_numbers` argument is\napplied to the numerical data types in the container data type.\n    \n    \n\n **Return type** \n\n `JSON` \n\n **Examples** \n\nIn the following example, the query converts rows in a table to JSON values.\n\nIn the following example, the query returns a large numerical value as a\nJSON string.\n\nIn the following example, both queries return a large numerical value as a\nJSON number.\n\nIn the following example, only large numeric values are converted to\nJSON strings.\n\nIn this example, the values `9007199254740993` ( `INT64` )\nand `2.1` ( `FLOAT64` ) are converted\nto the common supertype `FLOAT64` , which is not\naffected by the `stringify_wide_numbers` argument.\n\n\n"
  },
  {
    "name": "TO_JSON_STRING",
    "arguments": [],
    "category": "JSON",
    "description_markdown": " **Description** \n\nConverts a SQL value to a JSON-formatted `STRING` value.\n\nArguments:\n\n-  `    value` : A SQL value. You can review the GoogleSQL data types that\nthis function supports and their JSON encodings [here](#json_encodings) .\n-  `    pretty_print` : Optional boolean parameter. If `    pretty_print` is `    true` , the\n`returned value is formatted for easy readability.\n\n **Return type** \n\nA JSON-formatted `STRING` \n\n **Examples** \n\nThe following query converts a `STRUCT` value to a JSON-formatted string:\n\nThe following query converts a `STRUCT` value to a JSON-formatted string that is\neasy to read:\n\n\n"
  },
  {
    "name": "TRANSLATE",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nIn `expression` , replaces each character in `source_characters` with the\ncorresponding character in `target_characters` . All inputs must be the same\ntype, either `STRING` or `BYTES` .\n\n- Each character in `    expression` is translated at most once.\n- A character in `    expression` that is not present in `    source_characters` is left\nunchanged in `    expression` .\n- A character in `    source_characters` without a corresponding character in `    target_characters` is omitted from the result.\n- A duplicate character in `    source_characters` results in an error.\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n"
  },
  {
    "name": "TRIM",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nTakes a `STRING` or `BYTES` value to trim.\n\nIf the value to trim is a `STRING` , removes from this value all leading and\ntrailing Unicode code points in `set_of_characters_to_remove` .\nThe set of code points is optional. If it is not specified, all\nwhitespace characters are removed from the beginning and end of the\nvalue to trim.\n\nIf the value to trim is `BYTES` , removes from this value all leading and\ntrailing bytes in `set_of_characters_to_remove` . The set of bytes is required.\n\n **Return type** \n\n-  `    STRING` if `    value_to_trim` is a `    STRING` value.\n-  `    BYTES` if `    value_to_trim` is a `    BYTES` value.\n\n **Examples** \n\nIn the following example, all leading and trailing whitespace characters are\nremoved from `item` because `set_of_characters_to_remove` is not specified.\n\nIn the following example, all leading and trailing `*` characters are removed\nfrom ' ** *apple* ** '.\n\nIn the following example, all leading and trailing `x` , `y` , and `z` characters\nare removed from 'xzxapplexxy'.\n\nIn the following example, examine how `TRIM` interprets characters as\nUnicode code-points. If your trailing character set contains a combining\ndiacritic mark over a particular letter, `TRIM` might strip the\nsame diacritic mark from a different letter.\n\nIn the following example, all leading and trailing `b'n'` , `b'a'` , `b'\\xab'` bytes are removed from `item` .\n\n\n"
  },
  {
    "name": "TRUNC",
    "arguments": [],
    "category": "Mathematical",
    "description_markdown": " **Description** \n\nIf only X is present, `TRUNC` rounds X to the nearest integer whose absolute\nvalue is not greater than the absolute value of X. If N is also present, `TRUNC` behaves like `ROUND(X, N)` , but always rounds towards zero and never overflows.\n\n| X | TRUNC(X) |\n| --- | --- |\n| 2.0 | 2.0 |\n| 2.3 | 2.0 |\n| 2.8 | 2.0 |\n| 2.5 | 2.0 |\n| -2.3 | -2.0 |\n| -2.8 | -2.0 |\n| -2.5 | -2.0 |\n| 0 | 0 |\n|  `+inf` |  `+inf` |\n|  `-inf` |  `-inf` |\n|  `NaN` |  `NaN` |\n\n **Return Data Type** \n\n| INPUT |  `INT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n| --- | --- | --- | --- | --- |\n| OUTPUT |  `FLOAT64` |  `NUMERIC` |  `BIGNUMERIC` |  `FLOAT64` |\n\n\n<span id=\"navigation_functions\"></span>\n## Navigation functions\n\n\nGoogleSQL for BigQuery supports navigation functions.\nNavigation functions are a subset of window functions. To create a\nwindow function call and learn about the syntax for window functions,\nsee [Window function_calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nNavigation functions generally compute some `value_expression` over a different row in the window frame from the\ncurrent row. The `OVER` clause syntax varies across navigation functions.\n\nFor all navigation functions, the result data type is the same type as `value_expression` .\n\n\n\n"
  },
  {
    "name": "UNICODE",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nReturns the Unicode [code point](https://en.wikipedia.org/wiki/Code_point) for the first character in `value` . Returns `0` if `value` is empty, or if the resulting Unicode code\npoint is `0` .\n\n **Return type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "UNIX_DATE",
    "arguments": [],
    "category": "Date",
    "description_markdown": " **Description** \n\nReturns the number of days since `1970-01-01` .\n\n **Return Data Type** \n\nINT64\n\n **Example** \n\n\n<span id=\"datetime_functions\"></span>\n## Datetime functions\n\n\nGoogleSQL for BigQuery supports the following datetime functions.\n\nAll outputs are automatically formatted as per [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) ,\nseparating date and time with a `T` .\n\n\n\n"
  },
  {
    "name": "UNIX_MICROS",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nReturns the number of microseconds since `1970-01-01 00:00:00 UTC` .\n\n **Return Data Type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "UNIX_MILLIS",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nReturns the number of milliseconds since `1970-01-01 00:00:00 UTC` . Truncates\nhigher levels of precision by rounding down to the beginning of the millisecond.\n\n **Return Data Type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "UNIX_SECONDS",
    "arguments": [],
    "category": "Timestamp",
    "description_markdown": " **Description** \n\nReturns the number of seconds since `1970-01-01 00:00:00 UTC` . Truncates higher\nlevels of precision by rounding down to the beginning of the second.\n\n **Return Data Type** \n\n `INT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "UPPER",
    "arguments": [],
    "category": "String",
    "description_markdown": " **Description** \n\nFor `STRING` arguments, returns the original string with all alphabetic\ncharacters in uppercase. Mapping between uppercase and lowercase is done\naccording to the [Unicode Character Database](http://unicode.org/ucd/) without taking into account language-specific mappings.\n\nFor `BYTES` arguments, the argument is treated as ASCII text, with all bytes\ngreater than 127 left intact.\n\n **Return type** \n\n `STRING` or `BYTES` \n\n **Examples** \n\n\n<span id=\"table_functions_built_in\"></span>\n## Table functions (built in)\n\n\nGoogleSQL for BigQuery supports built-in table functions.\n\nThis topic includes functions that produce columns of a table.\nYou can only use these functions in the `FROM` clause.\n\n\n\n"
  },
  {
    "name": "VARIANCE",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nAn alias of [VAR_SAMP](#var_samp) .\n\n\n<span id=\"string_functions\"></span>\n## String functions\n\n\nGoogleSQL for BigQuery supports string functions.\nThese string functions work on two different values: `STRING` and `BYTES` data types. `STRING` values must be well-formed UTF-8.\n\nFunctions that return position values, such as [STRPOS](#strpos) ,\nencode those positions as `INT64` . The value `1` refers to the first character (or byte), `2` refers to the second, and so on.\nThe value `0` indicates an invalid position. When working on `STRING` types, the\nreturned positions refer to character positions.\n\nAll string comparisons are done byte-by-byte, without regard to Unicode\ncanonical equivalence.\n\n\n\n"
  },
  {
    "name": "VAR_POP",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nReturns the population (biased) variance of the values. The return result is\nbetween `0` and `+Inf` .\n\nAll numeric types are supported. If the\ninput is `NUMERIC` or `BIGNUMERIC` then the internal aggregation is\nstable with the final output converted to a `FLOAT64` .\nOtherwise the input is converted to a `FLOAT64` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any `NULL` inputs. If all inputs are ignored, this\nfunction returns `NULL` . If this function receives a single non- `NULL` input,\nit returns `0` .\n\n `NaN` is produced if:\n\n- Any input value is `    NaN` \n- Any input value is positive infinity or negative infinity.\n\nIf this function is used with the `OVER` clause, it's part of a\nwindow function call. In a window function call,\naggregate function clauses can't be used.\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "VAR_SAMP",
    "arguments": [],
    "category": "Statistical_aggregate",
    "description_markdown": " **Description** \n\nReturns the sample (unbiased) variance of the values. The return result is\nbetween `0` and `+Inf` .\n\nAll numeric types are supported. If the\ninput is `NUMERIC` or `BIGNUMERIC` then the internal aggregation is\nstable with the final output converted to a `FLOAT64` .\nOtherwise the input is converted to a `FLOAT64` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any `NULL` inputs. If there are fewer than two non- `NULL` inputs, this function returns `NULL` .\n\n `NaN` is produced if:\n\n- Any input value is `    NaN` \n- Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass\ninto this function, see [Aggregate function calls](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nThis function can be used with the [AGGREGATION_THRESHOLD clause](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nIf this function is used with the `OVER` clause, it's part of a\nwindow function call. In a window function call,\naggregate function clauses can't be used.\nTo learn more about the `OVER` clause and how to use it, see [Window function calls](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n **Return Data Type** \n\n `FLOAT64` \n\n **Examples** \n\n\n"
  },
  {
    "name": "VECTOR_SEARCH",
    "arguments": [],
    "category": "Search",
    "description_markdown": "To provide feedback or request support for this feature, send email to [bq-vector-search@google.com](mailto:bq-vector-search@google.com) .\n\n **Description** \n\nThe `VECTOR_SEARCH` function lets you search embeddings to find semantically\nsimilar entities.\n\nEmbeddings are high-dimensional numerical vectors that represent a given entity,\nlike a piece of text or an audio file. Machine learning (ML) models use\nembeddings to encode semantics about such entities to make it easier to\nreason about and compare them. For example, a common operation in clustering,\nclassification, and recommendation models is to measure the distance between\nvectors in an [embedding space](https://en.wikipedia.org/wiki/Latent_space) to\nfind items that are most semantically similar.\n\n **Definitions** \n\n-  `    base_table` : The table to search for nearest neighbor embeddings.\n-  `    base_table_query_statement` : A query that you can use to pre-filter the base\ntable. Only `    SELECT` , `    FROM` , and `    WHERE` clauses are allowed in this query.\nDon't apply any filters to the embedding column.\nYou can't use [logical views](/bigquery/docs/views-intro) in this query.\nUsing a [subquery](/bigquery/docs/reference/standard-sql/subqueries) might\ninterfere with index usage or cause your query to fail.\nIf the base table is indexed and the `    WHERE` clause contains columns that are\nnot stored in the index, then `    VECTOR_SEARCH` post-filters on those columns\ninstead. To learn more and enable pre-filtering, see [Store columns and pre-filter](/bigquery/docs/vector-index#stored-columns) . Use of this argument is in [preview](/products#product-launch-stages) .\n-  `    column_to_search` : The name of the base table column\nto search for nearest neighbor embeddings. The column must have\na type of `    ARRAY&lt;FLOAT64&gt;` . All elements in the array must be non- `    NULL` , and\nall values in the column must have the same array dimensions.\nIf the column has a vector index, BigQuery attempts to use it.\nTo determine if an index was used in the vector search, see [Vector index usage](/bigquery/docs/vector-index#vector_index_usage) .\n-  `    query_table` : The table that provides the\nembeddings for which to find nearest neighbors. All columns are passed\nthrough as output columns.\n-  `    query_statement` : A query that provides the\nembeddings for which to find nearest neighbors. All columns are passed\nthrough as output columns.\n-  `    query_column_to_search` : A named argument with a `    STRING` value. `    query_column_to_search_value` specifies the name of the column in the query\ntable or statement that contains the embeddings for which to find nearest\nneighbors. The column must have a type of `    ARRAY&lt;FLOAT64&gt;` . All elements in\nthe array must be non- `    NULL` and all values in the column must have the same\narray dimensions as the values in the `    column_to_search` column. If you don't\nspecify `    query_column_to_search_value` , the function uses the `    column_to_search` value or picks the most appropriate column.\n-  `    top_k` : A named argument with an `    INT64` value. `    top_k_value` specifies the number of nearest neighbors to\nreturn. The default is `    10` . A negative value is treated as infinity, meaning\nthat all values are counted as neighbors and returned.\n-  `    distance_type` : A named argument with a `    STRING` value. `    distance_type_value` specifies the type of metric to use to\ncompute the distance between two vectors. Supported distance types are [EUCLIDEAN](https://en.wikipedia.org/wiki/Euclidean_distance) , [COSINE](https://en.wikipedia.org/wiki/Cosine_similarity#Cosine_Distance) , and [DOT_PRODUCT](https://en.wikipedia.org/wiki/Dot_product) .\nThe default is `    EUCLIDEAN` .\n    \n    If you don't specify `    distance_type_value` and the `    column_to_search` column has a vector index that is used, `    VECTOR_SEARCH` uses the distance\ntype specified in the [distance_type option](/bigquery/docs/reference/standard-sql/data-definition-language#vector_index_option_list) of the `    CREATE VECTOR INDEX` statement.\n    \n    \n-  `    options` : A named argument with a JSON-formatted `    STRING` value. `    options_value` is a literal that specifies the following vector search\noptions:\n    \n    \n    -  `        fraction_lists_to_search` : A JSON number that specifies the\npercentage of lists to search. For example, `        options =&gt; '{\"fraction_lists_to_search\":0.15}'` . The `        fraction_lists_to_search` value must be in the range `        0.0` to `        1.0` ,\nexclusive.\n        \n        Specifying a higher percentage leads to higher recall and slower\nperformance, and the converse is true when specifying a lower percentage.\n        \n         `        fraction_lists_to_search` is only used when a vector index is also used.\nIf you don't specify a `        fraction_lists_to_search` value but an index is\nmatched, an appropriate value is picked.\n        \n        The number of available lists to search is determined by the [num_lists option](/bigquery/docs/reference/standard-sql/data-definition-language#vector_index_option_list) in the `        ivf_options` option or derived from\nthe [leaf_node_embedding_count option](/bigquery/docs/reference/standard-sql/data-definition-language#vector_index_option_list) in the `        tree_ah_options` option of the `        CREATE VECTOR INDEX` statement if\nspecified. Otherwise, BigQuery calculates an appropriate number.\n        \n        You can't specify `        fraction_lists_to_search` when `        use_brute_force` is\nset to `        true` .\n        \n        \n    -  `        use_brute_force` : A JSON boolean that determines whether to use brute\nforce search by skipping the vector index if one is available. For\nexample, `        options =&gt; '{\"use_brute_force\":true}'` . The\ndefault is `        false` . If you specify `        use_brute_force=false` and there is\nno useable vector index available, brute force is used anyway.\n        \n         `    options` defaults to `    '{}'` to denote that all underlying options use their\ncorresponding default values.\n    \n    \n\n **Details** \n\nYou can optionally use `VECTOR_SEARCH` with a [vector index](/bigquery/docs/vector-index) . When\na vector index is used, `VECTOR_SEARCH` uses the [Approximate Nearest\nNeighbor](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods) search technique to help improve vector search performance, with\nthe trade-off of reducing [recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall#recallsearch_term_rules) and so returning more approximate\nresults. Brute force is used to return exact results when a vector index isn't\navailable, and you can choose to use brute force to get exact results even when\na vector index is available.\n\n **Output** \n\nFor each row in the query data, the output contains multiple rows from the\nbase table that satisfy the search criteria. The number of results rows per\nquery table row is either 10 or the `top_k` value if it is specified. The\norder of the output isn't guaranteed.\n\nThe output includes the following columns:\n\n-  `    query` : A `    STRUCT` value that contains all selected columns from the query\ndata.\n-  `    base` : A `    STRUCT` value that contains all columns from `    base_table` or a\nsubset of the columns from `    base_table` that you selected in the `    base_table_query_statement` query.\n-  `    distance` : A `    FLOAT64` value that represents the distance between the base\ndata and the query data.\n\n **Limitations** \n\nBigQuery data security and governance rules apply to the use of `VECTOR_SEARCH` , which results in the following behavior:\n\n- If the base table has [row-level security policies](/bigquery/docs/row-level-security-intro) , `    VECTOR_SEARCH` applies the row-level\naccess policies to the query results.\n- If the indexed column from the base table has [data masking policies](/bigquery/docs/column-data-masking-intro) , `    VECTOR_SEARCH` succeeds only if the user\nrunning the query has the [Fine-Grained Reader](/iam/docs/understanding-roles#datacatalog.categoryFineGrainedReader) role on the policy tags\nthat are used. Otherwise, `    VECTOR_SEARCH` fails with an invalid query error.\n- If any base table column or any column in the query table or statement has [column-level security policies](/bigquery/docs/column-level-security) and you don't have appropriate\npermissions to access the column, `    VECTOR_SEARCH` fails with a permission\ndenied error.\n    \n    \n- The project that runs the query containing `    VECTOR_SEARCH` must match the\nproject that contains the base table.\n    \n    \n\n **Examples** \n\nThe following queries create test tables `table1` and `table2` to use in\nsubsequent query examples :\n\nThe following example searches the `my_embedding` column of `table1` for the top\ntwo embeddings that match each row of data in the `embedding` column of `table2` :\n\nThe following example pre-filters `table1` to rows where `id` is not equal to\n4 and then searches the `my_embedding` column of `table1` for the top\ntwo embeddings that match each row of data in the `embedding` column of `table2` .\n\nThe following example searches the `my_embedding` column of `table1` for the top\ntwo embeddings that match each row of data in the `embedding` column of `table2` , and uses the `COSINE` distance type to measure the distance between\nthe embeddings:\n\n\n<span id=\"security_functions\"></span>\n## Security functions\n\n\nGoogleSQL for BigQuery supports the following security functions.\n\n\n\n"
  }
]